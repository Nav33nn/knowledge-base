{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "#from Env import CabDriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routines\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from itertools import permutations,product\n",
    "\n",
    "# Defining hyperparameters\n",
    "m = 5 # number of cities, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 7  # number of days, ranges from 0 ... d-1\n",
    "C = 5 # Per hour fuel and other costs\n",
    "R = 9 # per hour revenue from a passenger\n",
    "\n",
    "\n",
    "class CabDriver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"initialise your state and define your action space and state space\"\"\"\n",
    "        action_list = list(permutations(range(0,m) ,2))\n",
    "        action_list.append((0,0))\n",
    "        self.action_space = np.array(action_list) #action space is unique 2 values(source & destination) + the no op\n",
    "        self.state_space = list(product(*[list(range(0,m)), list(range(0,t)), list(range(0,d))])) #State space from MDP:\n",
    "        #ð‘ =ð‘‹ð‘–ð‘‡ð‘—ð·ð‘˜ ð‘¤â„Žð‘’ð‘Ÿð‘’ ð‘–=0â€¦ð‘šâˆ’1;ð‘—=0â€¦.ð‘¡âˆ’1;ð‘˜=0â€¦..ð‘‘âˆ’1, Where ð‘‹ð‘– represents a driverâ€™s current location, ð‘‡ð‘— represents time component (more specifically hour of the day), ð·ð‘˜ represents the day of the week\n",
    "        self.state_size = len(self.state_space)\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_init = random.choice(self.state_space) #Initialises to any random self_space\n",
    "        self.encode_vector = np.array([24*7, 7, 1]).reshape(3, 1)\n",
    "\n",
    "\n",
    "        # Start the first round\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    ## Encoding state (or state-action) for NN input\n",
    "\n",
    "    def state_encod_arch1(self, curr_state, batch_size=1):\n",
    "        \"\"\"convert the state into a vector so that it can be fed to the NN. This method converts a given state into a vector format. Hint: The vector is of size m + t + d.\"\"\"\n",
    "       \n",
    "        #Encoded values of m + t + d\n",
    "        \n",
    "        curr_state = np.array(curr_state).reshape(1, 3)\n",
    "        #print(curr_state.shape)\n",
    "        #enc_mat = self.encode_vector\n",
    "        # pos = (state[0]*24*7) + (state[1]*7) + state[2]\n",
    "        \n",
    "        pos_mat = np.dot(curr_state, self.encode_vector)\n",
    "        state_encod =  np.zeros((1, self.state_size))\n",
    "        # state_encod[pos] = 1\n",
    "        for i in range(batch_size):\n",
    "            state_encod[i][pos_mat[i]] = 1\n",
    "\n",
    "        return np.reshape(state_encod, [1, env.state_size])\n",
    "    \n",
    "\n",
    "\n",
    "    # Use this function if you are using architecture-2 \n",
    "    # def state_encod_arch2(self, state, action):\n",
    "    #     \"\"\"convert the (state-action) into a vector so that it can be fed to the NN. This method converts a given state-action pair into a vector format. Hint: The vector is of size m + t + d + m + m.\"\"\"\n",
    "\n",
    "        \n",
    "    #     return state_encod\n",
    "\n",
    "\n",
    "    ## Getting number of requests\n",
    "\n",
    "    def requests(self, state):\n",
    "        \"\"\"Determining the number of requests basis the location. \n",
    "        Use the table specified in the MDP and complete for rest of the locations\"\"\"\n",
    "        location = state[0]\n",
    "        requests = 0\n",
    "        if location == 0:\n",
    "            requests = np.random.poisson(2)\n",
    "\n",
    "        if location == 1:\n",
    "            requests = np.random.poisson(12)   #MDP Poisson distribution\n",
    "        \n",
    "        if location == 2:\n",
    "            requests = np.random.poisson(4)    #MDP Poisson distribution\n",
    "            \n",
    "        if location == 3:\n",
    "            requests = np.random.poisson(7)    #MDP Poisson distribution\n",
    "\n",
    "        if location == 4:\n",
    "            requests = np.random.poisson(8)    #MDP Poisson distribution  \n",
    "            \n",
    "        if requests > 15:\n",
    "            requests = 15\n",
    "\n",
    "        possible_actions_index = random.sample(range(0, (m-1)*m), requests) # (0,0) is not considered as customer request\n",
    "        possible_actions_index.append(20) #add the index of No-OP action (0, 0)\n",
    "        actions = [self.action_space[i] for i in possible_actions_index]\n",
    "\n",
    "        print('Number of actions available', len(actions))\n",
    "        return possible_actions_index, actions   \n",
    "\n",
    "\n",
    "\n",
    "    def reward_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes in state, action and Time-matrix and returns the reward\"\"\"\n",
    "        if action[0] == action[1]:\n",
    "            reward = -C \n",
    "            return reward\n",
    "\n",
    "        #print('reward:' ,state, action)\n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = state[0]\n",
    "        time = state[1]\n",
    "        day = state[2]\n",
    "        #print('reward vals:', (p, q, i, time, day))\n",
    "        t_pq = Time_matrix[p][q][time][day]\n",
    "        t_ip = Time_matrix[i][p][time][day]\n",
    "        \n",
    "        \n",
    "        reward = (R*t_pq)-(C*(t_pq+t_ip))\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def next_state_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes state and action as input and returns next state\"\"\"\n",
    "        \n",
    "        #print('next_state :', state, action)\n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = state[0]\n",
    "        time_curr = state[1]\n",
    "        day_curr = state[2]\n",
    "        #print('next_state_vals :', (p, q, i, time_curr, day_curr))\n",
    "        time_next = time_curr + Time_matrix[p][q][time_curr][day_curr]\n",
    "\n",
    "        day_next = int((day_curr+int(time_next/24)) % 7)\n",
    "        time_next = int(time_next % 24)\n",
    "            \n",
    "        next_state = (q,time_next,day_next)\n",
    "        return next_state\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        return self.action_space, self.state_space, self.state_init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_pickle(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, discount_factor=0.95, learning_rate=0.01,\n",
    "                       epsilon=0.99, epsilon_decay=0.99, epsilon_min=0.01):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate        \n",
    "        self.epsilon_max = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model_history = None\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        #self.batch_size = 1\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "\n",
    "        # hidden layers\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))     \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, cstate, all_actions, pos_act_ind):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment\n",
    "        actions = all_actions[pos_act_ind]\n",
    "        if np.random.rand() <= self.epsilon_max:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            print('Exploring')\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            print('Exploiting')\n",
    "            cstate = cstate.reshape(1, self.state_size) \n",
    "            q_value = self.model.predict(x=cstate)\n",
    "            max_index = np.argmax(q_value[0])\n",
    "            action = all_actions[max_index] if max_index in pos_act_ind else random.choice(actions)\n",
    "        print('Selected action ', action)    \n",
    "        return action\n",
    "        \n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                \n",
    "                # Write your code from here\n",
    "                # 1. Predict the target from earlier model\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            \n",
    "            target = self.model.predict(update_input)\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            #print(target, target.shape)\n",
    "            #print(target.shape, target_qval.shape)\n",
    "\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                #print(i, actions[i])\n",
    "                if done[i]:\n",
    "                    #target[i][actions[i]] = rewards[i]\n",
    "                    target[i] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    #target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                    target[i] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "            return self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=1)\n",
    "            \n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "# make dir to store model weights\n",
    "if not os.path.exists(\"saved_model_weights\"):\n",
    "    os.mkdir(\"saved_model_weights\")\n",
    "\n",
    "# n_episodes\n",
    "n_episodes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                26912     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 28,661\n",
      "Trainable params: 28,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of actions available 7\n",
      "Episode : 0\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2563.9480\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 2487.5459\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 2544.6052\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2498.7925\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 2361.7910\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 63us/step - loss: 2188.1575\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 2061.9553\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 2386.6558\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2169.0310\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 2139.0171\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 2165.3452\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 1770.5720\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 1876.2490\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 1916.6565\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 2030.7421\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 1786.8508\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 1987.8446\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2079.7976\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 2064.9331\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 2172.5532\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 34us/step - loss: 2182.4463\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 2093.4910\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 63us/step - loss: 2623.9827\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 2286.6040\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 2315.9443\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2189.0020\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 2277.4460\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 2431.2856\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2761.0024\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 3093.5620\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 2646.8999\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 2621.1279\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 2144.8013\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 36us/step - loss: 2876.0122\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 3336.5327\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 3421.9951\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 3221.4727\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 3534.2864\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 4343.9150\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 4127.2319\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 4607.7930\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 4954.3413\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 5919.9204\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 6773.9858\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 6861.4033\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 6398.0654\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 7736.1953\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 10042.0654\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 8838.9141\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 9901.5303\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 11839.4658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 12735.9297\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 14275.6045\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 15198.1699\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 14969.0381\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 18284.8594\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 20610.4121\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 20707.7441\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 27845.1680\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 26093.2031\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 32113.9941\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 28389.9336\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 31020.7930\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 36us/step - loss: 38896.6797\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 42513.0781\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 48785.3555\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 43785.0352\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 45622.5039\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 47500.7031\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 58765.9062\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 59356.5156\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 60649.8164\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 63084.7891\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 75155.7109\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 68024.9375\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 79717.2578\n",
      "Number of actions available 9\n",
      "Episode : 1\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 78492.1953\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 69us/step - loss: 82526.7500\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 90555.6719\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 93566.5703\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 92742.1250\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 100720.9062\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 61us/step - loss: 99266.7500\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 95683.6016\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 100892.0859\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 100927.3984\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 103558.6719\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 109931.4609\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 114403.1094\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 65us/step - loss: 110734.1406\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 115577.3438\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 126967.0625\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 127326.5625\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 111306.9609\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 59us/step - loss: 128381.0781\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 120684.8125\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 116051.1719\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 123143.2344\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 115219.7344\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 120356.6250\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 105996.9609\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 60us/step - loss: 117626.3516\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 105724.4922\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 114747.7734\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 91949.3125\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 100918.7031\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 95635.2734\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 100786.2109\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 81055.0156\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 83284.1719\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 79175.7422\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 71951.5938\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 65296.2266\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 73044.9141\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 72197.4844\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 75490.2812\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 72872.6094\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 62972.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 56159.1016\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 59457.4023\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 60036.6875\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 52954.1602\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 49367.3281\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 52821.4297\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 60295.8555\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 64726.9844\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 46682.7656\n",
      "Exploiting\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 53953.9102\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 63883.7656\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 44542.8438\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 48228.1484\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 44493.4453\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 35677.9922\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 41544.0469\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 37452.4336\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 32825.3750\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 41007.2930\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 34695.9414\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 34054.5977\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 38490.5898\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 35108.6758\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 30843.1836\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 34437.1797\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 29791.1543\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 31735.0508\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 30567.6680\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 30234.1406\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 34748.0742\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 23815.2031\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 37355.4297\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 39718.0469\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 21975.5723\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 32933.1797\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 29770.3027\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 31602.6211\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 24560.1055\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 31182.7148\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 31315.7754\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 21107.1836\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 16878.1211\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 16761.2363\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 25546.6680\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 61us/step - loss: 20757.5977\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 24119.6133\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 23241.5938\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 25834.2578\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 28823.3633\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 26536.1660\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 26792.9160\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 25534.8809\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 20506.4609\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 60us/step - loss: 18014.3867\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 20657.8711\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 35329.1016\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 24276.5391\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 30817.0469\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 26001.1562\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 36951.7852\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 21241.9668\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 31392.3711\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 17823.6562\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 24506.1914\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 21158.1016\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 37779.3750\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 23109.7227\n",
      "Exploiting\n",
      "Selected action  [0 1]\n",
      "[0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 28843.6836\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 24567.2773\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 25067.2832\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 22562.8945\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 27156.4180\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 27082.0098\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 27057.3125\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 25583.4883\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 37425.8633\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 26844.3770\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 20407.1250\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 23029.5176\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 32606.0938\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 19087.1035\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 22092.8867\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 25243.3359\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 27286.8555\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 35722.3789\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 29292.6191\n",
      "Number of actions available 7\n",
      "Episode : 2\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 38157.6289\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 23000.3672\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 24733.2344\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 24424.8945\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 24540.3262\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 17910.1699\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 23639.7266\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 19122.4629\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 25367.8359\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 21292.6172\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 31095.9473\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 21082.5059\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 21191.2754\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 32133.1621\n",
      "Exploiting\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 22653.4766\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 25370.0117\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 62us/step - loss: 26438.9102\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 15206.6426\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 28811.2051\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 31431.9414\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 21354.3691\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 16645.7930\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 25662.5938\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 27746.9062\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 19314.6680\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 31147.8086\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 28993.1680\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 17132.8555\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 23571.1855\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 26241.7773\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 26221.5293\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 25056.6953\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 27919.7637\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 23317.6465\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 18293.5781\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 16830.8027\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 22512.4297\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 21153.9766\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 16688.1914\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 21441.8125\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 23252.3359\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 25372.9082\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 23915.7148\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 19240.8203\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 26868.1094\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 18655.0742\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 33544.9258\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 25638.3633\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 15864.7559\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 22933.8047\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 26840.6348\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 28182.5508\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 24088.2871\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 28601.7734\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 24293.6445\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 23530.4492\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 38516.8906\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 33321.7031\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 30747.2871\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 22366.5117\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 33391.0234\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 35847.6211\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 30057.1777\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 30695.8867\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 33314.8203\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 23336.1562\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 64us/step - loss: 21616.8203\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 26553.4023\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 25736.0859\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 36128.1133\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 38344.5898\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 22175.7441\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 25918.1777\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 36750.5117\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 22719.8398\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 40040.5000\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 43569.4727\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 22022.0742\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 28915.2715\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 15849.1670\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 19206.8711\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 40085.3633\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 30046.2949\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 38702.4297\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 21343.7012\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 29113.6094\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 52118.2578\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 38470.1328\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 36764.7344\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 41494.0938\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 32565.2500\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 29039.5312\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 37576.3086\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 48539.1172\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 37157.6875\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 37907.7344\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 26812.8828\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 47947.9531\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 64us/step - loss: 60643.9766\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 37557.8320\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 47943.7422\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 61644.1875\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 58982.8086\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 49025.9414\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 42360.7109\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 49261.7188\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 59845.6094\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 36786.2969\n",
      "Exploiting\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 45527.6172\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 55006.3750\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 49235.6211\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 49830.3320\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 40994.3203\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 54525.9297\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 34195.3438\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 66805.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 72658.6172\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 45467.1094\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 61141.9648\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 45233.0742\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 76594.7031\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 79161.2812\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 56160.6562\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 51652.8516\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 43319.1250\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 88196.3906\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 74371.7812\n",
      "Number of actions available 13\n",
      "Episode : 3\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 51826.1133\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 125904.8672\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 69689.1562\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 81923.5938\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 60625.5664\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 121296.0469\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 80050.1484\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 66us/step - loss: 126151.8125\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 52150.6641\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 107365.3516\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 58188.1250\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 65844.1641\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 71708.7656\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 98813.1016\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 133389.0625\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 60us/step - loss: 94230.5781\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 103974.0781\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 52207.0078\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 103458.8125\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 66638.6172\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 77044.2500\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 85194.0469\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 59us/step - loss: 92910.9531\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 112337.0234\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 98911.5469\n",
      "Exploiting\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 70532.7812\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 80588.7188\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 99801.4766\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 95874.9375\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 87605.8125\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 114882.0000\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 100318.6875\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 116938.0000\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 70010.5156\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 82053.4609\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 158955.2500\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 96987.1328\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 112900.6406\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 142925.5000\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 130404.9922\n",
      "Exploiting\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 133042.9375\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 82603.6094\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 171545.8438\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 140663.8125\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 234763.3125\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 173014.9375\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 131083.8906\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 143642.8750\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 157958.0469\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 158990.7656\n",
      "Exploiting\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 136361.9219\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 132335.7031\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 51us/step - loss: 174048.4844\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 60us/step - loss: 78623.4453\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 176837.0938\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 153903.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 122238.8906\n",
      "Exploiting\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 89605.5000\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 114449.7891\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 61us/step - loss: 147923.4375\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 133283.4844\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 148634.6406\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 135358.9375\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 145102.2812\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 102184.9766\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 140485.0000\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 129845.9688\n",
      "Exploiting\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 97397.2188\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 144271.0625\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 63us/step - loss: 93752.5469\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 67383.7188\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 131757.8906\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 161189.8438\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 120010.7812\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 102214.3047\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 105401.0547\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 107898.8438\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 96295.7188\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 147919.7344\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 96469.5156\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 132667.2812\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 81072.6875\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 66194.0625\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 89548.3516\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 141607.6562\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 99634.5938\n",
      "Exploiting\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 108026.0234\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 58894.2578\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 59us/step - loss: 146825.7031\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 85121.3594\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 72535.3359\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 113706.2969\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 89457.9531\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 91813.9688\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 80148.3047\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 78279.9531\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 43543.1016\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 76845.9062\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 58392.6094\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 81174.0000\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 81628.3906\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 88377.2266\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 36us/step - loss: 48327.5586\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 76096.4062\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 68867.9844\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 110320.1875\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 79193.7734\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 73976.6562\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 47072.4766\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 53980.2773\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 109002.3203\n",
      "Exploiting\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 50438.3711\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 93421.6797\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 79494.6875\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 115211.9219\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 82367.4297\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 83875.3281\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 65932.9688\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 73739.4219\n",
      "Number of actions available 6\n",
      "Episode : 4\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 70772.7734\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 75685.3750\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 66841.2031\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 96547.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 49204.8125\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 70949.6562\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 80200.1016\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 39148.6367\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 70335.1094\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 69124.2656\n",
      "Exploiting\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 49247.5469\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 70784.5938\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 74510.5312\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 60908.5156\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 96080.0938\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 96431.3906\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 86329.7188\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 119317.0547\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 64980.6523\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 38354.6133\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 62154.5273\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 92502.4375\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 55810.0273\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 61868.8242\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 80280.5625\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 95460.1250\n",
      "Exploiting\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 88970.0000\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 61141.5156\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 67us/step - loss: 61346.2227\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 60254.9961\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 53us/step - loss: 72151.3125\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 54us/step - loss: 87757.2969\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 49118.8047\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 58us/step - loss: 30580.2148\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 55265.6406\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 55538.4297\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 74984.6406\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 50188.4688\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 69733.9062\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 48871.1562\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 47006.7500\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 35879.9922\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 60533.6172\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 58109.8984\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 54285.4062\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 43792.9102\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 58406.6914\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 64us/step - loss: 83120.8906\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 82205.1094\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 62352.8789\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 57us/step - loss: 65057.7500\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 67250.7188\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 62675.8984\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 87200.2188\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 111562.0703\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 40us/step - loss: 46135.4141\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 117142.9375\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 61565.4531\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 62061.3672\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 62038.7773\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 58354.4688\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 72555.2656\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 101248.9141\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 73391.5625\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 94137.9141\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 56250.6680\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 135217.7812\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 37us/step - loss: 67936.8594\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 66us/step - loss: 97939.6406\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 94575.5469\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 88544.0312\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 90885.4375\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 56581.4531\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 47469.0547\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 41269.5391\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 76415.0078\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 86359.5156\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 91162.7344\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 72908.8438\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 68313.6875\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 69us/step - loss: 67056.9688\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 59us/step - loss: 66070.9141\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 68165.1406\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 38us/step - loss: 79123.6562\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 48298.2969\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 50us/step - loss: 44126.9883\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 87349.4531\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 62527.7070\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 38366.4141\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 47us/step - loss: 66330.8516\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 59870.5000\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 58349.3477\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 102152.1562\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 37819.6133\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 59924.3203\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 39us/step - loss: 128858.5234\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 49us/step - loss: 122243.2344\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 45us/step - loss: 61394.8867\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 47805.2539\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 52467.8906\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 56us/step - loss: 41356.2422\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 41us/step - loss: 78094.1094\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 46us/step - loss: 50097.6953\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 48us/step - loss: 80880.2344\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 55us/step - loss: 47749.3594\n",
      "Exploiting\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 55403.1016\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 106174.6406\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 49624.3984\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 44us/step - loss: 81372.0469\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 44227.4844\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 52264.0234\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 42us/step - loss: 58973.2383\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 68140.6719\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 43us/step - loss: 41148.1367\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 52us/step - loss: 49389.1289\n"
     ]
    }
   ],
   "source": [
    " # Call all the initialised variables of the environment\n",
    "env = CabDriver()\n",
    "#Call the DQN agent\n",
    "dqn = DQNAgent(env.state_size, env.action_size)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "   \n",
    "    _,_,curr_state = env.reset()\n",
    "    state_size = env.state_size\n",
    "    pos_act_ind, actions = env.requests(curr_state)\n",
    "    action = random.choice(actions)\n",
    "    #action_size = len(actions)\n",
    "    reward = 0\n",
    "    curr_time = 0\n",
    "    #print(curr_state)\n",
    "    \n",
    "    \n",
    "    terminal_state = False\n",
    "    print(\"Episode :\", episode)\n",
    "    \n",
    "    while not terminal_state:\n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        encoded_state = env.state_encod_arch1(curr_state)\n",
    "        #encoded_state = np.reshape(encoded_state, [1, env.state_size])\n",
    "        action = dqn.get_action(encoded_state, env.action_space, pos_act_ind)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = reward + env.reward_func(curr_state, action, Time_matrix)\n",
    "        next_state = env.next_state_func(curr_state,action,Time_matrix)\n",
    "        \n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = curr_state[0]\n",
    "        time = curr_state[1]\n",
    "        day = curr_state[2]\n",
    "        print(action)\n",
    "        curr_time = curr_time + Time_matrix[i][p][time][day]\n",
    "        \n",
    "        \n",
    "        day = int((day+int(time/24)) % 7)\n",
    "        time = int(time % 24)\n",
    "        \n",
    "        curr_time = curr_time + Time_matrix[p][q][time][day]\n",
    "        day = int((day+int(time/24)) % 7)\n",
    "        time = int(time % 24)\n",
    "        # 3. Append the experience to the memory\n",
    "        dqn.append_sample(curr_state, action, reward, next_state, terminal_state)\n",
    "        curr_state = next_state\n",
    "        \n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        history = dqn.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        \n",
    "        #if history:\n",
    "         #   print('rewards: ', reward, 'q-value: ', action)\n",
    "         #   print('loss:', history.history['loss'])\n",
    "        \n",
    "        if curr_time >= 24*30:\n",
    "            terminal_state = True\n",
    "    \n",
    "        # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(reward)\n",
    "    episodes.append(episode)\n",
    "        \n",
    "    if dqn.epsilon_max > dqn.epsilon_min:\n",
    "        dqn.epsilon_max *= dqn.epsilon_decay        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward of last 100 episodes is 46.4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make directory\n",
    "if not os.path.exists(\"saved_pickle_files\"):\n",
    "    os.mkdir(\"saved_pickle_files\")\n",
    "\n",
    "# save rewards_per_episode\n",
    "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")\n",
    "\n",
    "\n",
    "# plot results\n",
    "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
    "    rewards_per_episode = pickle.load(f)\n",
    "\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"reward per episode\")\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('rewards.png')\n",
    "\n",
    "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
