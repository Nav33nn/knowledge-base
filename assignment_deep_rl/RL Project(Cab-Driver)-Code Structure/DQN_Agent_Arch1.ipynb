{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "#from Env import CabDriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routines\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from itertools import permutations,product\n",
    "\n",
    "# Defining hyperparameters\n",
    "m = 5 # number of cities, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 7  # number of days, ranges from 0 ... d-1\n",
    "C = 5 # Per hour fuel and other costs\n",
    "R = 9 # per hour revenue from a passenger\n",
    "\n",
    "\n",
    "class CabDriver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"initialise your state and define your action space and state space\"\"\"\n",
    "        action_list = list(permutations(range(0,m) ,2))\n",
    "        action_list.append((0,0))\n",
    "        self.action_space = np.array(action_list) #action space is unique 2 values(source & destination) + the no op\n",
    "        self.state_space = list(product(*[list(range(0,m)), list(range(0,t)), list(range(0,d))])) #State space from MDP:\n",
    "        #𝑠=𝑋𝑖𝑇𝑗𝐷𝑘 𝑤ℎ𝑒𝑟𝑒 𝑖=0…𝑚−1;𝑗=0….𝑡−1;𝑘=0…..𝑑−1, Where 𝑋𝑖 represents a driver’s current location, 𝑇𝑗 represents time component (more specifically hour of the day), 𝐷𝑘 represents the day of the week\n",
    "        self.state_size = len(self.state_space)\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_init = random.choice(self.state_space) #Initialises to any random self_space\n",
    "        self.encode_vector = np.array([24*7, 7, 1]).reshape(3, 1)\n",
    "\n",
    "\n",
    "        # Start the first round\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    ## Encoding state (or state-action) for NN input\n",
    "\n",
    "    def state_encod_arch1(self, curr_state, batch_size=1):\n",
    "        \"\"\"convert the state into a vector so that it can be fed to the NN. This method converts a given state into a vector format. Hint: The vector is of size m + t + d.\"\"\"\n",
    "       \n",
    "        #Encoded values of m + t + d\n",
    "        \n",
    "        curr_state = np.array(curr_state).reshape(1, 3)\n",
    "        #print(curr_state.shape)\n",
    "        #enc_mat = self.encode_vector\n",
    "        # pos = (state[0]*24*7) + (state[1]*7) + state[2]\n",
    "        \n",
    "        pos_mat = np.dot(curr_state, self.encode_vector)\n",
    "        state_encod =  np.zeros((1, self.state_size))\n",
    "        # state_encod[pos] = 1\n",
    "        for i in range(batch_size):\n",
    "            state_encod[i][pos_mat[i]] = 1\n",
    "\n",
    "        return np.reshape(state_encod, [1, env.state_size])\n",
    "    \n",
    "\n",
    "\n",
    "    # Use this function if you are using architecture-2 \n",
    "    # def state_encod_arch2(self, state, action):\n",
    "    #     \"\"\"convert the (state-action) into a vector so that it can be fed to the NN. This method converts a given state-action pair into a vector format. Hint: The vector is of size m + t + d + m + m.\"\"\"\n",
    "\n",
    "        \n",
    "    #     return state_encod\n",
    "\n",
    "\n",
    "    ## Getting number of requests\n",
    "\n",
    "    def requests(self, state):\n",
    "        \"\"\"Determining the number of requests basis the location. \n",
    "        Use the table specified in the MDP and complete for rest of the locations\"\"\"\n",
    "        location = state[0]\n",
    "        requests = 0\n",
    "        if location == 0:\n",
    "            requests = np.random.poisson(2)\n",
    "\n",
    "        if location == 1:\n",
    "            requests = np.random.poisson(12)   #MDP Poisson distribution\n",
    "        \n",
    "        if location == 2:\n",
    "            requests = np.random.poisson(4)    #MDP Poisson distribution\n",
    "            \n",
    "        if location == 3:\n",
    "            requests = np.random.poisson(7)    #MDP Poisson distribution\n",
    "\n",
    "        if location == 4:\n",
    "            requests = np.random.poisson(8)    #MDP Poisson distribution  \n",
    "            \n",
    "        if requests > 15:\n",
    "            requests = 15\n",
    "\n",
    "        possible_actions_index = random.sample(range(0, (m-1)*m), requests) # (0,0) is not considered as customer request\n",
    "        possible_actions_index.append(20) #add the index of No-OP action (0, 0)\n",
    "        actions = [self.action_space[i] for i in possible_actions_index]\n",
    "\n",
    "        print('Number of actions available', len(actions))\n",
    "        return possible_actions_index, actions   \n",
    "\n",
    "\n",
    "\n",
    "    def reward_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes in state, action and Time-matrix and returns the reward\"\"\"\n",
    "        if action[0] == action[1]:\n",
    "            reward = -C \n",
    "            return reward\n",
    "\n",
    "        #print('reward:' ,state, action)\n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = state[0]\n",
    "        time = state[1]\n",
    "        day = state[2]\n",
    "        #print('reward vals:', (p, q, i, time, day))\n",
    "        t_pq = Time_matrix[p][q][time][day]\n",
    "        t_ip = Time_matrix[i][p][time][day]\n",
    "        \n",
    "        \n",
    "        reward = (R*t_pq)-(C*(t_pq+t_ip))\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def next_state_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes state and action as input and returns next state\"\"\"\n",
    "        \n",
    "        #print('next_state :', state, action)\n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = state[0]\n",
    "        time_curr = state[1]\n",
    "        day_curr = state[2]\n",
    "        #print('next_state_vals :', (p, q, i, time_curr, day_curr))\n",
    "        time_next = time_curr + Time_matrix[p][q][time_curr][day_curr]\n",
    "\n",
    "        day_next = int((day_curr+int(time_next/24)) % 7)\n",
    "        time_next = int(time_next % 24)\n",
    "            \n",
    "        next_state = (q,time_next,day_next)\n",
    "        return next_state\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        return self.action_space, self.state_space, self.state_init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_pickle(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, discount_factor=0.95, learning_rate=0.01,\n",
    "                       epsilon=0.99, epsilon_decay=0.99, epsilon_min=0.01):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate        \n",
    "        self.epsilon_max = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model_history = None\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        #self.batch_size = 1\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "\n",
    "        # hidden layers\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))     \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, cstate, all_actions, pos_act_ind):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in ε after we generate each sample from the environment\n",
    "        actions = all_actions[pos_act_ind]\n",
    "        q_value = 0\n",
    "        if np.random.rand() <= self.epsilon_max:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            print('Exploring')\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            print('Exploiting')\n",
    "            #cstate = cstate.reshape(1, self.state_size) \n",
    "            q_value = self.model.predict(x=cstate)\n",
    "            max_index = np.argmax(q_value[0])\n",
    "            action = all_actions[max_index] if max_index in pos_act_ind else random.choice(actions)\n",
    "        print('Selected action ', action)    \n",
    "        return action, q_value\n",
    "        \n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                \n",
    "                # Write your code from here\n",
    "                # 1. Predict the target from earlier model\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            \n",
    "            target = self.model.predict(update_input)\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            #print(target, target.shape)\n",
    "            #print(target.shape, target_qval.shape)\n",
    "\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                #print(i, actions[i])\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                    #target[i] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                    #target[i] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "            return self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=1)\n",
    "            \n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes, q_vals_per_episode, loss = [], [], [], []\n",
    "\n",
    "# make dir to store model weights\n",
    "if not os.path.exists(\"saved_model_weights\"):\n",
    "    os.mkdir(\"saved_model_weights\")\n",
    "\n",
    "# n_episodes\n",
    "n_episodes = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                26912     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 28,661\n",
      "Trainable params: 28,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of actions available 13\n",
      "Episode : 0\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploiting\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1027.3828\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [1027.3828125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1015.4443\n",
      "rewards:  -83.0 q-value:  0\n",
      "loss: [1015.4443359375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 1008.0497\n",
      "rewards:  -111.0 q-value:  0\n",
      "loss: [1008.0497436523438]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 939.7676\n",
      "rewards:  -79.0 q-value:  0\n",
      "loss: [939.767578125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 964.2001\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [964.2000732421875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 977.3102\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [977.3101806640625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 917.6937\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [917.6937255859375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 864.5728\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [864.57275390625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 944.9729\n",
      "rewards:  -51.0 q-value:  0\n",
      "loss: [944.972900390625]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 933.7404\n",
      "rewards:  -39.0 q-value:  0\n",
      "loss: [933.7404174804688]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 890.5585\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [890.5584716796875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 872.8856\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [872.8855590820312]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 265us/step - loss: 898.0408\n",
      "rewards:  -69.0 q-value:  0\n",
      "loss: [898.040771484375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 892.0648\n",
      "rewards:  -45.0 q-value:  0\n",
      "loss: [892.0648193359375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 780.4667\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [780.4666748046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 828.7819\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [828.7818603515625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 791.4663\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [791.46630859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 844.7533\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [844.7532958984375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 804.9452\n",
      "rewards:  -24.0 q-value:  0\n",
      "loss: [804.9451904296875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 785.8983\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [785.8982543945312]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 747.5400\n",
      "rewards:  -45.0 q-value:  0\n",
      "loss: [747.5399780273438]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 690.0682\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [690.0682373046875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 799.4635\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [799.4635009765625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 697.2362\n",
      "rewards:  11.0 q-value:  0\n",
      "loss: [697.2362060546875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 710.2472\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [710.2471923828125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 686.5072\n",
      "rewards:  36.0 q-value:  0\n",
      "loss: [686.5072021484375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 728.7910\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [728.791015625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 217us/step - loss: 663.3964\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [663.3963623046875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 595.4043\n",
      "rewards:  55.0 q-value:  0\n",
      "loss: [595.404296875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 598.1871\n",
      "rewards:  54.0 q-value:  0\n",
      "loss: [598.1870727539062]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 615.9202\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [615.920166015625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 330us/step - loss: 648.1122\n",
      "rewards:  38.0 q-value:  0\n",
      "loss: [648.1121826171875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 528.6812\n",
      "rewards:  45.0 q-value:  0\n",
      "loss: [528.68115234375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 703.2827\n",
      "rewards:  51.0 q-value:  0\n",
      "loss: [703.2826538085938]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 651.6487\n",
      "rewards:  52.0 q-value:  0\n",
      "loss: [651.6487426757812]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 617.7643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  51.0 q-value:  0\n",
      "loss: [617.7643432617188]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 510.9820\n",
      "rewards:  35.0 q-value:  0\n",
      "loss: [510.9819641113281]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 546.4935\n",
      "rewards:  39.0 q-value:  0\n",
      "loss: [546.4935302734375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 464.7519\n",
      "rewards:  22.0 q-value:  0\n",
      "loss: [464.7519226074219]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 630.7762\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [630.7761840820312]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 540.7274\n",
      "rewards:  -11.0 q-value:  0\n",
      "loss: [540.7274169921875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 584.9933\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [584.9932861328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 607.3501\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [607.35009765625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 520.1939\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [520.1939086914062]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 571.8207\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [571.8206787109375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 644.7206\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [644.7206420898438]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 449.3203\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [449.32025146484375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 563.8835\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [563.883544921875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 499.8037\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [499.8037414550781]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 426.5509\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [426.5508728027344]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 469.4543\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [469.4543151855469]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 417.7356\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [417.7356262207031]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 465.9102\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [465.91021728515625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 514.4386\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [514.4385986328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 605.9526\n",
      "rewards:  -27.0 q-value:  0\n",
      "loss: [605.95263671875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 460.1635\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [460.16351318359375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 254us/step - loss: 594.6085\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [594.6085205078125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 531.1971\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [531.1971435546875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 366.9027\n",
      "rewards:  -95.0 q-value:  0\n",
      "loss: [366.9027404785156]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 583.2036\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [583.20361328125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 465.0339\n",
      "rewards:  -69.0 q-value:  0\n",
      "loss: [465.0339050292969]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 559.5104\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [559.5103759765625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 432.4518\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [432.4517822265625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 334.4756\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [334.47564697265625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 662.5020\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [662.5020141601562]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 453.0725\n",
      "rewards:  -65.0 q-value:  0\n",
      "loss: [453.072509765625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 556.8791\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [556.8790893554688]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 331.5071\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [331.507080078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 273us/step - loss: 372.1562\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [372.1562194824219]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 494.6081\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [494.60809326171875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 405.6987\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [405.69873046875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 386.5688\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [386.56884765625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 78us/step - loss: 503.2350\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [503.23504638671875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 470.6209\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [470.6208801269531]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 496.5735\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [496.5734558105469]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 278.3986\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [278.3985595703125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 219us/step - loss: 466.2854\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [466.285400390625]\n",
      "Number of actions available 7\n",
      "Episode : 1\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 407.9885\n",
      "rewards:  -5 q-value:  0\n",
      "loss: [407.98846435546875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 481.2780\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [481.27801513671875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 388.0841\n",
      "rewards:  12.0 q-value:  0\n",
      "loss: [388.0841064453125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 553.1521\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [553.152099609375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 241.4576\n",
      "rewards:  12.0 q-value:  0\n",
      "loss: [241.45755004882812]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 577.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  7.0 q-value:  0\n",
      "loss: [577.791748046875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 359.1054\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [359.10540771484375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 222us/step - loss: 367.2470\n",
      "rewards:  25.0 q-value:  0\n",
      "loss: [367.2469787597656]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 307.2423\n",
      "rewards:  37.0 q-value:  0\n",
      "loss: [307.2422790527344]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 305.5494\n",
      "rewards:  63.0 q-value:  0\n",
      "loss: [305.54937744140625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 426.3515\n",
      "rewards:  36.0 q-value:  0\n",
      "loss: [426.3515319824219]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 419.0176\n",
      "rewards:  25.0 q-value:  0\n",
      "loss: [419.0176086425781]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 345.8371\n",
      "rewards:  33.0 q-value:  0\n",
      "loss: [345.83709716796875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 217us/step - loss: 407.5182\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [407.51824951171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 511.7125\n",
      "rewards:  12.0 q-value:  0\n",
      "loss: [511.7124938964844]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 380.3462\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [380.34619140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 435.5875\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [435.5874938964844]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 279.1960\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [279.19598388671875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 369.7368\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [369.7367858886719]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 229us/step - loss: 438.9285\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [438.928466796875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 247us/step - loss: 445.4272\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [445.42724609375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 342.8524\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [342.8524475097656]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 389.5003\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [389.5003356933594]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 449.3217\n",
      "rewards:  -53.0 q-value:  0\n",
      "loss: [449.3216857910156]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 337.8369\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [337.83685302734375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 339.6592\n",
      "rewards:  -46.0 q-value:  [[0.         0.         0.         0.         0.         0.32294858\n",
      "  0.         0.         0.         0.         0.         0.0558755\n",
      "  0.03836239 0.         0.         0.         0.08823328 0.11498456\n",
      "  0.         0.         0.        ]]\n",
      "loss: [339.6591796875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 317.6853\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [317.685302734375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 370.9631\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [370.963134765625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 289us/step - loss: 452.0033\n",
      "rewards:  -87.0 q-value:  0\n",
      "loss: [452.0032958984375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 420.0782\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [420.07818603515625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 425.9950\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [425.9949645996094]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 318.8185\n",
      "rewards:  -67.0 q-value:  0\n",
      "loss: [318.81854248046875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 246us/step - loss: 421.4185\n",
      "rewards:  -67.0 q-value:  0\n",
      "loss: [421.41845703125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 359.9985\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [359.99853515625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 414.9008\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [414.9007568359375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 398.4350\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [398.4349670410156]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 396.6988\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [396.69879150390625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 327.9466\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [327.9466247558594]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 412.3201\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [412.320068359375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 356.9538\n",
      "rewards:  -24.0 q-value:  0\n",
      "loss: [356.95379638671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 388.7512\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [388.75115966796875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 352.5161\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [352.51611328125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 396.0497\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [396.0496520996094]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 432.7724\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [432.7724304199219]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 211us/step - loss: 347.0489\n",
      "rewards:  -19.0 q-value:  0\n",
      "loss: [347.04888916015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 315.4869\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [315.4869384765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 245.5134\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [245.51341247558594]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 264us/step - loss: 407.2370\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [407.2370300292969]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 273.9652\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [273.9652099609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 550.1624\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [550.162353515625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 439.2941\n",
      "rewards:  -77.0 q-value:  0\n",
      "loss: [439.29412841796875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 251.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -80.0 q-value:  0\n",
      "loss: [251.00128173828125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 409.0840\n",
      "rewards:  -56.0 q-value:  0\n",
      "loss: [409.08404541015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 329.4883\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [329.48828125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 379.3337\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [379.33367919921875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 399.9736\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [399.9736328125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 396.6104\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [396.61041259765625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 340.7108\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [340.7107849121094]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 81us/step - loss: 352.6506\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [352.65057373046875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 302.4189\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [302.4189453125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 331.8226\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [331.82257080078125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 365.6163\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [365.6163330078125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 455.8885\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [455.8884582519531]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 310.1996\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [310.1995544433594]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 308.7498\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [308.7498474121094]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 382.4466\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [382.4466247558594]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 227.7744\n",
      "rewards:  -66.0 q-value:  0\n",
      "loss: [227.77438354492188]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 235us/step - loss: 473.7874\n",
      "rewards:  -71.0 q-value:  0\n",
      "loss: [473.78741455078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 192us/step - loss: 285.6046\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [285.6046142578125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 321us/step - loss: 298.7879\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [298.7879333496094]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 339.8762\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [339.876220703125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 385.1401\n",
      "rewards:  -92.0 q-value:  0\n",
      "loss: [385.14013671875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 331.4991\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [331.4991455078125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 442.9496\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [442.9495544433594]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 400.9062\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [400.90618896484375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 342.1173\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [342.1173095703125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 77us/step - loss: 382.1501\n",
      "rewards:  -77.0 q-value:  0\n",
      "loss: [382.15008544921875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 337.3654\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [337.3653564453125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 303.6348\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [303.634765625]\n",
      "Exploiting\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 460.0775\n",
      "rewards:  -132.0 q-value:  [[0.         0.         0.         0.         0.         0.3566862\n",
      "  0.         0.         0.         0.         0.         0.03164864\n",
      "  0.08555311 0.         0.         0.         0.12537025 0.13294797\n",
      "  0.         0.         0.        ]]\n",
      "loss: [460.0775146484375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 358.6642\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [358.6641540527344]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 333.3894\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [333.389404296875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 372.6119\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [372.61187744140625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 364.0909\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [364.09088134765625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 309.3249\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [309.32489013671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 460.2568\n",
      "rewards:  -113.0 q-value:  0\n",
      "loss: [460.25677490234375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 386.6838\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [386.6838073730469]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 593.1364\n",
      "rewards:  -92.0 q-value:  0\n",
      "loss: [593.1363525390625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 445.0315\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [445.0315246582031]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 507.5114\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [507.5113525390625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 488.0962\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [488.09619140625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 463.5651\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [463.5650939941406]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 381.0623\n",
      "rewards:  -53.0 q-value:  0\n",
      "loss: [381.0622863769531]\n",
      "Exploiting\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 364.5586\n",
      "rewards:  -56.0 q-value:  [[0.         0.         0.         0.         0.         0.58275884\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1088213  0.         0.         0.         0.06160046 0.332646\n",
      "  0.         0.         0.        ]]\n",
      "loss: [364.55859375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 428.6962\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [428.6961669921875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 388.0238\n",
      "rewards:  -65.0 q-value:  0\n",
      "loss: [388.0237731933594]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 426.0220\n",
      "rewards:  -70.0 q-value:  0\n",
      "loss: [426.0220031738281]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 352.2650\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [352.2650146484375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 583.6471\n",
      "rewards:  -107.0 q-value:  0\n",
      "loss: [583.6470947265625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 527.8972\n",
      "rewards:  -75.0 q-value:  0\n",
      "loss: [527.897216796875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 514.6439\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [514.6438598632812]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 428.7250\n",
      "rewards:  -75.0 q-value:  0\n",
      "loss: [428.7249755859375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 425.3858\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [425.3857727050781]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 332.1543\n",
      "rewards:  -93.0 q-value:  0\n",
      "loss: [332.1543273925781]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 453.3912\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [453.39117431640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 462.6750\n",
      "rewards:  -101.0 q-value:  0\n",
      "loss: [462.67498779296875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 391.0955\n",
      "rewards:  -82.0 q-value:  0\n",
      "loss: [391.095458984375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 268us/step - loss: 546.7473\n",
      "rewards:  -87.0 q-value:  [[0.         0.         0.         0.         0.         0.34217757\n",
      "  0.         0.         0.         0.         0.         0.01107758\n",
      "  0.08234762 0.         0.         0.         0.10322286 0.16317876\n",
      "  0.         0.         0.        ]]\n",
      "loss: [546.747314453125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 324.7350\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [324.7350158691406]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 480.2985\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [480.2984924316406]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 436.0390\n",
      "rewards:  -103.0 q-value:  0\n",
      "loss: [436.03900146484375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 461.1349\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [461.13494873046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 392.0553\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [392.0552978515625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 492.7140\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [492.7139892578125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 352.3333\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [352.33331298828125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 380.1695\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [380.1694641113281]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 574.3082\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [574.3082275390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 554.8517\n",
      "rewards:  -165.0 q-value:  0\n",
      "loss: [554.8517456054688]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 369.9325\n",
      "rewards:  -165.0 q-value:  0\n",
      "loss: [369.9324645996094]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 387.3681\n",
      "rewards:  -195.0 q-value:  0\n",
      "loss: [387.36810302734375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 351.8370\n",
      "rewards:  -210.0 q-value:  0\n",
      "loss: [351.83697509765625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 609.2702\n",
      "rewards:  -198.0 q-value:  0\n",
      "loss: [609.2702026367188]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 347.4721\n",
      "rewards:  -204.0 q-value:  0\n",
      "loss: [347.4720764160156]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 481.5764\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [481.576416015625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 216us/step - loss: 501.3651\n",
      "rewards:  -212.0 q-value:  0\n",
      "loss: [501.3651123046875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 721.1212\n",
      "rewards:  -222.0 q-value:  0\n",
      "loss: [721.1212158203125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 369.8238\n",
      "rewards:  -225.0 q-value:  0\n",
      "loss: [369.8237609863281]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 681.8961\n",
      "rewards:  -237.0 q-value:  0\n",
      "loss: [681.8961181640625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 700.5167\n",
      "rewards:  -225.0 q-value:  0\n",
      "loss: [700.5167236328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 681.9333\n",
      "rewards:  -246.0 q-value:  0\n",
      "loss: [681.933349609375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 870.4423\n",
      "rewards:  -219.0 q-value:  0\n",
      "loss: [870.4422607421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 883.2736\n",
      "rewards:  -233.0 q-value:  0\n",
      "loss: [883.2735595703125]\n",
      "Number of actions available 3\n",
      "Episode : 2\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 537.0116\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [537.0115966796875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 639.6720\n",
      "rewards:  -16.0 q-value:  0\n",
      "loss: [639.6719970703125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 750.2124\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [750.21240234375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 516.8198\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [516.81982421875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 676.4844\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [676.484375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 744.2290\n",
      "rewards:  -24.0 q-value:  0\n",
      "loss: [744.22900390625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 545.3871\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [545.3870849609375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 777.2509\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [777.2508544921875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 310.4589\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [310.45892333984375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 648.5897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -39.0 q-value:  0\n",
      "loss: [648.5897216796875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 806.7389\n",
      "rewards:  -40.0 q-value:  0\n",
      "loss: [806.7388916015625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 331us/step - loss: 773.5354\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [773.535400390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 643.4153\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [643.415283203125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 598.0669\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [598.06689453125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 748.4553\n",
      "rewards:  -56.0 q-value:  0\n",
      "loss: [748.455322265625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 419.2830\n",
      "rewards:  -66.0 q-value:  0\n",
      "loss: [419.282958984375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 342.5806\n",
      "rewards:  -74.0 q-value:  0\n",
      "loss: [342.58056640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 523.0784\n",
      "rewards:  -79.0 q-value:  0\n",
      "loss: [523.078369140625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 444.6088\n",
      "rewards:  -80.0 q-value:  0\n",
      "loss: [444.6087951660156]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 726us/step - loss: 570.9092\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [570.9091796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 663.1627\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [663.1627197265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 919.1672\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [919.1671752929688]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 883.8320\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [883.83203125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 604.0480\n",
      "rewards:  -103.0 q-value:  0\n",
      "loss: [604.0480346679688]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 821.0921\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [821.0921020507812]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 323.3939\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [323.39385986328125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 796.7579\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [796.7579345703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 320.1600\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [320.15997314453125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 299us/step - loss: 323.2750\n",
      "rewards:  -55.0 q-value:  [[0.         0.         0.         0.         0.         0.29793623\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05579024 0.         0.         0.         0.01358375 0.11403672\n",
      "  0.         0.         0.        ]]\n",
      "loss: [323.27496337890625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 469.3381\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [469.338134765625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 220us/step - loss: 816.0130\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [816.0130004882812]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 720.2917\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [720.2916870117188]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 512.6790\n",
      "rewards:  -65.0 q-value:  0\n",
      "loss: [512.678955078125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 482.0064\n",
      "rewards:  -70.0 q-value:  0\n",
      "loss: [482.0063781738281]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 239us/step - loss: 635.4473\n",
      "rewards:  -75.0 q-value:  0\n",
      "loss: [635.447265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 730.8546\n",
      "rewards:  -80.0 q-value:  0\n",
      "loss: [730.8546142578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 602us/step - loss: 867.9967\n",
      "rewards:  -85.0 q-value:  0\n",
      "loss: [867.9967041015625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 311us/step - loss: 688.4797\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [688.4796752929688]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 317us/step - loss: 700.2355\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [700.2355346679688]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 342.4510\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [342.45098876953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 1209.8494\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [1209.849365234375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 887.5096\n",
      "rewards:  -101.0 q-value:  0\n",
      "loss: [887.5096435546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 397.3462\n",
      "rewards:  -106.0 q-value:  0\n",
      "loss: [397.34619140625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 769.5417\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [769.541748046875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 545.5997\n",
      "rewards:  -110.0 q-value:  0\n",
      "loss: [545.5997314453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 800.0270\n",
      "rewards:  -115.0 q-value:  0\n",
      "loss: [800.0270385742188]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 595.2240\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [595.2239990234375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 661.3870\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [661.386962890625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 628.8996\n",
      "rewards:  -103.0 q-value:  0\n",
      "loss: [628.8995971679688]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 799.7672\n",
      "rewards:  -109.0 q-value:  0\n",
      "loss: [799.7671508789062]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 703.9393\n",
      "rewards:  -111.0 q-value:  0\n",
      "loss: [703.9393310546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 593.8513\n",
      "rewards:  -116.0 q-value:  0\n",
      "loss: [593.851318359375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 107us/step - loss: 458.7563\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [458.7562561035156]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 715.7688\n",
      "rewards:  -128.0 q-value:  0\n",
      "loss: [715.768798828125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 242us/step - loss: 429.9971\n",
      "rewards:  -158.0 q-value:  0\n",
      "loss: [429.9970703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 157us/step - loss: 514.3032\n",
      "rewards:  -163.0 q-value:  0\n",
      "loss: [514.30322265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 727.4567\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [727.4567260742188]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 538.0409\n",
      "rewards:  -198.0 q-value:  0\n",
      "loss: [538.0408935546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 492.1757\n",
      "rewards:  -203.0 q-value:  0\n",
      "loss: [492.1756591796875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 401.3698\n",
      "rewards:  -233.0 q-value:  0\n",
      "loss: [401.3697509765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 671.3292\n",
      "rewards:  -238.0 q-value:  0\n",
      "loss: [671.3292236328125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 616.4180\n",
      "rewards:  -228.0 q-value:  0\n",
      "loss: [616.41796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 535.7281\n",
      "rewards:  -233.0 q-value:  0\n",
      "loss: [535.7280883789062]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 859.2527\n",
      "rewards:  -238.0 q-value:  0\n",
      "loss: [859.252685546875]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 793.8160\n",
      "rewards:  -229.0 q-value:  [[0.         0.         0.         0.         0.         0.34437835\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03924166 0.         0.         0.         0.0516016  0.11003996\n",
      "  0.         0.03479169 0.        ]]\n",
      "loss: [793.8160400390625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 452.3809\n",
      "rewards:  -234.0 q-value:  0\n",
      "loss: [452.380859375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 233us/step - loss: 912.3321\n",
      "rewards:  -243.0 q-value:  0\n",
      "loss: [912.3320922851562]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 900.4020\n",
      "rewards:  -254.0 q-value:  0\n",
      "loss: [900.4019775390625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 880.2010\n",
      "rewards:  -258.0 q-value:  0\n",
      "loss: [880.2010498046875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 268us/step - loss: 840.5740\n",
      "rewards:  -262.0 q-value:  0\n",
      "loss: [840.573974609375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 721.5847\n",
      "rewards:  -264.0 q-value:  0\n",
      "loss: [721.5846557617188]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 445us/step - loss: 496.6730\n",
      "rewards:  -265.0 q-value:  0\n",
      "loss: [496.67303466796875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 800.4645\n",
      "rewards:  -267.0 q-value:  0\n",
      "loss: [800.4645385742188]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 769.8450\n",
      "rewards:  -272.0 q-value:  0\n",
      "loss: [769.8450317382812]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 226us/step - loss: 1053.5742\n",
      "rewards:  -274.0 q-value:  0\n",
      "loss: [1053.57421875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1129.0393\n",
      "rewards:  -276.0 q-value:  0\n",
      "loss: [1129.039306640625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 290us/step - loss: 957.0617\n",
      "rewards:  -278.0 q-value:  0\n",
      "loss: [957.0617065429688]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 755.9625\n",
      "rewards:  -283.0 q-value:  0\n",
      "loss: [755.9625244140625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 1247.8657\n",
      "rewards:  -273.0 q-value:  0\n",
      "loss: [1247.86572265625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 790.1555\n",
      "rewards:  -274.0 q-value:  0\n",
      "loss: [790.155517578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 1258.5298\n",
      "rewards:  -279.0 q-value:  0\n",
      "loss: [1258.52978515625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 939.8846\n",
      "rewards:  -275.0 q-value:  0\n",
      "loss: [939.8846435546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 227us/step - loss: 1233.9773\n",
      "rewards:  -280.0 q-value:  0\n",
      "loss: [1233.977294921875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1302.2838\n",
      "rewards:  -296.0 q-value:  0\n",
      "loss: [1302.2838134765625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 927.5143\n",
      "rewards:  -297.0 q-value:  0\n",
      "loss: [927.5143432617188]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 1439.0343\n",
      "rewards:  -298.0 q-value:  0\n",
      "loss: [1439.0343017578125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 1302.6205\n",
      "rewards:  -299.0 q-value:  0\n",
      "loss: [1302.6204833984375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 1428.5536\n",
      "rewards:  -307.0 q-value:  0\n",
      "loss: [1428.5535888671875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 975.6966\n",
      "rewards:  -308.0 q-value:  0\n",
      "loss: [975.6965942382812]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 321us/step - loss: 1167.9917\n",
      "rewards:  -315.0 q-value:  0\n",
      "loss: [1167.99169921875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 764.2275\n",
      "rewards:  -322.0 q-value:  0\n",
      "loss: [764.2274780273438]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 1150.9584\n",
      "rewards:  -323.0 q-value:  0\n",
      "loss: [1150.9583740234375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1749.0331\n",
      "rewards:  -333.0 q-value:  0\n",
      "loss: [1749.0330810546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 1350.3774\n",
      "rewards:  -338.0 q-value:  0\n",
      "loss: [1350.37744140625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 1077.0509\n",
      "rewards:  -302.0 q-value:  0\n",
      "loss: [1077.0509033203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 1949.5184\n",
      "rewards:  -307.0 q-value:  0\n",
      "loss: [1949.5184326171875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 1006.3696\n",
      "rewards:  -307.0 q-value:  0\n",
      "loss: [1006.3695678710938]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 2048.7046\n",
      "rewards:  -312.0 q-value:  0\n",
      "loss: [2048.70458984375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 922.3101\n",
      "rewards:  -324.0 q-value:  0\n",
      "loss: [922.3101196289062]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 1209.3140\n",
      "rewards:  -326.0 q-value:  0\n",
      "loss: [1209.31396484375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 241us/step - loss: 1349.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -326.0 q-value:  0\n",
      "loss: [1349.9732666015625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1436.4346\n",
      "rewards:  -328.0 q-value:  0\n",
      "loss: [1436.4345703125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 903.8947\n",
      "rewards:  -334.0 q-value:  0\n",
      "loss: [903.8946533203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 1280.9175\n",
      "rewards:  -339.0 q-value:  0\n",
      "loss: [1280.91748046875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 1131.5457\n",
      "rewards:  -324.0 q-value:  0\n",
      "loss: [1131.545654296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1906.8396\n",
      "rewards:  -329.0 q-value:  0\n",
      "loss: [1906.839599609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 1838.4366\n",
      "rewards:  -334.0 q-value:  0\n",
      "loss: [1838.4366455078125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 1493.2014\n",
      "rewards:  -333.0 q-value:  0\n",
      "loss: [1493.201416015625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 243us/step - loss: 1089.0679\n",
      "rewards:  -337.0 q-value:  0\n",
      "loss: [1089.06787109375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 1173.0762\n",
      "rewards:  -337.0 q-value:  0\n",
      "loss: [1173.076171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 1568.1508\n",
      "rewards:  -342.0 q-value:  0\n",
      "loss: [1568.1507568359375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 1278.6802\n",
      "rewards:  -344.0 q-value:  0\n",
      "loss: [1278.68017578125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 1229.9453\n",
      "rewards:  -346.0 q-value:  0\n",
      "loss: [1229.9453125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 1554.7140\n",
      "rewards:  -348.0 q-value:  0\n",
      "loss: [1554.7139892578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1498.1580\n",
      "rewards:  -353.0 q-value:  0\n",
      "loss: [1498.157958984375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1324.2069\n",
      "rewards:  -331.0 q-value:  0\n",
      "loss: [1324.2069091796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1285.4652\n",
      "rewards:  -336.0 q-value:  0\n",
      "loss: [1285.4652099609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 2267.3115\n",
      "rewards:  -341.0 q-value:  0\n",
      "loss: [2267.3115234375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 2008.6682\n",
      "rewards:  -331.0 q-value:  0\n",
      "loss: [2008.668212890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 1712.2456\n",
      "rewards:  -336.0 q-value:  0\n",
      "loss: [1712.24560546875]\n",
      "Number of actions available 8\n",
      "Episode : 3\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 805.2098\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [805.2097778320312]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 2045.9170\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [2045.9169921875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1909.7023\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [1909.7022705078125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 1524.7948\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [1524.7947998046875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 1389.7629\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [1389.762939453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 2576.6543\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [2576.654296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 1418.5076\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [1418.507568359375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 2132.7944\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [2132.79443359375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 1124.5370\n",
      "rewards:  -51.0 q-value:  0\n",
      "loss: [1124.5369873046875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 1430.0481\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [1430.048095703125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 222us/step - loss: 2117.4448\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [2117.44482421875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 2158.4531\n",
      "rewards:  14.0 q-value:  0\n",
      "loss: [2158.453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 1384.9686\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [1384.9686279296875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 2486.4854\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [2486.4853515625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1437.1807\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [1437.1806640625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 1832.8812\n",
      "rewards:  37.0 q-value:  0\n",
      "loss: [1832.8812255859375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1510.8228\n",
      "rewards:  32.0 q-value:  0\n",
      "loss: [1510.82275390625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 1434.6569\n",
      "rewards:  25.0 q-value:  0\n",
      "loss: [1434.6568603515625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 212us/step - loss: 1675.5173\n",
      "rewards:  25.0 q-value:  0\n",
      "loss: [1675.517333984375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1865.0867\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [1865.086669921875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 1846.5396\n",
      "rewards:  39.0 q-value:  0\n",
      "loss: [1846.53955078125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 1672.6797\n",
      "rewards:  67.0 q-value:  0\n",
      "loss: [1672.6796875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 2023.4495\n",
      "rewards:  59.0 q-value:  0\n",
      "loss: [2023.449462890625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 1206.0835\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [1206.08349609375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 248us/step - loss: 1493.6521\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [1493.652099609375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1567.3481\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [1567.34814453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 2483.5020\n",
      "rewards:  42.0 q-value:  0\n",
      "loss: [2483.501953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 2089.4673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  37.0 q-value:  0\n",
      "loss: [2089.46728515625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 1711.5359\n",
      "rewards:  41.0 q-value:  0\n",
      "loss: [1711.535888671875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 2009.9365\n",
      "rewards:  52.0 q-value:  0\n",
      "loss: [2009.9365234375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 1310.7788\n",
      "rewards:  68.0 q-value:  0\n",
      "loss: [1310.77880859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 2036.1338\n",
      "rewards:  63.0 q-value:  0\n",
      "loss: [2036.1337890625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 1852.6083\n",
      "rewards:  57.0 q-value:  0\n",
      "loss: [1852.6082763671875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 1024.7292\n",
      "rewards:  70.0 q-value:  0\n",
      "loss: [1024.729248046875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 833.9938\n",
      "rewards:  82.0 q-value:  0\n",
      "loss: [833.9938354492188]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 1123.7034\n",
      "rewards:  80.0 q-value:  0\n",
      "loss: [1123.703369140625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 2420.6206\n",
      "rewards:  52.0 q-value:  0\n",
      "loss: [2420.62060546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 1748.6099\n",
      "rewards:  32.0 q-value:  0\n",
      "loss: [1748.60986328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 1942.3420\n",
      "rewards:  40.0 q-value:  0\n",
      "loss: [1942.342041015625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 249us/step - loss: 1311.9229\n",
      "rewards:  30.0 q-value:  0\n",
      "loss: [1311.9228515625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 1073.7266\n",
      "rewards:  46.0 q-value:  0\n",
      "loss: [1073.7265625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 1941.6417\n",
      "rewards:  52.0 q-value:  0\n",
      "loss: [1941.6417236328125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 1281.0146\n",
      "rewards:  47.0 q-value:  0\n",
      "loss: [1281.0146484375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 1964.0049\n",
      "rewards:  54.0 q-value:  0\n",
      "loss: [1964.0048828125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 2044.9880\n",
      "rewards:  72.0 q-value:  0\n",
      "loss: [2044.988037109375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 3301.1919\n",
      "rewards:  72.0 q-value:  0\n",
      "loss: [3301.19189453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 1739.5879\n",
      "rewards:  72.0 q-value:  0\n",
      "loss: [1739.587890625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 1072.3595\n",
      "rewards:  100.0 q-value:  0\n",
      "loss: [1072.3594970703125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 2200.1382\n",
      "rewards:  134.0 q-value:  0\n",
      "loss: [2200.13818359375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 1648.3125\n",
      "rewards:  125.0 q-value:  0\n",
      "loss: [1648.3125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1321.8572\n",
      "rewards:  118.0 q-value:  0\n",
      "loss: [1321.857177734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 2025.4487\n",
      "rewards:  113.0 q-value:  0\n",
      "loss: [2025.44873046875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 1439.6174\n",
      "rewards:  126.0 q-value:  0\n",
      "loss: [1439.617431640625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 219us/step - loss: 2103.9727\n",
      "rewards:  150.0 q-value:  0\n",
      "loss: [2103.97265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 1713.6433\n",
      "rewards:  145.0 q-value:  0\n",
      "loss: [1713.643310546875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 1753.0066\n",
      "rewards:  145.0 q-value:  0\n",
      "loss: [1753.006591796875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 2715.2051\n",
      "rewards:  148.0 q-value:  0\n",
      "loss: [2715.205078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 1639.0413\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [1639.041259765625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 1614.7407\n",
      "rewards:  154.0 q-value:  0\n",
      "loss: [1614.74072265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 1179.4226\n",
      "rewards:  149.0 q-value:  0\n",
      "loss: [1179.422607421875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 1795.2810\n",
      "rewards:  147.0 q-value:  0\n",
      "loss: [1795.281005859375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 2074.6746\n",
      "rewards:  117.0 q-value:  0\n",
      "loss: [2074.674560546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 2496.0684\n",
      "rewards:  97.0 q-value:  0\n",
      "loss: [2496.068359375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 1954.3901\n",
      "rewards:  90.0 q-value:  0\n",
      "loss: [1954.39013671875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 292us/step - loss: 1205.7576\n",
      "rewards:  93.0 q-value:  0\n",
      "loss: [1205.757568359375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 1942.5674\n",
      "rewards:  100.0 q-value:  0\n",
      "loss: [1942.5673828125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 2580.3972\n",
      "rewards:  120.0 q-value:  0\n",
      "loss: [2580.397216796875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 1760.1790\n",
      "rewards:  116.0 q-value:  0\n",
      "loss: [1760.178955078125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 1980.5154\n",
      "rewards:  111.0 q-value:  0\n",
      "loss: [1980.515380859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 1251.0376\n",
      "rewards:  106.0 q-value:  0\n",
      "loss: [1251.03759765625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 1906.1506\n",
      "rewards:  110.0 q-value:  0\n",
      "loss: [1906.150634765625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 234us/step - loss: 2021.9204\n",
      "rewards:  121.0 q-value:  0\n",
      "loss: [2021.92041015625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 1904.2557\n",
      "rewards:  129.0 q-value:  0\n",
      "loss: [1904.2557373046875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 1709.2749\n",
      "rewards:  132.0 q-value:  0\n",
      "loss: [1709.27490234375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 1202.1254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  130.0 q-value:  0\n",
      "loss: [1202.1253662109375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1007.8177\n",
      "rewards:  138.0 q-value:  0\n",
      "loss: [1007.8177490234375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 1252.4088\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [1252.4088134765625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 2556.2056\n",
      "rewards:  118.0 q-value:  0\n",
      "loss: [2556.20556640625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 1938.9620\n",
      "rewards:  101.0 q-value:  0\n",
      "loss: [1938.9620361328125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 1896.8706\n",
      "rewards:  111.0 q-value:  0\n",
      "loss: [1896.87060546875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 1672.1406\n",
      "rewards:  108.0 q-value:  0\n",
      "loss: [1672.140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 2218.3979\n",
      "rewards:  103.0 q-value:  0\n",
      "loss: [2218.39794921875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 1221.8198\n",
      "rewards:  139.0 q-value:  0\n",
      "loss: [1221.81982421875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 1800.9038\n",
      "rewards:  137.0 q-value:  0\n",
      "loss: [1800.90380859375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 1264.9741\n",
      "rewards:  153.0 q-value:  0\n",
      "loss: [1264.97412109375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 1630.9415\n",
      "rewards:  177.0 q-value:  0\n",
      "loss: [1630.9415283203125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 2522.4136\n",
      "rewards:  183.0 q-value:  0\n",
      "loss: [2522.41357421875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 1743.2362\n",
      "rewards:  197.0 q-value:  0\n",
      "loss: [1743.2362060546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 250us/step - loss: 1155.7468\n",
      "rewards:  195.0 q-value:  0\n",
      "loss: [1155.746826171875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 1242.8949\n",
      "rewards:  159.0 q-value:  0\n",
      "loss: [1242.8948974609375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 273us/step - loss: 1744.5101\n",
      "rewards:  155.0 q-value:  0\n",
      "loss: [1744.5101318359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 1435.0339\n",
      "rewards:  150.0 q-value:  0\n",
      "loss: [1435.033935546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 818.6715\n",
      "rewards:  151.0 q-value:  0\n",
      "loss: [818.6715087890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 1050.6733\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [1050.67333984375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 1441.2142\n",
      "rewards:  130.0 q-value:  0\n",
      "loss: [1441.2142333984375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1747.7839\n",
      "rewards:  124.0 q-value:  0\n",
      "loss: [1747.783935546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 1400.3022\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [1400.30224609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 1820.7639\n",
      "rewards:  141.0 q-value:  0\n",
      "loss: [1820.763916015625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 1296.9792\n",
      "rewards:  138.0 q-value:  0\n",
      "loss: [1296.979248046875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 1131.7456\n",
      "rewards:  152.0 q-value:  0\n",
      "loss: [1131.74560546875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 1329.0161\n",
      "rewards:  148.0 q-value:  0\n",
      "loss: [1329.01611328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 1725.7212\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [1725.72119140625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 771.0734\n",
      "rewards:  154.0 q-value:  0\n",
      "loss: [771.0734252929688]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 241us/step - loss: 1592.7258\n",
      "rewards:  149.0 q-value:  0\n",
      "loss: [1592.725830078125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 231us/step - loss: 1829.3188\n",
      "rewards:  126.0 q-value:  0\n",
      "loss: [1829.31884765625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 192us/step - loss: 1662.4381\n",
      "rewards:  158.0 q-value:  0\n",
      "loss: [1662.4381103515625]\n",
      "Number of actions available 9\n",
      "Episode : 4\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 211us/step - loss: 1007.9843\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [1007.9842529296875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 862.6036\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [862.6036376953125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 1519.3921\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [1519.39208984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 1022.7754\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [1022.775390625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 1738.2036\n",
      "rewards:  -27.0 q-value:  0\n",
      "loss: [1738.20361328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 2368.8223\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [2368.822265625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 1974.8596\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [1974.859619140625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 1244.0273\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [1244.02734375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 1360.9058\n",
      "rewards:  -38.0 q-value:  [[0.         0.         0.         0.         0.         0.24603109\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0192894  0.         0.         0.01726266 0.         0.11005323\n",
      "  0.         0.03148265 0.        ]]\n",
      "loss: [1360.90576171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 1698.4279\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [1698.4278564453125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1339.3479\n",
      "rewards:  -19.0 q-value:  0\n",
      "loss: [1339.347900390625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 2629.4458\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [2629.44580078125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 1441.5237\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [1441.523681640625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 2112.0215\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [2112.021484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 131us/step - loss: 2092.4824\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [2092.482421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 879.9117\n",
      "rewards:  7.0 q-value:  0\n",
      "loss: [879.9117431640625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1648.1128\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [1648.11279296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 1271.9260\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [1271.926025390625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1350.8230\n",
      "rewards:  2.0 q-value:  0\n",
      "loss: [1350.822998046875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 1056.0652\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [1056.065185546875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 1345.2643\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [1345.2642822265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 1763.1248\n",
      "rewards:  -8.0 q-value:  0\n",
      "loss: [1763.124755859375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 462.6407\n",
      "rewards:  -24.0 q-value:  [[0.         0.         0.         0.         0.         0.51968837\n",
      "  0.         0.         0.         0.         0.         0.02113765\n",
      "  0.08826232 0.         0.         0.         0.09903321 0.19494376\n",
      "  0.         0.         0.        ]]\n",
      "loss: [462.6407470703125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 2047.6475\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [2047.6474609375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 268us/step - loss: 1672.4524\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [1672.452392578125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 2160.0273\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [2160.02734375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 981.2759\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [981.27587890625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 1125.6512\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [1125.6512451171875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 1190.4307\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [1190.4306640625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 1523.9421\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [1523.942138671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1850.9979\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [1850.9979248046875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1629.9224\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [1629.92236328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 1506.7288\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [1506.728759765625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 1475.3916\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [1475.3916015625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 835.9300\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [835.9299926757812]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 919.4000\n",
      "rewards:  -56.0 q-value:  0\n",
      "loss: [919.3999633789062]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 1451.5481\n",
      "rewards:  -56.0 q-value:  0\n",
      "loss: [1451.548095703125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 769.1733\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [769.17333984375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 1016.8016\n",
      "rewards:  -68.0 q-value:  0\n",
      "loss: [1016.8015747070312]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 1229.5614\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [1229.5614013671875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1388.1370\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [1388.136962890625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 2206.5923\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [2206.59228515625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 1656.8340\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [1656.833984375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 251us/step - loss: 1642.4703\n",
      "rewards:  -45.0 q-value:  0\n",
      "loss: [1642.4703369140625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1663.2517\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [1663.251708984375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 926.5848\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [926.5848388671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 719.1948\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [719.19482421875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 1265.5073\n",
      "rewards:  -78.0 q-value:  0\n",
      "loss: [1265.50732421875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 1207.2806\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [1207.2806396484375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 572.8539\n",
      "rewards:  -126.0 q-value:  0\n",
      "loss: [572.8538818359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 1099.0779\n",
      "rewards:  -128.0 q-value:  0\n",
      "loss: [1099.077880859375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 1353.9714\n",
      "rewards:  -130.0 q-value:  0\n",
      "loss: [1353.971435546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 1173.0576\n",
      "rewards:  -135.0 q-value:  0\n",
      "loss: [1173.0576171875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 1659.9384\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [1659.9383544921875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 1083.2932\n",
      "rewards:  -133.0 q-value:  0\n",
      "loss: [1083.293212890625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 1272.6868\n",
      "rewards:  -151.0 q-value:  0\n",
      "loss: [1272.686767578125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1797.1025\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [1797.1025390625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 1659.8806\n",
      "rewards:  -133.0 q-value:  0\n",
      "loss: [1659.880615234375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 862.5442\n",
      "rewards:  -114.0 q-value:  0\n",
      "loss: [862.544189453125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 1123.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -126.0 q-value:  0\n",
      "loss: [1123.5205078125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 1894.4313\n",
      "rewards:  -118.0 q-value:  0\n",
      "loss: [1894.4312744140625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 1243.5103\n",
      "rewards:  -146.0 q-value:  0\n",
      "loss: [1243.51025390625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 1751.6774\n",
      "rewards:  -162.0 q-value:  0\n",
      "loss: [1751.6773681640625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 229us/step - loss: 1712.5500\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [1712.550048828125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 1109.1272\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [1109.127197265625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 1659.2683\n",
      "rewards:  -162.0 q-value:  0\n",
      "loss: [1659.268310546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 1070.7367\n",
      "rewards:  -167.0 q-value:  0\n",
      "loss: [1070.7366943359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 1288.6268\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [1288.6268310546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 2081.4963\n",
      "rewards:  -173.0 q-value:  0\n",
      "loss: [2081.496337890625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 1968.8484\n",
      "rewards:  -174.0 q-value:  0\n",
      "loss: [1968.848388671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 1593.5417\n",
      "rewards:  -166.0 q-value:  0\n",
      "loss: [1593.541748046875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 1810.1370\n",
      "rewards:  -169.0 q-value:  0\n",
      "loss: [1810.136962890625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 1565.8292\n",
      "rewards:  -172.0 q-value:  0\n",
      "loss: [1565.8292236328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 2292.3433\n",
      "rewards:  -174.0 q-value:  0\n",
      "loss: [2292.34326171875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 1912.6470\n",
      "rewards:  -174.0 q-value:  0\n",
      "loss: [1912.64697265625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 1654.5167\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [1654.5167236328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 1443.0680\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [1443.0679931640625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1128.3408\n",
      "rewards:  -118.0 q-value:  [[0.         0.         0.         0.         0.         0.35146198\n",
      "  0.         0.         0.         0.         0.         0.02284801\n",
      "  0.05697512 0.         0.         0.02609559 0.00530413 0.15012258\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1128.3408203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1738.3938\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [1738.393798828125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 1406.6169\n",
      "rewards:  -130.0 q-value:  [[0.         0.         0.         0.         0.         0.3622407\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.07779339 0.         0.00376655 0.         0.04331584 0.1793533\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1406.616943359375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 1197.4523\n",
      "rewards:  -137.0 q-value:  0\n",
      "loss: [1197.4522705078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 2120.0693\n",
      "rewards:  -140.0 q-value:  0\n",
      "loss: [2120.0693359375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 1482.9490\n",
      "rewards:  -148.0 q-value:  0\n",
      "loss: [1482.948974609375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 973.0139\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [973.013916015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 1228.9976\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [1228.99755859375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 1329.2917\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [1329.291748046875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 2054.4551\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [2054.455078125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 2732.1958\n",
      "rewards:  -185.0 q-value:  0\n",
      "loss: [2732.19580078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 1607.0879\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [1607.087890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 2670.6858\n",
      "rewards:  -166.0 q-value:  0\n",
      "loss: [2670.685791015625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 232us/step - loss: 832.5271\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [832.527099609375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 1116.3234\n",
      "rewards:  -153.0 q-value:  0\n",
      "loss: [1116.3233642578125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 1043.7021\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [1043.7021484375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 1505.8527\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [1505.8526611328125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 1636.0215\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [1636.021484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 1385.9557\n",
      "rewards:  -189.0 q-value:  0\n",
      "loss: [1385.9556884765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 1366.0188\n",
      "rewards:  -194.0 q-value:  0\n",
      "loss: [1366.018798828125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 1063.7126\n",
      "rewards:  -194.0 q-value:  0\n",
      "loss: [1063.712646484375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 1635.6196\n",
      "rewards:  -186.0 q-value:  [[0.         0.         0.         0.         0.         0.33543587\n",
      "  0.         0.         0.         0.         0.         0.0040089\n",
      "  0.06363429 0.         0.         0.         0.09518912 0.09357339\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1635.61962890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 2279.2607\n",
      "rewards:  -191.0 q-value:  0\n",
      "loss: [2279.2607421875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 1312.5338\n",
      "rewards:  -189.0 q-value:  0\n",
      "loss: [1312.5338134765625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1932.8489\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [1932.848876953125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 100us/step - loss: 1664.3210\n",
      "rewards:  -211.0 q-value:  0\n",
      "loss: [1664.321044921875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1689.8738\n",
      "rewards:  -181.0 q-value:  0\n",
      "loss: [1689.873779296875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 2106.7773\n",
      "rewards:  -183.0 q-value:  0\n",
      "loss: [2106.77734375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 1221.0917\n",
      "rewards:  -177.0 q-value:  0\n",
      "loss: [1221.0916748046875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1819.5808\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [1819.580810546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 1369.6342\n",
      "rewards:  -183.0 q-value:  0\n",
      "loss: [1369.6341552734375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 1976.6519\n",
      "rewards:  -163.0 q-value:  0\n",
      "loss: [1976.65185546875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 1116.8833\n",
      "rewards:  -170.0 q-value:  0\n",
      "loss: [1116.88330078125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 1415.1909\n",
      "rewards:  -196.0 q-value:  0\n",
      "loss: [1415.19091796875]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 945.1601\n",
      "rewards:  -219.0 q-value:  [[0.         0.         0.         0.         0.         0.5382324\n",
      "  0.         0.         0.         0.         0.         0.0333662\n",
      "  0.12163956 0.         0.         0.         0.17628926 0.24008974\n",
      "  0.         0.         0.        ]]\n",
      "loss: [945.1600952148438]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 289us/step - loss: 1133.8705\n",
      "rewards:  -224.0 q-value:  0\n",
      "loss: [1133.8704833984375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1189.0669\n",
      "rewards:  -220.0 q-value:  0\n",
      "loss: [1189.06689453125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 1270.1334\n",
      "rewards:  -213.0 q-value:  0\n",
      "loss: [1270.1334228515625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 1906.9475\n",
      "rewards:  -197.0 q-value:  0\n",
      "loss: [1906.947509765625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 221us/step - loss: 2256.6558\n",
      "rewards:  -192.0 q-value:  [[0.         0.         0.         0.         0.         0.54629314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13857718 0.         0.         0.         0.12410083 0.272928\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2256.65576171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 1875.1870\n",
      "rewards:  -197.0 q-value:  0\n",
      "loss: [1875.18701171875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 993.7550\n",
      "rewards:  -200.0 q-value:  0\n",
      "loss: [993.7550048828125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 1605.1548\n",
      "rewards:  -176.0 q-value:  0\n",
      "loss: [1605.15478515625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 1978.2584\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [1978.2584228515625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 2604.0264\n",
      "rewards:  -179.0 q-value:  0\n",
      "loss: [2604.0263671875]\n",
      "Number of actions available 3\n",
      "Episode : 5\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 1150.0223\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [1150.0223388671875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 2195.5454\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [2195.54541015625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 1510.0537\n",
      "rewards:  -33.0 q-value:  0\n",
      "loss: [1510.0537109375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 1505.3738\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [1505.373779296875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 1735.8097\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [1735.8096923828125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 1672.4565\n",
      "rewards:  -113.0 q-value:  0\n",
      "loss: [1672.45654296875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 1277.6477\n",
      "rewards:  -118.0 q-value:  [[0.         0.         0.         0.         0.         0.43314654\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09163103 0.         0.         0.         0.09337872 0.19373406\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1277.647705078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 217us/step - loss: 1710.8375\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [1710.8375244140625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 1181.8114\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [1181.8114013671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 1303.3364\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [1303.33642578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 1424.7449\n",
      "rewards:  -137.0 q-value:  0\n",
      "loss: [1424.744873046875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 1362.1113\n",
      "rewards:  -144.0 q-value:  0\n",
      "loss: [1362.111328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 192us/step - loss: 1650.1143\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [1650.1142578125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 275us/step - loss: 1825.5986\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [1825.5986328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 1375.6698\n",
      "rewards:  -187.0 q-value:  0\n",
      "loss: [1375.6697998046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 2649.9360\n",
      "rewards:  -192.0 q-value:  0\n",
      "loss: [2649.93603515625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 1674.8893\n",
      "rewards:  -229.0 q-value:  0\n",
      "loss: [1674.8892822265625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 1999.6725\n",
      "rewards:  -234.0 q-value:  [[0.         0.         0.         0.         0.         0.3065027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04220063 0.         0.         0.         0.05232976 0.1536788\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1999.6724853515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 1186.9565\n",
      "rewards:  -239.0 q-value:  0\n",
      "loss: [1186.95654296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 1841.2234\n",
      "rewards:  -244.0 q-value:  0\n",
      "loss: [1841.223388671875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 1801.4158\n",
      "rewards:  -281.0 q-value:  0\n",
      "loss: [1801.415771484375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 2042.7998\n",
      "rewards:  -307.0 q-value:  0\n",
      "loss: [2042.7998046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 1495.6259\n",
      "rewards:  -312.0 q-value:  0\n",
      "loss: [1495.6258544921875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 1414.6923\n",
      "rewards:  -313.0 q-value:  0\n",
      "loss: [1414.6922607421875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 1469.0214\n",
      "rewards:  -339.0 q-value:  0\n",
      "loss: [1469.0213623046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 1382.2841\n",
      "rewards:  -344.0 q-value:  0\n",
      "loss: [1382.2840576171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 1067.1150\n",
      "rewards:  -349.0 q-value:  0\n",
      "loss: [1067.114990234375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 2146.2429\n",
      "rewards:  -375.0 q-value:  0\n",
      "loss: [2146.242919921875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 1014.6390\n",
      "rewards:  -376.0 q-value:  0\n",
      "loss: [1014.6390380859375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 1586.5991\n",
      "rewards:  -402.0 q-value:  0\n",
      "loss: [1586.59912109375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 1323.9982\n",
      "rewards:  -409.0 q-value:  0\n",
      "loss: [1323.9981689453125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 2037.2297\n",
      "rewards:  -409.0 q-value:  0\n",
      "loss: [2037.229736328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 1498.5557\n",
      "rewards:  -414.0 q-value:  0\n",
      "loss: [1498.5556640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 394us/step - loss: 2010.9060\n",
      "rewards:  -427.0 q-value:  0\n",
      "loss: [2010.906005859375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 941.5732\n",
      "rewards:  -437.0 q-value:  0\n",
      "loss: [941.5732421875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 1235.7368\n",
      "rewards:  -450.0 q-value:  0\n",
      "loss: [1235.73681640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 2301.5635\n",
      "rewards:  -455.0 q-value:  0\n",
      "loss: [2301.5634765625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 2167.2874\n",
      "rewards:  -478.0 q-value:  0\n",
      "loss: [2167.287353515625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 1922.1593\n",
      "rewards:  -485.0 q-value:  0\n",
      "loss: [1922.1593017578125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 3119.6431\n",
      "rewards:  -489.0 q-value:  0\n",
      "loss: [3119.64306640625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 2299.7783\n",
      "rewards:  -519.0 q-value:  0\n",
      "loss: [2299.7783203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 244us/step - loss: 1596.7671\n",
      "rewards:  -524.0 q-value:  0\n",
      "loss: [1596.76708984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 2059.4375\n",
      "rewards:  -529.0 q-value:  0\n",
      "loss: [2059.4375]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 1230.8173\n",
      "rewards:  -533.0 q-value:  [[0.         0.         0.         0.         0.         0.32715702\n",
      "  0.00879136 0.         0.         0.         0.         0.\n",
      "  0.06785446 0.         0.         0.02841322 0.06637429 0.15801595\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1230.8172607421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 2875.7227\n",
      "rewards:  -538.0 q-value:  0\n",
      "loss: [2875.72265625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 3152.4043\n",
      "rewards:  -544.0 q-value:  0\n",
      "loss: [3152.404296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 1940.4399\n",
      "rewards:  -549.0 q-value:  0\n",
      "loss: [1940.43994140625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 2463.8652\n",
      "rewards:  -550.0 q-value:  0\n",
      "loss: [2463.865234375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 3291.8811\n",
      "rewards:  -551.0 q-value:  0\n",
      "loss: [3291.881103515625]\n",
      "Exploiting\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 1616.5430\n",
      "rewards:  -552.0 q-value:  [[0.         0.         0.         0.         0.         0.36944294\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06903709 0.         0.         0.         0.07951924 0.13983117\n",
      "  0.         0.0320149  0.        ]]\n",
      "loss: [1616.54296875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 214us/step - loss: 2745.9941\n",
      "rewards:  -567.0 q-value:  0\n",
      "loss: [2745.994140625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 233us/step - loss: 2267.4331\n",
      "rewards:  -570.0 q-value:  0\n",
      "loss: [2267.43310546875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 3340.1042\n",
      "rewards:  -579.0 q-value:  0\n",
      "loss: [3340.104248046875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 2418.0022\n",
      "rewards:  -590.0 q-value:  0\n",
      "loss: [2418.002197265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 3197.5122\n",
      "rewards:  -595.0 q-value:  0\n",
      "loss: [3197.51220703125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 2218.8499\n",
      "rewards:  -618.0 q-value:  0\n",
      "loss: [2218.849853515625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 1646.4088\n",
      "rewards:  -626.0 q-value:  0\n",
      "loss: [1646.4088134765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 2693.4048\n",
      "rewards:  -631.0 q-value:  0\n",
      "loss: [2693.40478515625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 2546.6487\n",
      "rewards:  -615.0 q-value:  0\n",
      "loss: [2546.648681640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 3694.6797\n",
      "rewards:  -620.0 q-value:  0\n",
      "loss: [3694.6796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 1524.2661\n",
      "rewards:  -625.0 q-value:  0\n",
      "loss: [1524.26611328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 3312.7256\n",
      "rewards:  -626.0 q-value:  0\n",
      "loss: [3312.7255859375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 5027.4766\n",
      "rewards:  -627.0 q-value:  0\n",
      "loss: [5027.4765625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 1188.9639\n",
      "rewards:  -636.0 q-value:  0\n",
      "loss: [1188.9638671875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 1814.4954\n",
      "rewards:  -640.0 q-value:  0\n",
      "loss: [1814.495361328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 2070.7458\n",
      "rewards:  -642.0 q-value:  0\n",
      "loss: [2070.745849609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 5063.6455\n",
      "rewards:  -644.0 q-value:  0\n",
      "loss: [5063.6455078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 3868.0679\n",
      "rewards:  -649.0 q-value:  0\n",
      "loss: [3868.06787109375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 3591.5164\n",
      "rewards:  -661.0 q-value:  0\n",
      "loss: [3591.516357421875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 224us/step - loss: 2835.3496\n",
      "rewards:  -687.0 q-value:  0\n",
      "loss: [2835.349609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 4895.2090\n",
      "rewards:  -699.0 q-value:  0\n",
      "loss: [4895.208984375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 320us/step - loss: 3346.6943\n",
      "rewards:  -701.0 q-value:  0\n",
      "loss: [3346.6943359375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 2102.1328\n",
      "rewards:  -705.0 q-value:  0\n",
      "loss: [2102.1328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 1572.3016\n",
      "rewards:  -709.0 q-value:  0\n",
      "loss: [1572.3016357421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 1452.2368\n",
      "rewards:  -714.0 q-value:  0\n",
      "loss: [1452.23681640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 3204.5996\n",
      "rewards:  -719.0 q-value:  0\n",
      "loss: [3204.599609375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 7606.6611\n",
      "rewards:  -722.0 q-value:  0\n",
      "loss: [7606.6611328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 4944.2891\n",
      "rewards:  -727.0 q-value:  0\n",
      "loss: [4944.2890625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 3763.2368\n",
      "rewards:  -737.0 q-value:  0\n",
      "loss: [3763.23681640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 3239.7805\n",
      "rewards:  -742.0 q-value:  0\n",
      "loss: [3239.780517578125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 4646.4902\n",
      "rewards:  -743.0 q-value:  0\n",
      "loss: [4646.490234375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 4217.2246\n",
      "rewards:  -753.0 q-value:  0\n",
      "loss: [4217.224609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 5418.6465\n",
      "rewards:  -753.0 q-value:  0\n",
      "loss: [5418.646484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 2883.4602\n",
      "rewards:  -758.0 q-value:  0\n",
      "loss: [2883.460205078125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 4193.2827\n",
      "rewards:  -759.0 q-value:  0\n",
      "loss: [4193.28271484375]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 220us/step - loss: 2571.9043\n",
      "rewards:  -769.0 q-value:  [[0.         0.         0.         0.         0.         0.39491954\n",
      "  0.         0.         0.         0.         0.         0.00045032\n",
      "  0.08554073 0.         0.         0.         0.09521912 0.17120144\n",
      "  0.         0.00283902 0.        ]]\n",
      "loss: [2571.904296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 3623.6099\n",
      "rewards:  -774.0 q-value:  0\n",
      "loss: [3623.60986328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 84us/step - loss: 3105.3564\n",
      "rewards:  -775.0 q-value:  0\n",
      "loss: [3105.3564453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 4935.5928\n",
      "rewards:  -780.0 q-value:  0\n",
      "loss: [4935.5927734375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 1659.8718\n",
      "rewards:  -781.0 q-value:  0\n",
      "loss: [1659.871826171875]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 4123.1475\n",
      "rewards:  -791.0 q-value:  [[0.         0.         0.         0.         0.         0.36083466\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02996193 0.         0.         0.         0.00698611 0.16596395\n",
      "  0.         0.0310515  0.        ]]\n",
      "loss: [4123.1474609375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 3626.2178\n",
      "rewards:  -802.0 q-value:  0\n",
      "loss: [3626.2177734375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 5666.1406\n",
      "rewards:  -803.0 q-value:  0\n",
      "loss: [5666.140625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 3436.4182\n",
      "rewards:  -811.0 q-value:  0\n",
      "loss: [3436.418212890625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 5315.7725\n",
      "rewards:  -812.0 q-value:  0\n",
      "loss: [5315.7724609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 5246.5010\n",
      "rewards:  -817.0 q-value:  0\n",
      "loss: [5246.5009765625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 8789.3457\n",
      "rewards:  -818.0 q-value:  0\n",
      "loss: [8789.345703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 3685.9849\n",
      "rewards:  -823.0 q-value:  0\n",
      "loss: [3685.98486328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 5263.8760\n",
      "rewards:  -828.0 q-value:  0\n",
      "loss: [5263.8759765625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 4116.1543\n",
      "rewards:  -818.0 q-value:  0\n",
      "loss: [4116.154296875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 2871.2896\n",
      "rewards:  -849.0 q-value:  0\n",
      "loss: [2871.28955078125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 7302.2588\n",
      "rewards:  -890.0 q-value:  0\n",
      "loss: [7302.2587890625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 6565.3208\n",
      "rewards:  -921.0 q-value:  0\n",
      "loss: [6565.32080078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 6457.9043\n",
      "rewards:  -926.0 q-value:  0\n",
      "loss: [6457.904296875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 2682.3506\n",
      "rewards:  -929.0 q-value:  0\n",
      "loss: [2682.3505859375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 3306.7603\n",
      "rewards:  -932.0 q-value:  0\n",
      "loss: [3306.76025390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 8985.0879\n",
      "rewards:  -937.0 q-value:  0\n",
      "loss: [8985.087890625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 2720.2788\n",
      "rewards:  -943.0 q-value:  0\n",
      "loss: [2720.27880859375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 7876.9053\n",
      "rewards:  -944.0 q-value:  0\n",
      "loss: [7876.9052734375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 9288.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -935.0 q-value:  0\n",
      "loss: [9288.0166015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 5048.6484\n",
      "rewards:  -940.0 q-value:  0\n",
      "loss: [5048.6484375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 9211.7617\n",
      "rewards:  -967.0 q-value:  0\n",
      "loss: [9211.76171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 2399.3975\n",
      "rewards:  -972.0 q-value:  0\n",
      "loss: [2399.3974609375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 4121.7231\n",
      "rewards:  -973.0 q-value:  0\n",
      "loss: [4121.72314453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 6443.9746\n",
      "rewards:  -978.0 q-value:  0\n",
      "loss: [6443.974609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 7986.3252\n",
      "rewards:  -1005.0 q-value:  0\n",
      "loss: [7986.3251953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 8360.2871\n",
      "rewards:  -1010.0 q-value:  0\n",
      "loss: [8360.287109375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 7491.3955\n",
      "rewards:  -1011.0 q-value:  0\n",
      "loss: [7491.3955078125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 6839.3179\n",
      "rewards:  -1012.0 q-value:  0\n",
      "loss: [6839.31787109375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 6785.2832\n",
      "rewards:  -1013.0 q-value:  0\n",
      "loss: [6785.283203125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 6503.5103\n",
      "rewards:  -1040.0 q-value:  0\n",
      "loss: [6503.51025390625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 1780.6156\n",
      "rewards:  -1042.0 q-value:  0\n",
      "loss: [1780.6156005859375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 4703.0742\n",
      "rewards:  -1044.0 q-value:  0\n",
      "loss: [4703.07421875]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 9074.4531\n",
      "rewards:  -1046.0 q-value:  [[0.         0.         0.         0.         0.         0.43770894\n",
      "  0.         0.         0.         0.         0.         0.02441718\n",
      "  0.09767295 0.         0.         0.         0.13704985 0.15269822\n",
      "  0.         0.         0.        ]]\n",
      "loss: [9074.453125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 13229.3320\n",
      "rewards:  -1040.0 q-value:  0\n",
      "loss: [13229.33203125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 5126.2271\n",
      "rewards:  -1035.0 q-value:  0\n",
      "loss: [5126.22705078125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 12984.6641\n",
      "rewards:  -1053.0 q-value:  0\n",
      "loss: [12984.6640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 5906.5264\n",
      "rewards:  -1058.0 q-value:  0\n",
      "loss: [5906.5263671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 226us/step - loss: 6672.4639\n",
      "rewards:  -1063.0 q-value:  0\n",
      "loss: [6672.4638671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 9880.1113\n",
      "rewards:  -1068.0 q-value:  0\n",
      "loss: [9880.111328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 8895.2910\n",
      "rewards:  -1073.0 q-value:  0\n",
      "loss: [8895.291015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 107us/step - loss: 9591.6113\n",
      "rewards:  -1078.0 q-value:  0\n",
      "loss: [9591.611328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 4548.5762\n",
      "rewards:  -1081.0 q-value:  0\n",
      "loss: [4548.576171875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 7005.7661\n",
      "rewards:  -1107.0 q-value:  0\n",
      "loss: [7005.76611328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 9514.5273\n",
      "rewards:  -1112.0 q-value:  0\n",
      "loss: [9514.52734375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 9538.0078\n",
      "rewards:  -1138.0 q-value:  0\n",
      "loss: [9538.0078125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 9796.2461\n",
      "rewards:  -1139.0 q-value:  0\n",
      "loss: [9796.24609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 12250.8516\n",
      "rewards:  -1140.0 q-value:  0\n",
      "loss: [12250.8515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 4714.0400\n",
      "rewards:  -1145.0 q-value:  0\n",
      "loss: [4714.0400390625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 222us/step - loss: 8521.7031\n",
      "rewards:  -1171.0 q-value:  0\n",
      "loss: [8521.703125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 13581.0371\n",
      "rewards:  -1172.0 q-value:  0\n",
      "loss: [13581.037109375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 17247.1328\n",
      "rewards:  -1177.0 q-value:  0\n",
      "loss: [17247.1328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 13682.1758\n",
      "rewards:  -1184.0 q-value:  0\n",
      "loss: [13682.17578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 10718.9883\n",
      "rewards:  -1189.0 q-value:  0\n",
      "loss: [10718.98828125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 11607.2051\n",
      "rewards:  -1194.0 q-value:  0\n",
      "loss: [11607.205078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 7238.2666\n",
      "rewards:  -1199.0 q-value:  0\n",
      "loss: [7238.2666015625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 9643.3477\n",
      "rewards:  -1212.0 q-value:  0\n",
      "loss: [9643.34765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 6566.0029\n",
      "rewards:  -1217.0 q-value:  0\n",
      "loss: [6566.0029296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 13167.2822\n",
      "rewards:  -1222.0 q-value:  0\n",
      "loss: [13167.2822265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 16164.6074\n",
      "rewards:  -1227.0 q-value:  0\n",
      "loss: [16164.607421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 14234.2363\n",
      "rewards:  -1232.0 q-value:  0\n",
      "loss: [14234.236328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 15724.4746\n",
      "rewards:  -1255.0 q-value:  0\n",
      "loss: [15724.474609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 3912.4204\n",
      "rewards:  -1258.0 q-value:  0\n",
      "loss: [3912.42041015625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 27047.5547\n",
      "rewards:  -1262.0 q-value:  0\n",
      "loss: [27047.5546875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 9003.8672\n",
      "rewards:  -1266.0 q-value:  0\n",
      "loss: [9003.8671875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 12462.1807\n",
      "rewards:  -1267.0 q-value:  0\n",
      "loss: [12462.1806640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 7476.5371\n",
      "rewards:  -1268.0 q-value:  0\n",
      "loss: [7476.537109375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 9534.0820\n",
      "rewards:  -1269.0 q-value:  0\n",
      "loss: [9534.08203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 12396.7402\n",
      "rewards:  -1274.0 q-value:  0\n",
      "loss: [12396.740234375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 19556.5000\n",
      "rewards:  -1280.0 q-value:  0\n",
      "loss: [19556.5]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 17115.3105\n",
      "rewards:  -1285.0 q-value:  0\n",
      "loss: [17115.310546875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 8237.2070\n",
      "rewards:  -1286.0 q-value:  0\n",
      "loss: [8237.20703125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 13666.4941\n",
      "rewards:  -1286.0 q-value:  0\n",
      "loss: [13666.494140625]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 9207.4707\n",
      "rewards:  -1314.0 q-value:  [[0.         0.         0.         0.         0.         0.38581082\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08366946 0.         0.00248015 0.         0.06781565 0.10492323\n",
      "  0.         0.04606693 0.        ]]\n",
      "loss: [9207.470703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 8928.4297\n",
      "rewards:  -1319.0 q-value:  0\n",
      "loss: [8928.4296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 14162.2324\n",
      "rewards:  -1324.0 q-value:  0\n",
      "loss: [14162.232421875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 12131.6230\n",
      "rewards:  -1324.0 q-value:  0\n",
      "loss: [12131.623046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 10091.4541\n",
      "rewards:  -1329.0 q-value:  0\n",
      "loss: [10091.4541015625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 13480.4219\n",
      "rewards:  -1357.0 q-value:  0\n",
      "loss: [13480.421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 239us/step - loss: 10101.0039\n",
      "rewards:  -1362.0 q-value:  0\n",
      "loss: [10101.00390625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 12321.4355\n",
      "rewards:  -1366.0 q-value:  0\n",
      "loss: [12321.435546875]\n",
      "Number of actions available 12\n",
      "Episode : 6\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 13439.3740\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [13439.3740234375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 263us/step - loss: 14405.5713\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [14405.5712890625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 13095.2031\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [13095.203125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 10356.1416\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [10356.1416015625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 20299.8906\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [20299.890625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 12131.3047\n",
      "rewards:  -51.0 q-value:  0\n",
      "loss: [12131.3046875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 14711.9268\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [14711.9267578125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 9667.2734\n",
      "rewards:  -66.0 q-value:  0\n",
      "loss: [9667.2734375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 10652.1602\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [10652.16015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 14338.6562\n",
      "rewards:  -65.0 q-value:  0\n",
      "loss: [14338.65625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 6576.6914\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [6576.69140625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 14511.8369\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [14511.8369140625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 9631.9375\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [9631.9375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 19281.3887\n",
      "rewards:  -10.0 q-value:  0\n",
      "loss: [19281.388671875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 5851.8755\n",
      "rewards:  2.0 q-value:  0\n",
      "loss: [5851.87548828125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 16525.8438\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [16525.84375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 18100.5703\n",
      "rewards:  11.0 q-value:  0\n",
      "loss: [18100.5703125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 12262.3750\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [12262.375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 16013.0498\n",
      "rewards:  3.0 q-value:  0\n",
      "loss: [16013.0498046875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 15030.8457\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [15030.845703125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 13224.5430\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [13224.54296875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 14841.9727\n",
      "rewards:  -51.0 q-value:  0\n",
      "loss: [14841.97265625]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 18478.2148\n",
      "rewards:  -47.0 q-value:  [[0.         0.         0.         0.         0.         0.39596346\n",
      "  0.         0.         0.         0.         0.         0.04621534\n",
      "  0.06799196 0.         0.         0.         0.12057351 0.16647929\n",
      "  0.         0.         0.        ]]\n",
      "loss: [18478.21484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 82us/step - loss: 23642.2070\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [23642.20703125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 21167.5000\n",
      "rewards:  -94.0 q-value:  0\n",
      "loss: [21167.5]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 221us/step - loss: 20694.4102\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [20694.41015625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 19478.7383\n",
      "rewards:  -90.0 q-value:  0\n",
      "loss: [19478.73828125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 13156.0322\n",
      "rewards:  -114.0 q-value:  0\n",
      "loss: [13156.0322265625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 17443.6055\n",
      "rewards:  -119.0 q-value:  0\n",
      "loss: [17443.60546875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 4112.1299\n",
      "rewards:  -100.0 q-value:  0\n",
      "loss: [4112.1298828125]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 10053.0361\n",
      "rewards:  -109.0 q-value:  [[0.         0.         0.         0.         0.         0.4704502\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.07857762 0.         0.         0.         0.10673242 0.15297358\n",
      "  0.         0.04857763 0.        ]]\n",
      "loss: [10053.0361328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 16113.8867\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [16113.88671875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 17444.2578\n",
      "rewards:  -151.0 q-value:  0\n",
      "loss: [17444.2578125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 10673.5205\n",
      "rewards:  -154.0 q-value:  0\n",
      "loss: [10673.5205078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 3551.8816\n",
      "rewards:  -159.0 q-value:  0\n",
      "loss: [3551.881591796875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 5620.9434\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [5620.943359375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 11286.0469\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [11286.046875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 10297.0439\n",
      "rewards:  -159.0 q-value:  0\n",
      "loss: [10297.0439453125]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 210us/step - loss: 10155.6758\n",
      "rewards:  -185.0 q-value:  [[0.         0.         0.         0.         0.         0.47973564\n",
      "  0.         0.         0.         0.         0.         0.05097383\n",
      "  0.07209069 0.         0.00719659 0.         0.1188417  0.26370078\n",
      "  0.         0.         0.        ]]\n",
      "loss: [10155.67578125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 20334.9551\n",
      "rewards:  -207.0 q-value:  0\n",
      "loss: [20334.955078125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 13757.5576\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [13757.5576171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 11811.1611\n",
      "rewards:  -197.0 q-value:  0\n",
      "loss: [11811.1611328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 9812.3965\n",
      "rewards:  -181.0 q-value:  0\n",
      "loss: [9812.396484375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 8446.4531\n",
      "rewards:  -185.0 q-value:  0\n",
      "loss: [8446.453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 23026.1973\n",
      "rewards:  -186.0 q-value:  0\n",
      "loss: [23026.197265625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 15848.7031\n",
      "rewards:  -192.0 q-value:  0\n",
      "loss: [15848.703125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 9689.8799\n",
      "rewards:  -202.0 q-value:  0\n",
      "loss: [9689.8798828125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 8237.9990\n",
      "rewards:  -203.0 q-value:  0\n",
      "loss: [8237.9990234375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 14054.7344\n",
      "rewards:  -179.0 q-value:  0\n",
      "loss: [14054.734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 15040.1367\n",
      "rewards:  -184.0 q-value:  0\n",
      "loss: [15040.13671875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 10234.6992\n",
      "rewards:  -188.0 q-value:  0\n",
      "loss: [10234.69921875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 17977.6680\n",
      "rewards:  -189.0 q-value:  0\n",
      "loss: [17977.66796875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 10300.2207\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [10300.220703125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 7758.5415\n",
      "rewards:  -156.0 q-value:  0\n",
      "loss: [7758.54150390625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 7142.8154\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [7142.8154296875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 4458.9849\n",
      "rewards:  -158.0 q-value:  0\n",
      "loss: [4458.98486328125]\n",
      "Exploiting\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 14075.7461\n",
      "rewards:  -163.0 q-value:  [[0.         0.         0.         0.         0.         0.3344695\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04102929 0.         0.         0.         0.01190085 0.08947996\n",
      "  0.         0.00043836 0.        ]]\n",
      "loss: [14075.74609375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 13798.4980\n",
      "rewards:  -139.0 q-value:  0\n",
      "loss: [13798.498046875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 12326.5898\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [12326.58984375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 10363.2285\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [10363.228515625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 14109.2100\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [14109.2099609375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 12550.0713\n",
      "rewards:  -129.0 q-value:  0\n",
      "loss: [12550.0712890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 5862.2124\n",
      "rewards:  -135.0 q-value:  0\n",
      "loss: [5862.21240234375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 8927.4551\n",
      "rewards:  -119.0 q-value:  0\n",
      "loss: [8927.455078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 20004.4883\n",
      "rewards:  -124.0 q-value:  0\n",
      "loss: [20004.48828125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 15828.0479\n",
      "rewards:  -134.0 q-value:  0\n",
      "loss: [15828.0478515625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 18064.9160\n",
      "rewards:  -152.0 q-value:  0\n",
      "loss: [18064.916015625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 4672.1865\n",
      "rewards:  -177.0 q-value:  0\n",
      "loss: [4672.1865234375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 10896.4141\n",
      "rewards:  -165.0 q-value:  0\n",
      "loss: [10896.4140625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 9291.0879\n",
      "rewards:  -172.0 q-value:  0\n",
      "loss: [9291.087890625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 17188.9570\n",
      "rewards:  -177.0 q-value:  0\n",
      "loss: [17188.95703125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 15332.1182\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [15332.1181640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 15560.6738\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [15560.673828125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 6780.6279\n",
      "rewards:  -141.0 q-value:  0\n",
      "loss: [6780.6279296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 262us/step - loss: 15957.9043\n",
      "rewards:  -146.0 q-value:  0\n",
      "loss: [15957.904296875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 10301.0137\n",
      "rewards:  -143.0 q-value:  0\n",
      "loss: [10301.013671875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 9711.6289\n",
      "rewards:  -147.0 q-value:  0\n",
      "loss: [9711.62890625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 10928.4785\n",
      "rewards:  -147.0 q-value:  0\n",
      "loss: [10928.478515625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 8230.8662\n",
      "rewards:  -151.0 q-value:  0\n",
      "loss: [8230.8662109375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 13998.8594\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [13998.859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 2256.2588\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [2256.2587890625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 21474.1680\n",
      "rewards:  -144.0 q-value:  0\n",
      "loss: [21474.16796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 5161.9072\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [5161.9072265625]\n",
      "Exploiting\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 11350.2852\n",
      "rewards:  -121.0 q-value:  [[0.         0.         0.         0.         0.         0.29143775\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0407974  0.         0.         0.         0.03754707 0.11879759\n",
      "  0.         0.         0.        ]]\n",
      "loss: [11350.28515625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 5199.7349\n",
      "rewards:  -139.0 q-value:  0\n",
      "loss: [5199.73486328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 15000.5703\n",
      "rewards:  -115.0 q-value:  0\n",
      "loss: [15000.5703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 20824.9746\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [20824.974609375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 17059.1348\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [17059.134765625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 107us/step - loss: 9478.2988\n",
      "rewards:  -106.0 q-value:  0\n",
      "loss: [9478.298828125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 2913.8184\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [2913.818359375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 5881.1050\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [5881.10498046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 6889.4893\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [6889.4892578125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 10615.5605\n",
      "rewards:  -119.0 q-value:  0\n",
      "loss: [10615.560546875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 13819.5342\n",
      "rewards:  -141.0 q-value:  0\n",
      "loss: [13819.5341796875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 6377.0938\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [6377.09375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 14066.6240\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [14066.6240234375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 20240.7773\n",
      "rewards:  -143.0 q-value:  0\n",
      "loss: [20240.77734375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 8323.8428\n",
      "rewards:  -119.0 q-value:  0\n",
      "loss: [8323.8427734375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 12402.2891\n",
      "rewards:  -95.0 q-value:  0\n",
      "loss: [12402.2890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 12405.0273\n",
      "rewards:  -105.0 q-value:  0\n",
      "loss: [12405.02734375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 16507.4414\n",
      "rewards:  -105.0 q-value:  0\n",
      "loss: [16507.44140625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 212us/step - loss: 14517.2305\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [14517.23046875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 19741.8359\n",
      "rewards:  -113.0 q-value:  0\n",
      "loss: [19741.8359375]\n",
      "Exploiting\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 10479.0898\n",
      "rewards:  -106.0 q-value:  [[0.         0.         0.         0.         0.         0.43471104\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08378413 0.         0.         0.         0.07959609 0.17669404\n",
      "  0.         0.04391154 0.        ]]\n",
      "loss: [10479.08984375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 6918.7344\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [6918.734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 8119.7710\n",
      "rewards:  -103.0 q-value:  0\n",
      "loss: [8119.77099609375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 3071.7007\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [3071.70068359375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 19359.7617\n",
      "rewards:  -75.0 q-value:  0\n",
      "loss: [19359.76171875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 10092.9834\n",
      "rewards:  -79.0 q-value:  0\n",
      "loss: [10092.9833984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 13007.8467\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [13007.8466796875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 15348.1533\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [15348.1533203125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 10293.4150\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [10293.4150390625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 7063.2539\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [7063.25390625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 23886.1992\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [23886.19921875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 16319.8701\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [16319.8701171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 11980.8418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  29.0 q-value:  0\n",
      "loss: [11980.841796875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 19258.9570\n",
      "rewards:  27.0 q-value:  0\n",
      "loss: [19258.95703125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 2746.6685\n",
      "rewards:  22.0 q-value:  0\n",
      "loss: [2746.66845703125]\n",
      "Number of actions available 5\n",
      "Episode : 7\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 14690.0156\n",
      "rewards:  -5 q-value:  0\n",
      "loss: [14690.015625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 8417.8574\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [8417.857421875]\n",
      "Exploiting\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 7095.4473\n",
      "rewards:  8.0 q-value:  [[0.         0.         0.         0.         0.         0.3282604\n",
      "  0.         0.         0.         0.         0.         0.01830234\n",
      "  0.04670671 0.         0.         0.         0.05038361 0.17151774\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7095.447265625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 9876.3906\n",
      "rewards:  -10.0 q-value:  0\n",
      "loss: [9876.390625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 15487.3691\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [15487.369140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 12146.2959\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [12146.2958984375]\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 9886.4180\n",
      "rewards:  4.0 q-value:  [[0.         0.         0.         0.         0.         0.19964492\n",
      "  0.         0.         0.         0.02480842 0.         0.\n",
      "  0.00202443 0.         0.         0.04684129 0.         0.06943158\n",
      "  0.         0.0444722  0.        ]]\n",
      "loss: [9886.41796875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 14233.3330\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [14233.3330078125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 5439.9180\n",
      "rewards:  4.0 q-value:  0\n",
      "loss: [5439.91796875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 10677.1035\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [10677.103515625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 11696.0625\n",
      "rewards:  24.0 q-value:  0\n",
      "loss: [11696.0625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 7580.0776\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [7580.07763671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 6958.4531\n",
      "rewards:  14.0 q-value:  0\n",
      "loss: [6958.453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 87us/step - loss: 14112.5176\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [14112.517578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 12969.7109\n",
      "rewards:  4.0 q-value:  0\n",
      "loss: [12969.7109375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 12762.6572\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [12762.6572265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 10288.7773\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [10288.77734375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 11193.9971\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [11193.9970703125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 7770.8857\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [7770.8857421875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 230us/step - loss: 13183.9980\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [13183.998046875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 4568.2148\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [4568.21484375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 5526.3833\n",
      "rewards:  10.0 q-value:  0\n",
      "loss: [5526.38330078125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 9682.2793\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [9682.279296875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 12304.8477\n",
      "rewards:  4.0 q-value:  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.7391147e-01 1.7365813e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 5.2783702e-02 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 8.6676888e-02 1.3584247e-01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]]\n",
      "loss: [12304.84765625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 9815.1914\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [9815.19140625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 14387.1465\n",
      "rewards:  -8.0 q-value:  0\n",
      "loss: [14387.146484375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 12292.9512\n",
      "rewards:  16.0 q-value:  0\n",
      "loss: [12292.951171875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 18023.3164\n",
      "rewards:  -10.0 q-value:  0\n",
      "loss: [18023.31640625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 9316.1748\n",
      "rewards:  -13.0 q-value:  0\n",
      "loss: [9316.1748046875]\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 9359.2891\n",
      "rewards:  -26.0 q-value:  [[0.         0.         0.         0.         0.         0.21494548\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01928105 0.00209054 0.         0.00321525 0.00097099 0.05764326\n",
      "  0.         0.00215213 0.        ]]\n",
      "loss: [9359.2890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 8781.4873\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [8781.4873046875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 5729.9097\n",
      "rewards:  26.0 q-value:  0\n",
      "loss: [5729.90966796875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 6566.4355\n",
      "rewards:  22.0 q-value:  0\n",
      "loss: [6566.435546875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 9728.8623\n",
      "rewards:  -33.0 q-value:  0\n",
      "loss: [9728.8623046875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 10563.6982\n",
      "rewards:  -33.0 q-value:  0\n",
      "loss: [10563.6982421875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 4827.5161\n",
      "rewards:  -38.0 q-value:  [[0.         0.         0.         0.         0.         0.2809744\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01598867 0.         0.         0.06771364 0.         0.06323798\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4827.51611328125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 9374.3125\n",
      "rewards:  -14.0 q-value:  0\n",
      "loss: [9374.3125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 18413.3652\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [18413.365234375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 7710.7671\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [7710.76708984375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 18330.4863\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [18330.486328125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 6374.7822\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [6374.7822265625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 14062.4814\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [14062.4814453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 11935.4375\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [11935.4375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 8519.3984\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [8519.3984375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 19180.3262\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [19180.326171875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 8344.5098\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [8344.509765625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 10569.5117\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [10569.51171875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 12961.4922\n",
      "rewards:  17.0 q-value:  0\n",
      "loss: [12961.4921875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 6603.9307\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [6603.9306640625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 11088.4414\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [11088.44140625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 11788.3496\n",
      "rewards:  1.0 q-value:  [[0.         0.         0.         0.         0.         0.334461\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04271122 0.         0.         0.         0.03345746 0.1405526\n",
      "  0.         0.02789018 0.        ]]\n",
      "loss: [11788.349609375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 10574.3027\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [10574.302734375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 19082.5254\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [19082.525390625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 263us/step - loss: 9853.8633\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [9853.86328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 12292.0283\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [12292.0283203125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 17608.4863\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [17608.486328125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 6930.3184\n",
      "rewards:  -39.0 q-value:  0\n",
      "loss: [6930.318359375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 14942.4570\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [14942.45703125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 12656.2715\n",
      "rewards:  -53.0 q-value:  0\n",
      "loss: [12656.271484375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 15673.7246\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [15673.724609375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 15662.5967\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [15662.5966796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 14835.8223\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [14835.822265625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 17195.0391\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [17195.0390625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 264us/step - loss: 10232.2852\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [10232.28515625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 13811.0820\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [13811.08203125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 6192.1816\n",
      "rewards:  -65.0 q-value:  0\n",
      "loss: [6192.181640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 20402.3320\n",
      "rewards:  -70.0 q-value:  0\n",
      "loss: [20402.33203125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 9400.3711\n",
      "rewards:  -89.0 q-value:  0\n",
      "loss: [9400.37109375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 6476.3301\n",
      "rewards:  -106.0 q-value:  0\n",
      "loss: [6476.330078125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 6038.9639\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [6038.9638671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 13049.9785\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [13049.978515625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 22705.8242\n",
      "rewards:  -77.0 q-value:  0\n",
      "loss: [22705.82421875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 14584.4258\n",
      "rewards:  -83.0 q-value:  0\n",
      "loss: [14584.42578125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 12946.5215\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [12946.521484375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 23425.7441\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [23425.744140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 16721.4688\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [16721.46875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 13095.9746\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [13095.974609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 23399.2090\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [23399.208984375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 13120.2070\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [13120.20703125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 402us/step - loss: 8445.4844\n",
      "rewards:  -145.0 q-value:  0\n",
      "loss: [8445.484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 20845.2676\n",
      "rewards:  -177.0 q-value:  0\n",
      "loss: [20845.267578125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 6873.9336\n",
      "rewards:  -179.0 q-value:  0\n",
      "loss: [6873.93359375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 15768.2402\n",
      "rewards:  -175.0 q-value:  0\n",
      "loss: [15768.240234375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 4328.4282\n",
      "rewards:  -216.0 q-value:  0\n",
      "loss: [4328.42822265625]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 17190.0957\n",
      "rewards:  -208.0 q-value:  0\n",
      "loss: [17190.095703125]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 82us/step - loss: 3653.1958\n",
      "rewards:  -210.0 q-value:  0\n",
      "loss: [3653.19580078125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 84us/step - loss: 4307.5635\n",
      "rewards:  -188.0 q-value:  0\n",
      "loss: [4307.5634765625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 6491.1582\n",
      "rewards:  -188.0 q-value:  0\n",
      "loss: [6491.158203125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 1337.1578\n",
      "rewards:  -188.0 q-value:  0\n",
      "loss: [1337.1578369140625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 6031.8198\n",
      "rewards:  -193.0 q-value:  [[0.         0.         0.         0.         0.         0.24762435\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06405261 0.         0.         0.         0.05863293 0.09963582\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6031.81982421875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 8410.2715\n",
      "rewards:  -169.0 q-value:  0\n",
      "loss: [8410.271484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 4707.2280\n",
      "rewards:  -174.0 q-value:  0\n",
      "loss: [4707.22802734375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 9149.2217\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [9149.2216796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 8677.8018\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [8677.8017578125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 5506.0156\n",
      "rewards:  -175.0 q-value:  0\n",
      "loss: [5506.015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 5875.0625\n",
      "rewards:  -180.0 q-value:  0\n",
      "loss: [5875.0625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 15744.1582\n",
      "rewards:  -185.0 q-value:  0\n",
      "loss: [15744.158203125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 11195.1152\n",
      "rewards:  -199.0 q-value:  0\n",
      "loss: [11195.115234375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 18109.9844\n",
      "rewards:  -204.0 q-value:  [[0.         0.         0.         0.         0.         0.2783361\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.07132153 0.         0.05161744 0.         0.02865799 0.11975378\n",
      "  0.         0.05740885 0.        ]]\n",
      "loss: [18109.984375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 15522.6650\n",
      "rewards:  -218.0 q-value:  0\n",
      "loss: [15522.6650390625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 15541.3818\n",
      "rewards:  -201.0 q-value:  0\n",
      "loss: [15541.3818359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 9603.5293\n",
      "rewards:  -206.0 q-value:  0\n",
      "loss: [9603.529296875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 7487.5977\n",
      "rewards:  -223.0 q-value:  0\n",
      "loss: [7487.59765625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 16899.7812\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [16899.78125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 14157.3721\n",
      "rewards:  -211.0 q-value:  0\n",
      "loss: [14157.3720703125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 8455.8389\n",
      "rewards:  -249.0 q-value:  0\n",
      "loss: [8455.8388671875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 13717.3398\n",
      "rewards:  -249.0 q-value:  0\n",
      "loss: [13717.33984375]\n",
      "Exploiting\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 12790.8926\n",
      "rewards:  -213.0 q-value:  [[0.         0.         0.         0.         0.         0.28959325\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06880356 0.         0.         0.04373917 0.         0.11551157\n",
      "  0.         0.         0.        ]]\n",
      "loss: [12790.892578125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 10371.6465\n",
      "rewards:  -228.0 q-value:  0\n",
      "loss: [10371.646484375]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 20620.3789\n",
      "rewards:  -208.0 q-value:  0\n",
      "loss: [20620.37890625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 18131.1680\n",
      "rewards:  -184.0 q-value:  0\n",
      "loss: [18131.16796875]\n",
      "Exploring\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 158us/step - loss: 17218.1250\n",
      "rewards:  -181.0 q-value:  0\n",
      "loss: [17218.125]\n",
      "Exploiting\n",
      "Selected action  [4 3]\n",
      "[4 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 5259.2598\n",
      "rewards:  -187.0 q-value:  [[0.         0.         0.         0.         0.         0.32555285\n",
      "  0.         0.         0.         0.         0.         0.00365019\n",
      "  0.07751483 0.         0.         0.         0.07195448 0.14668068\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5259.259765625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 12442.1709\n",
      "rewards:  -192.0 q-value:  [[0.         0.         0.         0.         0.         0.43770894\n",
      "  0.         0.         0.         0.         0.         0.02441718\n",
      "  0.09767295 0.         0.         0.         0.13704985 0.15269822\n",
      "  0.         0.         0.        ]]\n",
      "loss: [12442.1708984375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 10920.4180\n",
      "rewards:  -194.0 q-value:  0\n",
      "loss: [10920.41796875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 3959.9702\n",
      "rewards:  -194.0 q-value:  0\n",
      "loss: [3959.97021484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 9299.3535\n",
      "rewards:  -199.0 q-value:  0\n",
      "loss: [9299.353515625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 8659.4111\n",
      "rewards:  -201.0 q-value:  0\n",
      "loss: [8659.4111328125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 11753.4434\n",
      "rewards:  -205.0 q-value:  0\n",
      "loss: [11753.443359375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 4223.6562\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [4223.65625]\n",
      "Number of actions available 7\n",
      "Episode : 8\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 5362.7129\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [5362.712890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 9472.1533\n",
      "rewards:  -6.0 q-value:  0\n",
      "loss: [9472.1533203125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 6132.3076\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [6132.3076171875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 20664.1406\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [20664.140625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 18031.6836\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [18031.68359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 7309.6372\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [7309.63720703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 15930.8350\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [15930.8349609375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 228us/step - loss: 15264.6934\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [15264.693359375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 10566.5859\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [10566.5859375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 10902.4336\n",
      "rewards:  -13.0 q-value:  0\n",
      "loss: [10902.43359375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 6934.1348\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [6934.134765625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 8791.9238\n",
      "rewards:  -39.0 q-value:  0\n",
      "loss: [8791.923828125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 4025.6216\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [4025.62158203125]\n",
      "Exploiting\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 11285.0742\n",
      "rewards:  -57.0 q-value:  [[0.         0.         0.         0.         0.         0.3340545\n",
      "  0.         0.00202009 0.         0.         0.         0.\n",
      "  0.06762973 0.         0.         0.         0.05957048 0.1986298\n",
      "  0.         0.         0.        ]]\n",
      "loss: [11285.07421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 8205.4004\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [8205.400390625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 9576.6406\n",
      "rewards:  -74.0 q-value:  0\n",
      "loss: [9576.640625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 13336.4961\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [13336.49609375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 5509.4839\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [5509.48388671875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 11772.3691\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [11772.369140625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 7058.4478\n",
      "rewards:  -40.0 q-value:  0\n",
      "loss: [7058.44775390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 7780.6270\n",
      "rewards:  -45.0 q-value:  0\n",
      "loss: [7780.626953125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 3868.0969\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [3868.096923828125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 12856.6348\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [12856.634765625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 13065.1250\n",
      "rewards:  -27.0 q-value:  0\n",
      "loss: [13065.125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 7608.5322\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [7608.5322265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 5534.7412\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [5534.7412109375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 17836.1543\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [17836.154296875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 14780.2285\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [14780.228515625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 10519.2305\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [10519.23046875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 212us/step - loss: 3367.8220\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [3367.822021484375]\n",
      "Exploiting\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 4471.2241\n",
      "rewards:  -21.0 q-value:  [[0.         0.         0.         0.         0.         0.3791582\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05338023 0.         0.         0.         0.03547618 0.1772861\n",
      "  0.         0.04067992 0.        ]]\n",
      "loss: [4471.22412109375]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 4423.6294\n",
      "rewards:  -48.0 q-value:  [[0.         0.         0.         0.         0.         0.39178935\n",
      "  0.         0.         0.         0.         0.         0.01825373\n",
      "  0.0971862  0.         0.         0.         0.11206739 0.18353304\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4423.62939453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 6129.3384\n",
      "rewards:  -68.0 q-value:  0\n",
      "loss: [6129.33837890625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 17837.8359\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [17837.8359375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 4681.9219\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [4681.921875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 11482.2617\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [11482.26171875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 15903.4434\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [15903.443359375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 13006.3486\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [13006.3486328125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 4458.4453\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [4458.4453125]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 8223.1484\n",
      "rewards:  -40.0 q-value:  [[0.         0.         0.         0.         0.         0.37124193\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05630649 0.         0.         0.         0.03525638 0.1991952\n",
      "  0.         0.         0.        ]]\n",
      "loss: [8223.1484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 6335.5312\n",
      "rewards:  -45.0 q-value:  0\n",
      "loss: [6335.53125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 16891.7539\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [16891.75390625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 5908.1812\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [5908.18115234375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 5492.4355\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [5492.435546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 5286.4951\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [5286.4951171875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 16567.4746\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [16567.474609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 11698.3643\n",
      "rewards:  -57.0 q-value:  0\n",
      "loss: [11698.3642578125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 4254.3564\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [4254.3564453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 16810.9609\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [16810.9609375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 85us/step - loss: 14412.1211\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [14412.12109375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 6876.1826\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [6876.1826171875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 10715.0859\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [10715.0859375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 229us/step - loss: 19869.2734\n",
      "rewards:  -92.0 q-value:  0\n",
      "loss: [19869.2734375]\n",
      "Exploiting\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 7840.5771\n",
      "rewards:  -89.0 q-value:  [[0.         0.         0.         0.         0.         0.32398278\n",
      "  0.         0.         0.         0.         0.         0.03793438\n",
      "  0.03541692 0.         0.         0.         0.07331775 0.11581267\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7840.5771484375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 6281.7275\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [6281.7275390625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 7425.8335\n",
      "rewards:  -93.0 q-value:  0\n",
      "loss: [7425.83349609375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 7855.2300\n",
      "rewards:  -93.0 q-value:  0\n",
      "loss: [7855.22998046875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 16307.0547\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [16307.0546875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 15510.2041\n",
      "rewards:  -105.0 q-value:  0\n",
      "loss: [15510.2041015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 6196.4375\n",
      "rewards:  -130.0 q-value:  0\n",
      "loss: [6196.4375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 5465.6753\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [5465.67529296875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 7470.2041\n",
      "rewards:  -126.0 q-value:  0\n",
      "loss: [7470.2041015625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 210us/step - loss: 17571.5898\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [17571.58984375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 9462.5654\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [9462.5654296875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 13349.5840\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [13349.583984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 9893.2715\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [9893.271484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 81us/step - loss: 6274.1128\n",
      "rewards:  -78.0 q-value:  0\n",
      "loss: [6274.11279296875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 6429.6758\n",
      "rewards:  -100.0 q-value:  0\n",
      "loss: [6429.67578125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 12611.2568\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [12611.2568359375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 2993.4951\n",
      "rewards:  -114.0 q-value:  0\n",
      "loss: [2993.4951171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 7012.7988\n",
      "rewards:  -109.0 q-value:  0\n",
      "loss: [7012.798828125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 10785.4453\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [10785.4453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 19975.0625\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [19975.0625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 10267.3730\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [10267.373046875]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 11112.9385\n",
      "rewards:  -129.0 q-value:  [[0.         0.         0.         0.         0.         0.48461202\n",
      "  0.         0.         0.         0.01260409 0.         0.\n",
      "  0.11841213 0.         0.         0.02082573 0.055291   0.30816397\n",
      "  0.         0.03390256 0.        ]]\n",
      "loss: [11112.9384765625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 5505.0801\n",
      "rewards:  -109.0 q-value:  0\n",
      "loss: [5505.080078125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 6067.0083\n",
      "rewards:  -112.0 q-value:  0\n",
      "loss: [6067.00830078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 12107.1904\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [12107.1904296875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 3686.1196\n",
      "rewards:  -85.0 q-value:  0\n",
      "loss: [3686.11962890625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 7638.9990\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [7638.9990234375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 8565.0879\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [8565.087890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 13838.7803\n",
      "rewards:  -123.0 q-value:  0\n",
      "loss: [13838.7802734375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 8903.0586\n",
      "rewards:  -83.0 q-value:  0\n",
      "loss: [8903.05859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 6786.7642\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [6786.76416015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 10733.8975\n",
      "rewards:  -93.0 q-value:  0\n",
      "loss: [10733.8974609375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 271us/step - loss: 9528.8701\n",
      "rewards:  -77.0 q-value:  0\n",
      "loss: [9528.8701171875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 8999.1416\n",
      "rewards:  -83.0 q-value:  0\n",
      "loss: [8999.1416015625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 11994.1846\n",
      "rewards:  -87.0 q-value:  0\n",
      "loss: [11994.1845703125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 5762.6494\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [5762.6494140625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 8738.1357\n",
      "rewards:  -95.0 q-value:  0\n",
      "loss: [8738.1357421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 10080.1924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -99.0 q-value:  0\n",
      "loss: [10080.1923828125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 8080.0762\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [8080.076171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 8682.1797\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [8682.1796875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 9072.3389\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [9072.3388671875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 18341.7246\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [18341.724609375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 12241.6562\n",
      "rewards:  -128.0 q-value:  0\n",
      "loss: [12241.65625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 12514.2881\n",
      "rewards:  -133.0 q-value:  0\n",
      "loss: [12514.2880859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 7319.4180\n",
      "rewards:  -138.0 q-value:  0\n",
      "loss: [7319.41796875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 13349.9561\n",
      "rewards:  -141.0 q-value:  0\n",
      "loss: [13349.9560546875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 26393.0195\n",
      "rewards:  -134.0 q-value:  0\n",
      "loss: [26393.01953125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 10098.0127\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [10098.0126953125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 7789.0874\n",
      "rewards:  -162.0 q-value:  0\n",
      "loss: [7789.08740234375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 6377.5244\n",
      "rewards:  -169.0 q-value:  0\n",
      "loss: [6377.5244140625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 21575.3594\n",
      "rewards:  -174.0 q-value:  [[0.         0.         0.         0.         0.         0.3457383\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03959259 0.         0.         0.00063162 0.01745283 0.17039825\n",
      "  0.         0.         0.        ]]\n",
      "loss: [21575.359375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 8300.9883\n",
      "rewards:  -176.0 q-value:  0\n",
      "loss: [8300.98828125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 7357.0142\n",
      "rewards:  -156.0 q-value:  0\n",
      "loss: [7357.01416015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 12757.2686\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [12757.2685546875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 12118.6494\n",
      "rewards:  -137.0 q-value:  0\n",
      "loss: [12118.6494140625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 15920.9043\n",
      "rewards:  -126.0 q-value:  0\n",
      "loss: [15920.904296875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 13447.6426\n",
      "rewards:  -126.0 q-value:  0\n",
      "loss: [13447.642578125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 1024.1016\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [1024.1015625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 219us/step - loss: 1873.8510\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [1873.8509521484375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 9109.5791\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [9109.5791015625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 90us/step - loss: 6705.7422\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [6705.7421875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 4813.0508\n",
      "rewards:  -116.0 q-value:  0\n",
      "loss: [4813.05078125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 7861.3115\n",
      "rewards:  -112.0 q-value:  0\n",
      "loss: [7861.3115234375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 1548.5251\n",
      "rewards:  -114.0 q-value:  0\n",
      "loss: [1548.525146484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 16610.4961\n",
      "rewards:  -116.0 q-value:  0\n",
      "loss: [16610.49609375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 11969.9824\n",
      "rewards:  -101.0 q-value:  0\n",
      "loss: [11969.982421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 4893.5142\n",
      "rewards:  -106.0 q-value:  0\n",
      "loss: [4893.51416015625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 3798.8535\n",
      "rewards:  -74.0 q-value:  0\n",
      "loss: [3798.853515625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 7756.5840\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [7756.583984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 10629.8096\n",
      "rewards:  -81.0 q-value:  0\n",
      "loss: [10629.8095703125]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 5462.3950\n",
      "rewards:  -78.0 q-value:  [[0.         0.         0.         0.         0.         0.3340545\n",
      "  0.         0.00202009 0.         0.         0.         0.\n",
      "  0.06762973 0.         0.         0.         0.05957048 0.1986298\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5462.39501953125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 19758.9609\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [19758.9609375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 10736.8672\n",
      "rewards:  -87.0 q-value:  0\n",
      "loss: [10736.8671875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 7653.5635\n",
      "rewards:  -67.0 q-value:  0\n",
      "loss: [7653.5634765625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 6665.6182\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [6665.6181640625]\n",
      "Number of actions available 11\n",
      "Episode : 9\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 11920.6631\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [11920.6630859375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 15824.7041\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [15824.7041015625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 2599.4253\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [2599.42529296875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 9966.3271\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [9966.3271484375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 305us/step - loss: 10724.7061\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [10724.7060546875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 16782.6367\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [16782.63671875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 224us/step - loss: 4628.8428\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [4628.8427734375]\n",
      "Exploiting\n",
      "Selected action  [0 3]\n",
      "[0 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 9270.9395\n",
      "rewards:  32.0 q-value:  [[0.         0.         0.         0.         0.         0.4919832\n",
      "  0.         0.         0.         0.         0.         0.00377483\n",
      "  0.11554367 0.         0.01612853 0.         0.05664667 0.2897812\n",
      "  0.         0.00527227 0.        ]]\n",
      "loss: [9270.939453125]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 11085.7490\n",
      "rewards:  30.0 q-value:  0\n",
      "loss: [11085.7490234375]\n",
      "Exploiting\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 14025.7305\n",
      "rewards:  30.0 q-value:  [[0.         0.         0.         0.         0.         0.30454162\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05935901 0.         0.         0.         0.01068665 0.12018364\n",
      "  0.         0.         0.        ]]\n",
      "loss: [14025.73046875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 10807.2324\n",
      "rewards:  33.0 q-value:  0\n",
      "loss: [10807.232421875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 6569.5039\n",
      "rewards:  41.0 q-value:  0\n",
      "loss: [6569.50390625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 276us/step - loss: 22015.9648\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [22015.96484375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 294us/step - loss: 8024.5586\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [8024.55859375]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 9558.8008\n",
      "rewards:  -10.0 q-value:  0\n",
      "loss: [9558.80078125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 7007.7236\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [7007.7236328125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 205us/step - loss: 4325.7603\n",
      "rewards:  24.0 q-value:  0\n",
      "loss: [4325.76025390625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 7189.8828\n",
      "rewards:  38.0 q-value:  0\n",
      "loss: [7189.8828125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 11053.0723\n",
      "rewards:  70.0 q-value:  0\n",
      "loss: [11053.072265625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 79us/step - loss: 7306.8887\n",
      "rewards:  67.0 q-value:  0\n",
      "loss: [7306.888671875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 10633.6299\n",
      "rewards:  27.0 q-value:  0\n",
      "loss: [10633.6298828125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 8012.8916\n",
      "rewards:  21.0 q-value:  0\n",
      "loss: [8012.8916015625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 7480.6152\n",
      "rewards:  43.0 q-value:  0\n",
      "loss: [7480.615234375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 14125.7266\n",
      "rewards:  40.0 q-value:  0\n",
      "loss: [14125.7265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 210us/step - loss: 12100.2578\n",
      "rewards:  35.0 q-value:  0\n",
      "loss: [12100.2578125]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 4030.2988\n",
      "rewards:  20.0 q-value:  0\n",
      "loss: [4030.298828125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 11797.5762\n",
      "rewards:  17.0 q-value:  0\n",
      "loss: [11797.576171875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 10893.5195\n",
      "rewards:  16.0 q-value:  0\n",
      "loss: [10893.51953125]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 16197.4824\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [16197.482421875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 301us/step - loss: 2828.9136\n",
      "rewards:  10.0 q-value:  0\n",
      "loss: [2828.91357421875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 244us/step - loss: 8999.8369\n",
      "rewards:  46.0 q-value:  0\n",
      "loss: [8999.8369140625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 13314.7012\n",
      "rewards:  54.0 q-value:  0\n",
      "loss: [13314.701171875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 8679.6172\n",
      "rewards:  24.0 q-value:  0\n",
      "loss: [8679.6171875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 12098.9863\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [12098.986328125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 197us/step - loss: 14866.9922\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [14866.9921875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 12745.9580\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [12745.9580078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 17238.5605\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [17238.560546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 17125.2891\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [17125.2890625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 8476.3105\n",
      "rewards:  -40.0 q-value:  [[0.         0.         0.         0.         0.         0.4484829\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06238401 0.         0.         0.         0.07463124 0.13895696\n",
      "  0.         0.06021333 0.        ]]\n",
      "loss: [8476.310546875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 5177.6768\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [5177.6767578125]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 11358.7520\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [11358.751953125]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 5673.5342\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [5673.5341796875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 14618.1367\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [14618.13671875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 8568.7402\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [8568.740234375]\n",
      "Exploiting\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 13041.6914\n",
      "rewards:  -64.0 q-value:  [[0.         0.         0.         0.         0.         0.3646671\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06910945 0.         0.         0.         0.065116   0.16416334\n",
      "  0.         0.00672938 0.        ]]\n",
      "loss: [13041.69140625]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 3123.4434\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [3123.443359375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 6430.9312\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [6430.93115234375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 523us/step - loss: 8626.6133\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [8626.61328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 4022.9956\n",
      "rewards:  -33.0 q-value:  0\n",
      "loss: [4022.99560546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 8474.1230\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [8474.123046875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 16111.9375\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [16111.9375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 4909.8193\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [4909.8193359375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 244us/step - loss: 4353.4448\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [4353.44482421875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 12009.9512\n",
      "rewards:  14.0 q-value:  0\n",
      "loss: [12009.951171875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 258us/step - loss: 8362.5098\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [8362.509765625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 216us/step - loss: 14701.9199\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [14701.919921875]\n",
      "Exploiting\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 6768.3584\n",
      "rewards:  4.0 q-value:  [[0.         0.         0.         0.         0.         0.4248218\n",
      "  0.         0.         0.         0.         0.         0.02002457\n",
      "  0.04820918 0.         0.         0.         0.09574321 0.14260952\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6768.3583984375]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 12849.2402\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [12849.240234375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 6400.9336\n",
      "rewards:  -11.0 q-value:  0\n",
      "loss: [6400.93359375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 462us/step - loss: 10778.1914\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [10778.19140625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 251us/step - loss: 13199.1504\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [13199.150390625]\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 2866.9395\n",
      "rewards:  -28.0 q-value:  [[0.         0.         0.         0.         0.         0.46298078\n",
      "  0.         0.         0.         0.         0.         0.03325897\n",
      "  0.11205041 0.         0.         0.         0.07122944 0.24803099\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2866.939453125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 4693.5151\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [4693.51513671875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 2192.2544\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [2192.25439453125]\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 233us/step - loss: 8369.9297\n",
      "rewards:  -55.0 q-value:  [[0.         0.         0.         0.         0.         0.40984663\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03489511 0.         0.         0.         0.03469653 0.15531063\n",
      "  0.         0.0319103  0.        ]]\n",
      "loss: [8369.9296875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 10111.5195\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [10111.51953125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 14966.6758\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [14966.67578125]\n",
      "Exploiting\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 269us/step - loss: 7424.1997\n",
      "rewards:  -46.0 q-value:  [[0.         0.         0.         0.         0.         0.49148065\n",
      "  0.         0.         0.         0.         0.         0.01059395\n",
      "  0.12475222 0.         0.         0.         0.08010545 0.24186197\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7424.19970703125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 3986.7915\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [3986.79150390625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 854.2126\n",
      "rewards:  -40.0 q-value:  0\n",
      "loss: [854.212646484375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 4392.7925\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [4392.79248046875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 4496.1655\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [4496.16552734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 4913.4370\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [4913.43701171875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 10849.9893\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [10849.9892578125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 6157.4561\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [6157.4560546875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 4322.9502\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [4322.9501953125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 328us/step - loss: 12926.9775\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [12926.9775390625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 17710.4941\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [17710.494140625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 8320.3936\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [8320.3935546875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 10965.0352\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [10965.03515625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 7688.6147\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [7688.61474609375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 13871.9551\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [13871.955078125]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 6948.1162\n",
      "rewards:  -12.0 q-value:  [[0.         0.         0.         0.         0.         0.266246\n",
      "  0.         0.         0.         0.02053297 0.         0.\n",
      "  0.01636722 0.         0.         0.05258476 0.         0.10371709\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6948.1162109375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 8569.2559\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [8569.255859375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 220us/step - loss: 5233.9902\n",
      "rewards:  -13.0 q-value:  0\n",
      "loss: [5233.990234375]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 10862.9180\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [10862.91796875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 5968.3320\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [5968.33203125]\n",
      "Exploiting\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 6571.0483\n",
      "rewards:  -18.0 q-value:  [[0.         0.         0.         0.         0.         0.3348001\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06962102 0.0008222  0.         0.         0.04924978 0.1419016\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6571.04833984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 7468.7266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -23.0 q-value:  0\n",
      "loss: [7468.7265625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 12083.9492\n",
      "rewards:  -11.0 q-value:  0\n",
      "loss: [12083.94921875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 13324.4297\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [13324.4296875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 14101.5586\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [14101.55859375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 4559.8799\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [4559.8798828125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 7548.5767\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [7548.57666015625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 10170.5391\n",
      "rewards:  -66.0 q-value:  0\n",
      "loss: [10170.5390625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 19839.4648\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [19839.46484375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 11145.9365\n",
      "rewards:  -41.0 q-value:  0\n",
      "loss: [11145.9365234375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 227us/step - loss: 13833.1875\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [13833.1875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 6866.3486\n",
      "rewards:  -78.0 q-value:  0\n",
      "loss: [6866.3486328125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 11612.1562\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [11612.15625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 7518.3857\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [7518.3857421875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 8207.9023\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [8207.90234375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 14018.2676\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [14018.267578125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 14036.5508\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [14036.55078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 268us/step - loss: 4120.1689\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [4120.1689453125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 4166.9590\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [4166.958984375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 9797.5439\n",
      "rewards:  -87.0 q-value:  0\n",
      "loss: [9797.5439453125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 9328.6982\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [9328.6982421875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 10851.4668\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [10851.466796875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 13389.4082\n",
      "rewards:  -84.0 q-value:  0\n",
      "loss: [13389.408203125]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 5198.9170\n",
      "rewards:  -80.0 q-value:  0\n",
      "loss: [5198.9169921875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 3785.9539\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [3785.953857421875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 2076.8669\n",
      "rewards:  -73.0 q-value:  0\n",
      "loss: [2076.866943359375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 16721.0781\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [16721.078125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 12489.3350\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [12489.3349609375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 5381.8525\n",
      "rewards:  -112.0 q-value:  0\n",
      "loss: [5381.8525390625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 4064.0266\n",
      "rewards:  -92.0 q-value:  0\n",
      "loss: [4064.026611328125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 6289.0581\n",
      "rewards:  -100.0 q-value:  0\n",
      "loss: [6289.05810546875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 262us/step - loss: 3171.5684\n",
      "rewards:  -68.0 q-value:  0\n",
      "loss: [3171.568359375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 5679.1118\n",
      "rewards:  -86.0 q-value:  0\n",
      "loss: [5679.11181640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 7234.7412\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [7234.7412109375]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 8429.5361\n",
      "rewards:  -63.0 q-value:  0\n",
      "loss: [8429.5361328125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 253us/step - loss: 11067.3691\n",
      "rewards:  -61.0 q-value:  0\n",
      "loss: [11067.369140625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1014.6372\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [1014.63720703125]\n",
      "Number of actions available 5\n",
      "Episode : 10\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 7836.5220\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [7836.52197265625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 7748.3301\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [7748.330078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 16211.2734\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [16211.2734375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 7645.8398\n",
      "rewards:  4.0 q-value:  0\n",
      "loss: [7645.83984375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 7165.9248\n",
      "rewards:  32.0 q-value:  0\n",
      "loss: [7165.9248046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 6935.6387\n",
      "rewards:  27.0 q-value:  0\n",
      "loss: [6935.638671875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 17028.0137\n",
      "rewards:  35.0 q-value:  0\n",
      "loss: [17028.013671875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 5645.0059\n",
      "rewards:  30.0 q-value:  [[0.         0.         0.         0.         0.         0.3656813\n",
      "  0.         0.         0.         0.         0.         0.00777944\n",
      "  0.07291423 0.         0.         0.         0.05368509 0.19532363\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5645.005859375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 190us/step - loss: 1157.7942\n",
      "rewards:  10.0 q-value:  0\n",
      "loss: [1157.794189453125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 4301.0308\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [4301.03076171875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 13711.6611\n",
      "rewards:  16.0 q-value:  0\n",
      "loss: [13711.6611328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 11679.9023\n",
      "rewards:  11.0 q-value:  0\n",
      "loss: [11679.90234375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 12689.8418\n",
      "rewards:  2.0 q-value:  0\n",
      "loss: [12689.841796875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 1453.0741\n",
      "rewards:  -4.0 q-value:  0\n",
      "loss: [1453.0740966796875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 4354.6631\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [4354.6630859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 6462.0518\n",
      "rewards:  3.0 q-value:  0\n",
      "loss: [6462.0517578125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 252us/step - loss: 13832.3828\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [13832.3828125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 8422.6064\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [8422.6064453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 2546.4673\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [2546.46728515625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 17718.8301\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [17718.830078125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 18217.5488\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [18217.548828125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 4968.5259\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [4968.52587890625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 9997.7520\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [9997.751953125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 7997.1553\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [7997.1552734375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 4450.4922\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [4450.4921875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 14604.4609\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [14604.4609375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 8954.5117\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [8954.51171875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 12105.6094\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [12105.609375]\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 195us/step - loss: 2670.3984\n",
      "rewards:  -48.0 q-value:  [[0.         0.         0.         0.         0.         0.4592189\n",
      "  0.         0.         0.         0.         0.         0.03356162\n",
      "  0.10534759 0.         0.00216104 0.         0.14365    0.2299692\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2670.3984375]\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 9473.3203\n",
      "rewards:  -49.0 q-value:  [[0.         0.         0.         0.         0.         0.2880275\n",
      "  0.         0.         0.         0.         0.         0.02836319\n",
      "  0.05000114 0.         0.         0.         0.01680708 0.17137666\n",
      "  0.         0.         0.        ]]\n",
      "loss: [9473.3203125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 6870.4648\n",
      "rewards:  -50.0 q-value:  0\n",
      "loss: [6870.46484375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 8550.8545\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [8550.8544921875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 5790.3994\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [5790.3994140625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 6631.7939\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [6631.7939453125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 16718.6348\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [16718.634765625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 9808.3975\n",
      "rewards:  -14.0 q-value:  0\n",
      "loss: [9808.3974609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 4177.4604\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [4177.46044921875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 2745.4392\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [2745.439208984375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 8787.9209\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [8787.9208984375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 4462.3721\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [4462.3720703125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 15095.7559\n",
      "rewards:  -58.0 q-value:  0\n",
      "loss: [15095.755859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 3898.2751\n",
      "rewards:  -54.0 q-value:  0\n",
      "loss: [3898.275146484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 79us/step - loss: 12927.3789\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [12927.37890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 220us/step - loss: 3161.0356\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [3161.03564453125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 7496.9707\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [7496.970703125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 8451.5850\n",
      "rewards:  -62.0 q-value:  0\n",
      "loss: [8451.5849609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 4559.5581\n",
      "rewards:  -67.0 q-value:  0\n",
      "loss: [4559.55810546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 7690.8105\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [7690.810546875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 3645.5981\n",
      "rewards:  -60.0 q-value:  0\n",
      "loss: [3645.59814453125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 5680.8955\n",
      "rewards:  -100.0 q-value:  0\n",
      "loss: [5680.8955078125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 1090.2053\n",
      "rewards:  -95.0 q-value:  0\n",
      "loss: [1090.205322265625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 4614.8525\n",
      "rewards:  -97.0 q-value:  0\n",
      "loss: [4614.8525390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 216us/step - loss: 13283.3945\n",
      "rewards:  -102.0 q-value:  0\n",
      "loss: [13283.39453125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 16720.6133\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [16720.61328125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 275us/step - loss: 1963.2783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -96.0 q-value:  0\n",
      "loss: [1963.2783203125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 12711.4521\n",
      "rewards:  -112.0 q-value:  0\n",
      "loss: [12711.4521484375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 7770.0391\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [7770.0390625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 9614.4688\n",
      "rewards:  -94.0 q-value:  0\n",
      "loss: [9614.46875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 2305.0535\n",
      "rewards:  -96.0 q-value:  0\n",
      "loss: [2305.053466796875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 12939.6406\n",
      "rewards:  -68.0 q-value:  0\n",
      "loss: [12939.640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 6651.3799\n",
      "rewards:  -95.0 q-value:  0\n",
      "loss: [6651.3798828125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 3866.6658\n",
      "rewards:  -97.0 q-value:  0\n",
      "loss: [3866.665771484375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 15821.0605\n",
      "rewards:  -99.0 q-value:  0\n",
      "loss: [15821.060546875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 2437.1025\n",
      "rewards:  -101.0 q-value:  0\n",
      "loss: [2437.1025390625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 9946.6475\n",
      "rewards:  -103.0 q-value:  0\n",
      "loss: [9946.6474609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 12701.1758\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [12701.17578125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 16161.1250\n",
      "rewards:  -124.0 q-value:  0\n",
      "loss: [16161.125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 1898.8369\n",
      "rewards:  -125.0 q-value:  0\n",
      "loss: [1898.8369140625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 15226.8359\n",
      "rewards:  -121.0 q-value:  0\n",
      "loss: [15226.8359375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 2503.0889\n",
      "rewards:  -122.0 q-value:  0\n",
      "loss: [2503.0888671875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 16228.7461\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [16228.74609375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 286us/step - loss: 18155.8359\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [18155.8359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 12882.3535\n",
      "rewards:  -92.0 q-value:  0\n",
      "loss: [12882.353515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 7541.2056\n",
      "rewards:  -97.0 q-value:  0\n",
      "loss: [7541.20556640625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 7956.8711\n",
      "rewards:  -104.0 q-value:  0\n",
      "loss: [7956.87109375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 7170.2500\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [7170.25]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 13179.8496\n",
      "rewards:  -113.0 q-value:  0\n",
      "loss: [13179.849609375]\n",
      "Exploiting\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 5319.3105\n",
      "rewards:  -122.0 q-value:  [[0.         0.         0.         0.         0.         0.32433382\n",
      "  0.         0.         0.         0.         0.         0.00727005\n",
      "  0.03572983 0.         0.         0.00995573 0.04419341 0.11293568\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5319.310546875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 11296.0850\n",
      "rewards:  -114.0 q-value:  0\n",
      "loss: [11296.0849609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 8193.0176\n",
      "rewards:  -120.0 q-value:  0\n",
      "loss: [8193.017578125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 10036.4297\n",
      "rewards:  -100.0 q-value:  0\n",
      "loss: [10036.4296875]\n",
      "Exploiting\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 244us/step - loss: 2891.1931\n",
      "rewards:  -123.0 q-value:  [[0.         0.         0.         0.         0.         0.43822992\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08060713 0.         0.         0.         0.06388992 0.20022124\n",
      "  0.         0.02865584 0.        ]]\n",
      "loss: [2891.193115234375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 4140.7227\n",
      "rewards:  -128.0 q-value:  0\n",
      "loss: [4140.72265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 8164.5400\n",
      "rewards:  -133.0 q-value:  0\n",
      "loss: [8164.5400390625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 7457.6445\n",
      "rewards:  -121.0 q-value:  0\n",
      "loss: [7457.64453125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 4242.5107\n",
      "rewards:  -124.0 q-value:  0\n",
      "loss: [4242.5107421875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 5778.4980\n",
      "rewards:  -142.0 q-value:  0\n",
      "loss: [5778.498046875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 4027.9197\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [4027.919677734375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 3107.7314\n",
      "rewards:  -200.0 q-value:  0\n",
      "loss: [3107.7314453125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 1684.3682\n",
      "rewards:  -204.0 q-value:  0\n",
      "loss: [1684.3681640625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 5435.3032\n",
      "rewards:  -184.0 q-value:  0\n",
      "loss: [5435.30322265625]\n",
      "Exploiting\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 263us/step - loss: 12740.1289\n",
      "rewards:  -168.0 q-value:  [[0.         0.         0.         0.         0.         0.30516654\n",
      "  0.         0.         0.         0.         0.         0.00299162\n",
      "  0.02160991 0.         0.         0.         0.02588052 0.13337551\n",
      "  0.         0.00951074 0.        ]]\n",
      "loss: [12740.12890625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 13355.2676\n",
      "rewards:  -182.0 q-value:  0\n",
      "loss: [13355.267578125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 9835.1426\n",
      "rewards:  -189.0 q-value:  0\n",
      "loss: [9835.142578125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 2106.5593\n",
      "rewards:  -165.0 q-value:  0\n",
      "loss: [2106.559326171875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 15674.0215\n",
      "rewards:  -164.0 q-value:  0\n",
      "loss: [15674.021484375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 7164.2744\n",
      "rewards:  -170.0 q-value:  0\n",
      "loss: [7164.2744140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 223us/step - loss: 11944.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -175.0 q-value:  0\n",
      "loss: [11944.5537109375]\n",
      "Exploiting\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 11695.4355\n",
      "rewards:  -163.0 q-value:  [[0.         0.         0.         0.         0.         0.37238088\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0678615  0.         0.         0.         0.0914225  0.13212939\n",
      "  0.         0.         0.        ]]\n",
      "loss: [11695.435546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 17229.8789\n",
      "rewards:  -168.0 q-value:  0\n",
      "loss: [17229.87890625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 10754.4365\n",
      "rewards:  -148.0 q-value:  0\n",
      "loss: [10754.4365234375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 7554.6558\n",
      "rewards:  -158.0 q-value:  0\n",
      "loss: [7554.65576171875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 6511.1631\n",
      "rewards:  -122.0 q-value:  0\n",
      "loss: [6511.1630859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 3628.8818\n",
      "rewards:  -126.0 q-value:  0\n",
      "loss: [3628.8818359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 13811.6133\n",
      "rewards:  -131.0 q-value:  0\n",
      "loss: [13811.61328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 9986.1094\n",
      "rewards:  -136.0 q-value:  0\n",
      "loss: [9986.109375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 11091.9844\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [11091.984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 7813.8164\n",
      "rewards:  -137.0 q-value:  0\n",
      "loss: [7813.81640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 9179.6816\n",
      "rewards:  -143.0 q-value:  0\n",
      "loss: [9179.681640625]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 8553.9580\n",
      "rewards:  -144.0 q-value:  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.1678310e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.2330975e-03 2.9339220e-02 2.3334846e-04 0.0000000e+00\n",
      "  0.0000000e+00 2.1142803e-02 6.2853225e-02 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]]\n",
      "loss: [8553.9580078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 232us/step - loss: 6703.6694\n",
      "rewards:  -149.0 q-value:  0\n",
      "loss: [6703.66943359375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 14197.4746\n",
      "rewards:  -151.0 q-value:  0\n",
      "loss: [14197.474609375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 9760.4443\n",
      "rewards:  -127.0 q-value:  0\n",
      "loss: [9760.4443359375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 16271.7285\n",
      "rewards:  -164.0 q-value:  0\n",
      "loss: [16271.728515625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 12415.3057\n",
      "rewards:  -166.0 q-value:  0\n",
      "loss: [12415.3056640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 13198.2158\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [13198.2158203125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 9195.5420\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [9195.5419921875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 228us/step - loss: 4375.0381\n",
      "rewards:  -203.0 q-value:  0\n",
      "loss: [4375.0380859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 5459.0903\n",
      "rewards:  -206.0 q-value:  0\n",
      "loss: [5459.09033203125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 2603.0225\n",
      "rewards:  -209.0 q-value:  0\n",
      "loss: [2603.0224609375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 6250.8848\n",
      "rewards:  -212.0 q-value:  0\n",
      "loss: [6250.884765625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 7470.5420\n",
      "rewards:  -249.0 q-value:  0\n",
      "loss: [7470.5419921875]\n",
      "Number of actions available 11\n",
      "Episode : 11\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 6000.2871\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [6000.287109375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 4065.0012\n",
      "rewards:  20.0 q-value:  [[0.         0.         0.         0.         0.         0.33865944\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03506874 0.         0.         0.01736718 0.0179013  0.14287263\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4065.001220703125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 11494.0371\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [11494.037109375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 8934.6211\n",
      "rewards:  25.0 q-value:  0\n",
      "loss: [8934.62109375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 10601.3633\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [10601.36328125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 10366.0605\n",
      "rewards:  57.0 q-value:  0\n",
      "loss: [10366.060546875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 5639.4351\n",
      "rewards:  63.0 q-value:  0\n",
      "loss: [5639.43505859375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 9364.4219\n",
      "rewards:  74.0 q-value:  0\n",
      "loss: [9364.421875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 256us/step - loss: 8941.7949\n",
      "rewards:  63.0 q-value:  0\n",
      "loss: [8941.794921875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 6660.3760\n",
      "rewards:  58.0 q-value:  0\n",
      "loss: [6660.3759765625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 10524.6289\n",
      "rewards:  52.0 q-value:  0\n",
      "loss: [10524.62890625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 8725.3750\n",
      "rewards:  68.0 q-value:  0\n",
      "loss: [8725.375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 7475.1377\n",
      "rewards:  67.0 q-value:  0\n",
      "loss: [7475.1376953125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 2665.4634\n",
      "rewards:  77.0 q-value:  0\n",
      "loss: [2665.46337890625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 10324.0771\n",
      "rewards:  90.0 q-value:  0\n",
      "loss: [10324.0771484375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 17678.1445\n",
      "rewards:  95.0 q-value:  0\n",
      "loss: [17678.14453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 10604.4531\n",
      "rewards:  97.0 q-value:  0\n",
      "loss: [10604.453125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 9253.1807\n",
      "rewards:  86.0 q-value:  [[0.         0.         0.         0.         0.         0.25365302\n",
      "  0.00620678 0.         0.         0.         0.         0.\n",
      "  0.04587608 0.00420631 0.         0.00801057 0.03619569 0.12165054\n",
      "  0.         0.         0.        ]]\n",
      "loss: [9253.1806640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 15208.4385\n",
      "rewards:  85.0 q-value:  0\n",
      "loss: [15208.4384765625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 11142.5361\n",
      "rewards:  110.0 q-value:  0\n",
      "loss: [11142.5361328125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 10950.4658\n",
      "rewards:  114.0 q-value:  0\n",
      "loss: [10950.4658203125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 7042.3525\n",
      "rewards:  150.0 q-value:  [[0.         0.         0.         0.         0.         0.29793623\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05579024 0.         0.         0.         0.01358375 0.11403672\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7042.3525390625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 15685.6562\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [15685.65625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 14203.2500\n",
      "rewards:  160.0 q-value:  0\n",
      "loss: [14203.25]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 7054.0840\n",
      "rewards:  179.0 q-value:  0\n",
      "loss: [7054.083984375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 263us/step - loss: 13218.1777\n",
      "rewards:  173.0 q-value:  0\n",
      "loss: [13218.177734375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 11243.9297\n",
      "rewards:  175.0 q-value:  0\n",
      "loss: [11243.9296875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 6373.5273\n",
      "rewards:  168.0 q-value:  0\n",
      "loss: [6373.52734375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 12372.8301\n",
      "rewards:  168.0 q-value:  0\n",
      "loss: [12372.830078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 11217.0029\n",
      "rewards:  170.0 q-value:  0\n",
      "loss: [11217.0029296875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 192us/step - loss: 12147.6963\n",
      "rewards:  192.0 q-value:  0\n",
      "loss: [12147.6962890625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 6116.4775\n",
      "rewards:  187.0 q-value:  [[0.         0.         0.         0.         0.         0.5422842\n",
      "  0.         0.         0.         0.         0.         0.02863971\n",
      "  0.0993269  0.         0.         0.         0.13222808 0.20986186\n",
      "  0.         0.01451388 0.        ]]\n",
      "loss: [6116.4775390625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 9745.9717\n",
      "rewards:  177.0 q-value:  0\n",
      "loss: [9745.9716796875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 260us/step - loss: 1741.9171\n",
      "rewards:  147.0 q-value:  0\n",
      "loss: [1741.9171142578125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 21499.4062\n",
      "rewards:  147.0 q-value:  0\n",
      "loss: [21499.40625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 2122.3892\n",
      "rewards:  146.0 q-value:  0\n",
      "loss: [2122.38916015625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 10799.2842\n",
      "rewards:  158.0 q-value:  0\n",
      "loss: [10799.2841796875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 230us/step - loss: 18627.8086\n",
      "rewards:  157.0 q-value:  0\n",
      "loss: [18627.80859375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 6427.3911\n",
      "rewards:  181.0 q-value:  0\n",
      "loss: [6427.39111328125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 3186.6438\n",
      "rewards:  201.0 q-value:  0\n",
      "loss: [3186.643798828125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 217us/step - loss: 5118.9463\n",
      "rewards:  198.0 q-value:  0\n",
      "loss: [5118.9462890625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 5016.3169\n",
      "rewards:  196.0 q-value:  0\n",
      "loss: [5016.31689453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 9959.8057\n",
      "rewards:  204.0 q-value:  0\n",
      "loss: [9959.8056640625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 8914.1904\n",
      "rewards:  216.0 q-value:  0\n",
      "loss: [8914.1904296875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 6386.3896\n",
      "rewards:  211.0 q-value:  0\n",
      "loss: [6386.3896484375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 11719.1025\n",
      "rewards:  195.0 q-value:  0\n",
      "loss: [11719.1025390625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 8935.3027\n",
      "rewards:  169.0 q-value:  0\n",
      "loss: [8935.302734375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 4646.9414\n",
      "rewards:  168.0 q-value:  0\n",
      "loss: [4646.94140625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 2670.5005\n",
      "rewards:  172.0 q-value:  0\n",
      "loss: [2670.50048828125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 6179.4268\n",
      "rewards:  167.0 q-value:  0\n",
      "loss: [6179.4267578125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 14950.5371\n",
      "rewards:  191.0 q-value:  0\n",
      "loss: [14950.537109375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 9154.1074\n",
      "rewards:  193.0 q-value:  0\n",
      "loss: [9154.107421875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 316us/step - loss: 3503.8081\n",
      "rewards:  176.0 q-value:  0\n",
      "loss: [3503.80810546875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 8113.6572\n",
      "rewards:  181.0 q-value:  0\n",
      "loss: [8113.6572265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 7214.3081\n",
      "rewards:  176.0 q-value:  0\n",
      "loss: [7214.30810546875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 236us/step - loss: 10552.8340\n",
      "rewards:  185.0 q-value:  0\n",
      "loss: [10552.833984375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 6918.6748\n",
      "rewards:  189.0 q-value:  0\n",
      "loss: [6918.6748046875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 1278.8982\n",
      "rewards:  178.0 q-value:  0\n",
      "loss: [1278.898193359375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 219us/step - loss: 4423.4375\n",
      "rewards:  171.0 q-value:  [[0.         0.         0.         0.         0.         0.42575213\n",
      "  0.         0.         0.         0.         0.         0.030217\n",
      "  0.05333678 0.         0.         0.         0.07585484 0.20109415\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4423.4375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 2090.9478\n",
      "rewards:  168.0 q-value:  0\n",
      "loss: [2090.94775390625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 15188.5566\n",
      "rewards:  165.0 q-value:  0\n",
      "loss: [15188.556640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 235us/step - loss: 10096.5254\n",
      "rewards:  176.0 q-value:  0\n",
      "loss: [10096.525390625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 3717.9526\n",
      "rewards:  141.0 q-value:  0\n",
      "loss: [3717.95263671875]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 2037.7983\n",
      "rewards:  140.0 q-value:  0\n",
      "loss: [2037.79833984375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 9778.2939\n",
      "rewards:  139.0 q-value:  0\n",
      "loss: [9778.2939453125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 12503.2305\n",
      "rewards:  163.0 q-value:  0\n",
      "loss: [12503.23046875]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 280us/step - loss: 6177.1333\n",
      "rewards:  161.0 q-value:  [[0.         0.         0.         0.         0.         0.34772414\n",
      "  0.         0.         0.         0.         0.         0.00770267\n",
      "  0.0809109  0.         0.         0.         0.06697267 0.14964747\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6177.13330078125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 390us/step - loss: 3899.4863\n",
      "rewards:  159.0 q-value:  0\n",
      "loss: [3899.486328125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 1292.3643\n",
      "rewards:  191.0 q-value:  0\n",
      "loss: [1292.3642578125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 6997.3403\n",
      "rewards:  180.0 q-value:  0\n",
      "loss: [6997.34033203125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 294us/step - loss: 1057.0568\n",
      "rewards:  196.0 q-value:  0\n",
      "loss: [1057.0567626953125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 8688.2666\n",
      "rewards:  194.0 q-value:  0\n",
      "loss: [8688.2666015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 85us/step - loss: 15088.9570\n",
      "rewards:  189.0 q-value:  0\n",
      "loss: [15088.95703125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 4766.8086\n",
      "rewards:  174.0 q-value:  0\n",
      "loss: [4766.80859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 6540.0425\n",
      "rewards:  169.0 q-value:  0\n",
      "loss: [6540.04248046875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 8753.2227\n",
      "rewards:  162.0 q-value:  0\n",
      "loss: [8753.22265625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 1309.4565\n",
      "rewards:  145.0 q-value:  0\n",
      "loss: [1309.45654296875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 8964.5107\n",
      "rewards:  155.0 q-value:  0\n",
      "loss: [8964.5107421875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 261us/step - loss: 2212.8071\n",
      "rewards:  153.0 q-value:  0\n",
      "loss: [2212.80712890625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 8981.4688\n",
      "rewards:  148.0 q-value:  0\n",
      "loss: [8981.46875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 1956.6790\n",
      "rewards:  143.0 q-value:  0\n",
      "loss: [1956.678955078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 7446.3643\n",
      "rewards:  137.0 q-value:  0\n",
      "loss: [7446.3642578125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 4966.1650\n",
      "rewards:  163.0 q-value:  0\n",
      "loss: [4966.1650390625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 3998.8611\n",
      "rewards:  187.0 q-value:  0\n",
      "loss: [3998.861083984375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 3079.8340\n",
      "rewards:  181.0 q-value:  0\n",
      "loss: [3079.833984375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 7911.3989\n",
      "rewards:  193.0 q-value:  0\n",
      "loss: [7911.39892578125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 3545.6626\n",
      "rewards:  190.0 q-value:  [[0.         0.         0.         0.         0.         0.42022723\n",
      "  0.         0.         0.         0.         0.         0.02998855\n",
      "  0.05980297 0.         0.         0.         0.07384732 0.20287138\n",
      "  0.         0.03687596 0.        ]]\n",
      "loss: [3545.66259765625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 11666.3008\n",
      "rewards:  179.0 q-value:  0\n",
      "loss: [11666.30078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 11486.8564\n",
      "rewards:  203.0 q-value:  0\n",
      "loss: [11486.8564453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 3409.2771\n",
      "rewards:  198.0 q-value:  0\n",
      "loss: [3409.277099609375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 15162.3184\n",
      "rewards:  206.0 q-value:  0\n",
      "loss: [15162.318359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 9947.9531\n",
      "rewards:  200.0 q-value:  0\n",
      "loss: [9947.953125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 7590.4141\n",
      "rewards:  204.0 q-value:  0\n",
      "loss: [7590.4140625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 2958.2749\n",
      "rewards:  208.0 q-value:  0\n",
      "loss: [2958.27490234375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 8365.5986\n",
      "rewards:  207.0 q-value:  0\n",
      "loss: [8365.5986328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 10602.5957\n",
      "rewards:  191.0 q-value:  0\n",
      "loss: [10602.595703125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 192us/step - loss: 5206.3926\n",
      "rewards:  186.0 q-value:  0\n",
      "loss: [5206.392578125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 8916.5293\n",
      "rewards:  214.0 q-value:  0\n",
      "loss: [8916.529296875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 5185.2842\n",
      "rewards:  208.0 q-value:  0\n",
      "loss: [5185.2841796875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 9642.2930\n",
      "rewards:  212.0 q-value:  0\n",
      "loss: [9642.29296875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 9791.2578\n",
      "rewards:  205.0 q-value:  0\n",
      "loss: [9791.2578125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 10075.3564\n",
      "rewards:  204.0 q-value:  0\n",
      "loss: [10075.3564453125]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 7388.8340\n",
      "rewards:  228.0 q-value:  0\n",
      "loss: [7388.833984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 7479.3311\n",
      "rewards:  223.0 q-value:  0\n",
      "loss: [7479.3310546875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 9863.9766\n",
      "rewards:  208.0 q-value:  0\n",
      "loss: [9863.9765625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 17877.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  236.0 q-value:  [[0.         0.         0.         0.         0.         0.3609627\n",
      "  0.         0.         0.         0.         0.         0.0036481\n",
      "  0.09235047 0.         0.         0.         0.05774345 0.22048181\n",
      "  0.         0.0085496  0.        ]]\n",
      "loss: [17877.109375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 173us/step - loss: 1648.5784\n",
      "rewards:  235.0 q-value:  [[0.         0.         0.         0.         0.         0.55208457\n",
      "  0.         0.         0.         0.         0.         0.03216945\n",
      "  0.13238113 0.         0.         0.         0.17012155 0.22591165\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1648.578369140625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 7003.4873\n",
      "rewards:  227.0 q-value:  0\n",
      "loss: [7003.4873046875]\n",
      "Number of actions available 9\n",
      "Episode : 12\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 5919.3833\n",
      "rewards:  26.0 q-value:  [[0.         0.         0.         0.         0.         0.47776443\n",
      "  0.         0.         0.         0.         0.         0.04733036\n",
      "  0.07174876 0.         0.         0.         0.11875437 0.17243308\n",
      "  0.         0.01938195 0.        ]]\n",
      "loss: [5919.38330078125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 1915.5828\n",
      "rewards:  15.0 q-value:  [[0.         0.         0.         0.         0.         0.17506921\n",
      "  0.         0.         0.         0.         0.         0.00291352\n",
      "  0.01215059 0.         0.         0.         0.         0.07864267\n",
      "  0.         0.02691013 0.        ]]\n",
      "loss: [1915.582763671875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 10860.8174\n",
      "rewards:  17.0 q-value:  0\n",
      "loss: [10860.8173828125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 10445.5947\n",
      "rewards:  16.0 q-value:  0\n",
      "loss: [10445.5947265625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 7245.3252\n",
      "rewards:  12.0 q-value:  0\n",
      "loss: [7245.3251953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 19270.0820\n",
      "rewards:  7.0 q-value:  0\n",
      "loss: [19270.08203125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 7032.7271\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [7032.72705078125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 3370.5291\n",
      "rewards:  3.0 q-value:  0\n",
      "loss: [3370.529052734375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 6295.5269\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [6295.52685546875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 9598.9756\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [9598.9755859375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 10242.4150\n",
      "rewards:  -4.0 q-value:  0\n",
      "loss: [10242.4150390625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 5860.2622\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [5860.26220703125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 15197.7295\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [15197.7294921875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 85us/step - loss: 19213.7383\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [19213.73828125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 7211.1777\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [7211.177734375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 11973.7129\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [11973.712890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 2527.3999\n",
      "rewards:  3.0 q-value:  0\n",
      "loss: [2527.39990234375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 4299.4932\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [4299.4931640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 10485.9688\n",
      "rewards:  -16.0 q-value:  0\n",
      "loss: [10485.96875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 13133.1289\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [13133.12890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 3059.6724\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [3059.67236328125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 1638.1794\n",
      "rewards:  2.0 q-value:  0\n",
      "loss: [1638.179443359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 8921.7822\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [8921.7822265625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 218us/step - loss: 2559.0759\n",
      "rewards:  -8.0 q-value:  0\n",
      "loss: [2559.075927734375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 1480.5154\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [1480.515380859375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 8719.1826\n",
      "rewards:  -19.0 q-value:  [[0.         0.         0.         0.         0.         0.27779555\n",
      "  0.         0.         0.         0.         0.         0.00904721\n",
      "  0.03512071 0.         0.         0.         0.0638646  0.08145127\n",
      "  0.         0.         0.        ]]\n",
      "loss: [8719.1826171875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 178us/step - loss: 12472.2812\n",
      "rewards:  -19.0 q-value:  0\n",
      "loss: [12472.28125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 206us/step - loss: 12089.0859\n",
      "rewards:  -27.0 q-value:  0\n",
      "loss: [12089.0859375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 5176.4033\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [5176.4033203125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 380us/step - loss: 10613.9434\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [10613.943359375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 6708.7798\n",
      "rewards:  -31.0 q-value:  0\n",
      "loss: [6708.77978515625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 3751.1121\n",
      "rewards:  -32.0 q-value:  0\n",
      "loss: [3751.112060546875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 5845.4004\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [5845.400390625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 3169.9849\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [3169.98486328125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 8825.8721\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [8825.8720703125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 13066.9551\n",
      "rewards:  -8.0 q-value:  0\n",
      "loss: [13066.955078125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 4699.5957\n",
      "rewards:  -14.0 q-value:  0\n",
      "loss: [4699.595703125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 9749.8008\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [9749.80078125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 3528.0898\n",
      "rewards:  5.0 q-value:  0\n",
      "loss: [3528.08984375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 10100.1240\n",
      "rewards:  -10.0 q-value:  0\n",
      "loss: [10100.1240234375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 4363.5840\n",
      "rewards:  2.0 q-value:  [[0.         0.         0.         0.         0.         0.43471104\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08378413 0.         0.         0.         0.07959609 0.17669404\n",
      "  0.         0.04391154 0.        ]]\n",
      "loss: [4363.583984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 17081.6758\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [17081.67578125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 223us/step - loss: 13819.0684\n",
      "rewards:  -4.0 q-value:  0\n",
      "loss: [13819.068359375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 20235.2402\n",
      "rewards:  30.0 q-value:  0\n",
      "loss: [20235.240234375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 107us/step - loss: 11654.8164\n",
      "rewards:  17.0 q-value:  0\n",
      "loss: [11654.81640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 8539.3799\n",
      "rewards:  11.0 q-value:  0\n",
      "loss: [8539.3798828125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 10752.8115\n",
      "rewards:  10.0 q-value:  0\n",
      "loss: [10752.8115234375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 5869.3252\n",
      "rewards:  -2.0 q-value:  0\n",
      "loss: [5869.3251953125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 227us/step - loss: 1780.5630\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [1780.56298828125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 266us/step - loss: 5983.4688\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [5983.46875]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 12795.3242\n",
      "rewards:  -19.0 q-value:  [[0.         0.         0.         0.         0.         0.26410547\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01946468 0.         0.03572112 0.         0.         0.17869313\n",
      "  0.         0.04511077 0.        ]]\n",
      "loss: [12795.32421875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 9609.1553\n",
      "rewards:  -24.0 q-value:  0\n",
      "loss: [9609.1552734375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 216us/step - loss: 9513.7070\n",
      "rewards:  -26.0 q-value:  0\n",
      "loss: [9513.70703125]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 7417.5371\n",
      "rewards:  -36.0 q-value:  0\n",
      "loss: [7417.537109375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 5095.9600\n",
      "rewards:  -14.0 q-value:  0\n",
      "loss: [5095.9599609375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 5047.7837\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [5047.78369140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 4566.1855\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [4566.185546875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 1054.6936\n",
      "rewards:  -18.0 q-value:  0\n",
      "loss: [1054.693603515625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 13030.9717\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [13030.9716796875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 5241.9561\n",
      "rewards:  -7.0 q-value:  0\n",
      "loss: [5241.9560546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 10275.7129\n",
      "rewards:  -4.0 q-value:  0\n",
      "loss: [10275.712890625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 3830.3152\n",
      "rewards:  36.0 q-value:  [[0.         0.         0.         0.         0.         0.44621328\n",
      "  0.         0.         0.         0.         0.         0.06421748\n",
      "  0.07791039 0.         0.         0.         0.11461341 0.22124563\n",
      "  0.         0.         0.        ]]\n",
      "loss: [3830.315185546875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 8796.2617\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [8796.26171875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 5908.0205\n",
      "rewards:  -8.0 q-value:  0\n",
      "loss: [5908.0205078125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 227us/step - loss: 1151.9026\n",
      "rewards:  0.0 q-value:  0\n",
      "loss: [1151.902587890625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 183us/step - loss: 6997.9062\n",
      "rewards:  -4.0 q-value:  0\n",
      "loss: [6997.90625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 4946.1084\n",
      "rewards:  10.0 q-value:  [[0.         0.         0.         0.         0.         0.3654755\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05009797 0.         0.         0.0032334  0.02182764 0.15115103\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4946.1083984375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 5381.0889\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [5381.0888671875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 10239.9590\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [10239.958984375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 13313.7822\n",
      "rewards:  -19.0 q-value:  0\n",
      "loss: [13313.7822265625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 18098.5352\n",
      "rewards:  -22.0 q-value:  0\n",
      "loss: [18098.53515625]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 2039.6855\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [2039.685546875]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 7002.3115\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [7002.3115234375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 310us/step - loss: 22348.3730\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [22348.373046875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 3750.3123\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [3750.312255859375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 3305.5898\n",
      "rewards:  -14.0 q-value:  0\n",
      "loss: [3305.58984375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 17237.4844\n",
      "rewards:  -23.0 q-value:  0\n",
      "loss: [17237.484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 8883.8096\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [8883.8095703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 186us/step - loss: 3185.8472\n",
      "rewards:  -33.0 q-value:  0\n",
      "loss: [3185.84716796875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 9769.4121\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [9769.412109375]\n",
      "Exploring\n",
      "Selected action  [3 0]\n",
      "[3 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 4991.8530\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [4991.85302734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 2622.2456\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [2622.24560546875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 8648.8945\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [8648.89453125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 255us/step - loss: 6899.0908\n",
      "rewards:  -78.0 q-value:  0\n",
      "loss: [6899.0908203125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 7831.3384\n",
      "rewards:  -74.0 q-value:  0\n",
      "loss: [7831.33837890625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 8614.5361\n",
      "rewards:  -77.0 q-value:  0\n",
      "loss: [8614.5361328125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 7970.6001\n",
      "rewards:  -88.0 q-value:  0\n",
      "loss: [7970.60009765625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 7900.3418\n",
      "rewards:  -86.0 q-value:  [[0.         0.         0.         0.         0.         0.3282604\n",
      "  0.         0.         0.         0.         0.         0.01830234\n",
      "  0.04670671 0.         0.         0.         0.05038361 0.17151774\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7900.341796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 79us/step - loss: 4422.3159\n",
      "rewards:  -91.0 q-value:  0\n",
      "loss: [4422.31591796875]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 11250.8682\n",
      "rewards:  -62.0 q-value:  [[0.         0.         0.         0.         0.         0.17813192\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0278997  0.         0.         0.01525487 0.         0.11890816\n",
      "  0.         0.02513181 0.        ]]\n",
      "loss: [11250.8681640625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 84us/step - loss: 1050.7593\n",
      "rewards:  -68.0 q-value:  0\n",
      "loss: [1050.75927734375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 4916.2021\n",
      "rewards:  -70.0 q-value:  0\n",
      "loss: [4916.2021484375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 4952.8247\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [4952.82470703125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 4910.4492\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [4910.44921875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 16416.3496\n",
      "rewards:  -78.0 q-value:  0\n",
      "loss: [16416.349609375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 14783.7715\n",
      "rewards:  -101.0 q-value:  0\n",
      "loss: [14783.771484375]\n",
      "Number of actions available 6\n",
      "Episode : 13\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 4834.1934\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [4834.193359375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 4689.0615\n",
      "rewards:  -10.0 q-value:  [[0.         0.         0.         0.         0.         0.32555285\n",
      "  0.         0.         0.         0.         0.         0.00365019\n",
      "  0.07751483 0.         0.         0.         0.07195448 0.14668068\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4689.0615234375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 8625.2246\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [8625.224609375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 9234.0977\n",
      "rewards:  -39.0 q-value:  0\n",
      "loss: [9234.09765625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 6776.2129\n",
      "rewards:  -49.0 q-value:  0\n",
      "loss: [6776.212890625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 5498.3604\n",
      "rewards:  -43.0 q-value:  0\n",
      "loss: [5498.3603515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 11668.2852\n",
      "rewards:  -48.0 q-value:  0\n",
      "loss: [11668.28515625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 9310.1836\n",
      "rewards:  -55.0 q-value:  0\n",
      "loss: [9310.18359375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 8038.6045\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [8038.6044921875]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 5720.1348\n",
      "rewards:  -24.0 q-value:  [[0.         0.         0.         0.         0.         0.25893188\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.022562   0.         0.         0.02436956 0.00736897 0.09070774\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5720.134765625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 96us/step - loss: 8742.2793\n",
      "rewards:  -12.0 q-value:  0\n",
      "loss: [8742.279296875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 10849.4189\n",
      "rewards:  -16.0 q-value:  0\n",
      "loss: [10849.4189453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 5781.4385\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [5781.4384765625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 145us/step - loss: 5602.7866\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [5602.78662109375]\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 1262.3407\n",
      "rewards:  -31.0 q-value:  [[0.         0.         0.         0.         0.         0.29088378\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06659155 0.         0.         0.         0.05489898 0.08966336\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1262.3406982421875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 11629.0117\n",
      "rewards:  -15.0 q-value:  0\n",
      "loss: [11629.01171875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 9872.6885\n",
      "rewards:  -20.0 q-value:  0\n",
      "loss: [9872.6884765625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 11791.8652\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [11791.865234375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 12890.6348\n",
      "rewards:  -6.0 q-value:  0\n",
      "loss: [12890.634765625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 5689.4043\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [5689.404296875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 4085.9683\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [4085.96826171875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 6808.3022\n",
      "rewards:  -46.0 q-value:  0\n",
      "loss: [6808.30224609375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 2822.6831\n",
      "rewards:  -76.0 q-value:  0\n",
      "loss: [2822.68310546875]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 2532.7153\n",
      "rewards:  -72.0 q-value:  [[0.         0.         0.         0.         0.         0.28709072\n",
      "  0.         0.         0.         0.         0.         0.01576093\n",
      "  0.03627099 0.         0.         0.00307682 0.03468905 0.1848255\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2532.71533203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 5802.5503\n",
      "rewards:  -72.0 q-value:  0\n",
      "loss: [5802.55029296875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 6534.3608\n",
      "rewards:  -108.0 q-value:  0\n",
      "loss: [6534.36083984375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 2512.0830\n",
      "rewards:  -109.0 q-value:  0\n",
      "loss: [2512.0830078125]\n",
      "Exploiting\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 7872.8394\n",
      "rewards:  -139.0 q-value:  [[0.         0.         0.         0.         0.         0.33562374\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03711441 0.         0.         0.         0.04241926 0.12336116\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7872.83935546875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 5983.0156\n",
      "rewards:  -147.0 q-value:  0\n",
      "loss: [5983.015625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 16314.8037\n",
      "rewards:  -158.0 q-value:  0\n",
      "loss: [16314.8037109375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 208us/step - loss: 4954.1431\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [4954.14306640625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 6457.5693\n",
      "rewards:  -162.0 q-value:  0\n",
      "loss: [6457.5693359375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 5789.7549\n",
      "rewards:  -170.0 q-value:  0\n",
      "loss: [5789.7548828125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 9036.9043\n",
      "rewards:  -146.0 q-value:  0\n",
      "loss: [9036.904296875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 14604.5479\n",
      "rewards:  -142.0 q-value:  0\n",
      "loss: [14604.5478515625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 4566.7529\n",
      "rewards:  -153.0 q-value:  0\n",
      "loss: [4566.7529296875]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 7772.9570\n",
      "rewards:  -153.0 q-value:  0\n",
      "loss: [7772.95703125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 8388.7881\n",
      "rewards:  -131.0 q-value:  0\n",
      "loss: [8388.7880859375]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 16717.0273\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [16717.02734375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 10997.1963\n",
      "rewards:  -167.0 q-value:  0\n",
      "loss: [10997.1962890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 151us/step - loss: 5890.4355\n",
      "rewards:  -172.0 q-value:  0\n",
      "loss: [5890.435546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 7699.0469\n",
      "rewards:  -177.0 q-value:  0\n",
      "loss: [7699.046875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 7774.8638\n",
      "rewards:  -187.0 q-value:  0\n",
      "loss: [7774.86376953125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 2067.4932\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [2067.4931640625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 16596.1953\n",
      "rewards:  -153.0 q-value:  0\n",
      "loss: [16596.1953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 1528.4465\n",
      "rewards:  -158.0 q-value:  0\n",
      "loss: [1528.446533203125]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 4550.7300\n",
      "rewards:  -151.0 q-value:  0\n",
      "loss: [4550.72998046875]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 9817.2852\n",
      "rewards:  -147.0 q-value:  0\n",
      "loss: [9817.28515625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 1065.0984\n",
      "rewards:  -117.0 q-value:  0\n",
      "loss: [1065.098388671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 8897.5400\n",
      "rewards:  -122.0 q-value:  0\n",
      "loss: [8897.5400390625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 5809.1304\n",
      "rewards:  -132.0 q-value:  0\n",
      "loss: [5809.13037109375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 6629.8501\n",
      "rewards:  -136.0 q-value:  0\n",
      "loss: [6629.85009765625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 11415.8340\n",
      "rewards:  -142.0 q-value:  0\n",
      "loss: [11415.833984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 3185.4766\n",
      "rewards:  -147.0 q-value:  0\n",
      "loss: [3185.4765625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 6167.1260\n",
      "rewards:  -144.0 q-value:  0\n",
      "loss: [6167.1259765625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 5144.4736\n",
      "rewards:  -169.0 q-value:  0\n",
      "loss: [5144.4736328125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 5295.2764\n",
      "rewards:  -172.0 q-value:  0\n",
      "loss: [5295.2763671875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 9032.8369\n",
      "rewards:  -156.0 q-value:  0\n",
      "loss: [9032.8369140625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 5391.0591\n",
      "rewards:  -203.0 q-value:  0\n",
      "loss: [5391.05908203125]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 3937.0200\n",
      "rewards:  -208.0 q-value:  0\n",
      "loss: [3937.02001953125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 2861.1296\n",
      "rewards:  -196.0 q-value:  0\n",
      "loss: [2861.129638671875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 4372.7446\n",
      "rewards:  -180.0 q-value:  0\n",
      "loss: [4372.74462890625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 251us/step - loss: 2359.1345\n",
      "rewards:  -176.0 q-value:  0\n",
      "loss: [2359.134521484375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 2285.4009\n",
      "rewards:  -160.0 q-value:  0\n",
      "loss: [2285.40087890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 5306.3945\n",
      "rewards:  -165.0 q-value:  0\n",
      "loss: [5306.39453125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 5623.2671\n",
      "rewards:  -159.0 q-value:  0\n",
      "loss: [5623.26708984375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 139us/step - loss: 4097.9253\n",
      "rewards:  -164.0 q-value:  [[0.         0.         0.         0.         0.         0.4171559\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09747741 0.         0.         0.         0.06633712 0.22108889\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4097.92529296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 10436.4346\n",
      "rewards:  -169.0 q-value:  0\n",
      "loss: [10436.4345703125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 5393.8398\n",
      "rewards:  -193.0 q-value:  0\n",
      "loss: [5393.83984375]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 8142.8906\n",
      "rewards:  -198.0 q-value:  [[0.         0.         0.         0.         0.         0.24338704\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03234629 0.         0.         0.00317863 0.02437754 0.09916785\n",
      "  0.         0.         0.        ]]\n",
      "loss: [8142.890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 4992.7568\n",
      "rewards:  -203.0 q-value:  0\n",
      "loss: [4992.7568359375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 249us/step - loss: 9794.9629\n",
      "rewards:  -213.0 q-value:  0\n",
      "loss: [9794.962890625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 6458.6577\n",
      "rewards:  -219.0 q-value:  0\n",
      "loss: [6458.65771484375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 17188.7207\n",
      "rewards:  -249.0 q-value:  0\n",
      "loss: [17188.720703125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 1749.6960\n",
      "rewards:  -289.0 q-value:  0\n",
      "loss: [1749.696044921875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 8617.4150\n",
      "rewards:  -294.0 q-value:  0\n",
      "loss: [8617.4150390625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 9094.7734\n",
      "rewards:  -324.0 q-value:  0\n",
      "loss: [9094.7734375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 15353.9893\n",
      "rewards:  -332.0 q-value:  0\n",
      "loss: [15353.9892578125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 8186.3237\n",
      "rewards:  -333.0 q-value:  0\n",
      "loss: [8186.32373046875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 134us/step - loss: 1306.6777\n",
      "rewards:  -338.0 q-value:  0\n",
      "loss: [1306.677734375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 1314.2463\n",
      "rewards:  -351.0 q-value:  0\n",
      "loss: [1314.246337890625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 14302.4941\n",
      "rewards:  -338.0 q-value:  0\n",
      "loss: [14302.494140625]\n",
      "Exploring\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 9058.5654\n",
      "rewards:  -326.0 q-value:  0\n",
      "loss: [9058.5654296875]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 15265.5820\n",
      "rewards:  -332.0 q-value:  [[0.         0.         0.         0.         0.         0.2303268\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04307101 0.         0.         0.         0.01000815 0.09755167\n",
      "  0.         0.01748317 0.        ]]\n",
      "loss: [15265.58203125]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 5163.6758\n",
      "rewards:  -343.0 q-value:  0\n",
      "loss: [5163.67578125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 107us/step - loss: 5337.2446\n",
      "rewards:  -311.0 q-value:  0\n",
      "loss: [5337.24462890625]\n",
      "Exploiting\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 110us/step - loss: 4786.4399\n",
      "rewards:  -291.0 q-value:  [[0.         0.         0.         0.         0.         0.22601348\n",
      "  0.         0.         0.         0.         0.         0.00781164\n",
      "  0.01823722 0.         0.         0.03933788 0.00237782 0.09227333\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4786.43994140625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 17159.7148\n",
      "rewards:  -296.0 q-value:  [[0.         0.         0.         0.         0.         0.32328337\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06712755 0.         0.         0.         0.03491706 0.1462839\n",
      "  0.         0.         0.        ]]\n",
      "loss: [17159.71484375]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 7980.9722\n",
      "rewards:  -290.0 q-value:  0\n",
      "loss: [7980.97216796875]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 8311.1758\n",
      "rewards:  -258.0 q-value:  0\n",
      "loss: [8311.17578125]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 3857.2500\n",
      "rewards:  -250.0 q-value:  0\n",
      "loss: [3857.25]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 9346.4727\n",
      "rewards:  -218.0 q-value:  0\n",
      "loss: [9346.47265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 3015.8899\n",
      "rewards:  -223.0 q-value:  0\n",
      "loss: [3015.889892578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 15245.9102\n",
      "rewards:  -228.0 q-value:  0\n",
      "loss: [15245.91015625]\n",
      "Exploiting\n",
      "Selected action  [1 3]\n",
      "[1 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 6714.9473\n",
      "rewards:  -269.0 q-value:  [[0.         0.         0.         0.         0.         0.18109712\n",
      "  0.02568549 0.         0.         0.         0.         0.\n",
      "  0.01589525 0.00519646 0.         0.00733221 0.02744439 0.07045053\n",
      "  0.         0.         0.        ]]\n",
      "loss: [6714.947265625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 3695.1206\n",
      "rewards:  -274.0 q-value:  0\n",
      "loss: [3695.12060546875]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 9946.8779\n",
      "rewards:  -281.0 q-value:  0\n",
      "loss: [9946.8779296875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 11026.6533\n",
      "rewards:  -241.0 q-value:  0\n",
      "loss: [11026.6533203125]\n",
      "Exploiting\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 9211.8447\n",
      "rewards:  -245.0 q-value:  [[0.         0.         0.         0.         0.         0.38178957\n",
      "  0.         0.         0.         0.         0.         0.02946359\n",
      "  0.04100212 0.         0.         0.         0.08804096 0.18420273\n",
      "  0.         0.         0.        ]]\n",
      "loss: [9211.8447265625]\n",
      "Exploring\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 7289.6235\n",
      "rewards:  -238.0 q-value:  0\n",
      "loss: [7289.62353515625]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 14272.7764\n",
      "rewards:  -214.0 q-value:  0\n",
      "loss: [14272.7763671875]\n",
      "Exploiting\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 2669.7146\n",
      "rewards:  -241.0 q-value:  [[0.         0.         0.         0.         0.         0.41667795\n",
      "  0.         0.         0.         0.         0.         0.02298111\n",
      "  0.04150574 0.         0.         0.         0.06532964 0.18371812\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2669.714599609375]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 3794.0068\n",
      "rewards:  -243.0 q-value:  0\n",
      "loss: [3794.0068359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 8824.3848\n",
      "rewards:  -248.0 q-value:  0\n",
      "loss: [8824.384765625]\n",
      "Exploring\n",
      "Selected action  [4 1]\n",
      "[4 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 11550.3350\n",
      "rewards:  -253.0 q-value:  0\n",
      "loss: [11550.3349609375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 7693.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -256.0 q-value:  0\n",
      "loss: [7693.49609375]\n",
      "Exploring\n",
      "Selected action  [2 3]\n",
      "[2 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 6601.6079\n",
      "rewards:  -259.0 q-value:  0\n",
      "loss: [6601.60791015625]\n",
      "Number of actions available 13\n",
      "Episode : 14\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 4127.1567\n",
      "rewards:  26.0 q-value:  [[0.         0.         0.         0.         0.         0.47776443\n",
      "  0.         0.         0.         0.         0.         0.04733036\n",
      "  0.07174876 0.         0.         0.         0.11875437 0.17243308\n",
      "  0.         0.01938195 0.        ]]\n",
      "loss: [4127.15673828125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 282us/step - loss: 6362.3848\n",
      "rewards:  34.0 q-value:  0\n",
      "loss: [6362.384765625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 3953.9932\n",
      "rewards:  32.0 q-value:  0\n",
      "loss: [3953.9931640625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 8997.9414\n",
      "rewards:  76.0 q-value:  [[0.         0.         0.         0.         0.         0.31518963\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03617282 0.         0.         0.         0.         0.09376602\n",
      "  0.         0.         0.        ]]\n",
      "loss: [8997.94140625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 7981.0762\n",
      "rewards:  69.0 q-value:  0\n",
      "loss: [7981.076171875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 6255.2642\n",
      "rewards:  77.0 q-value:  0\n",
      "loss: [6255.26416015625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 8441.0820\n",
      "rewards:  96.0 q-value:  0\n",
      "loss: [8441.08203125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 189us/step - loss: 1638.2642\n",
      "rewards:  90.0 q-value:  0\n",
      "loss: [1638.26416015625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 8338.3301\n",
      "rewards:  83.0 q-value:  0\n",
      "loss: [8338.330078125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 434us/step - loss: 7963.5088\n",
      "rewards:  80.0 q-value:  0\n",
      "loss: [7963.5087890625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 1597.7301\n",
      "rewards:  84.0 q-value:  0\n",
      "loss: [1597.7301025390625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 212us/step - loss: 7511.9531\n",
      "rewards:  79.0 q-value:  0\n",
      "loss: [7511.953125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 9508.0645\n",
      "rewards:  99.0 q-value:  0\n",
      "loss: [9508.064453125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 5022.7275\n",
      "rewards:  126.0 q-value:  0\n",
      "loss: [5022.7275390625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 230us/step - loss: 5763.5103\n",
      "rewards:  125.0 q-value:  0\n",
      "loss: [5763.51025390625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 15245.3008\n",
      "rewards:  138.0 q-value:  0\n",
      "loss: [15245.30078125]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 3542.8225\n",
      "rewards:  135.0 q-value:  0\n",
      "loss: [3542.822509765625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 11743.4492\n",
      "rewards:  147.0 q-value:  0\n",
      "loss: [11743.44921875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 7690.4482\n",
      "rewards:  136.0 q-value:  0\n",
      "loss: [7690.4482421875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 4878.4282\n",
      "rewards:  137.0 q-value:  0\n",
      "loss: [4878.42822265625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 12981.7764\n",
      "rewards:  137.0 q-value:  0\n",
      "loss: [12981.7763671875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 11532.6221\n",
      "rewards:  129.0 q-value:  0\n",
      "loss: [11532.6220703125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 9419.4580\n",
      "rewards:  117.0 q-value:  0\n",
      "loss: [9419.4580078125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 4812.4785\n",
      "rewards:  117.0 q-value:  0\n",
      "loss: [4812.478515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 93us/step - loss: 4079.7778\n",
      "rewards:  112.0 q-value:  0\n",
      "loss: [4079.77783203125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 253us/step - loss: 12665.3418\n",
      "rewards:  112.0 q-value:  0\n",
      "loss: [12665.341796875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 3703.8918\n",
      "rewards:  107.0 q-value:  0\n",
      "loss: [3703.891845703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 2423.9072\n",
      "rewards:  102.0 q-value:  0\n",
      "loss: [2423.9072265625]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 8424.9102\n",
      "rewards:  90.0 q-value:  0\n",
      "loss: [8424.91015625]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 102us/step - loss: 2945.7363\n",
      "rewards:  89.0 q-value:  0\n",
      "loss: [2945.736328125]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 14020.6582\n",
      "rewards:  84.0 q-value:  0\n",
      "loss: [14020.658203125]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 8233.5137\n",
      "rewards:  88.0 q-value:  0\n",
      "loss: [8233.513671875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 12291.0293\n",
      "rewards:  83.0 q-value:  0\n",
      "loss: [12291.029296875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 15099.5078\n",
      "rewards:  78.0 q-value:  0\n",
      "loss: [15099.5078125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 1701.7085\n",
      "rewards:  66.0 q-value:  0\n",
      "loss: [1701.70849609375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 8012.3394\n",
      "rewards:  57.0 q-value:  0\n",
      "loss: [8012.33935546875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 8747.9688\n",
      "rewards:  81.0 q-value:  0\n",
      "loss: [8747.96875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 85us/step - loss: 8834.8174\n",
      "rewards:  76.0 q-value:  0\n",
      "loss: [8834.8173828125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 105us/step - loss: 5882.2871\n",
      "rewards:  58.0 q-value:  0\n",
      "loss: [5882.287109375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 6708.8154\n",
      "rewards:  58.0 q-value:  0\n",
      "loss: [6708.8154296875]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 88us/step - loss: 3909.2009\n",
      "rewards:  43.0 q-value:  0\n",
      "loss: [3909.200927734375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 9288.2422\n",
      "rewards:  51.0 q-value:  0\n",
      "loss: [9288.2421875]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 7282.3501\n",
      "rewards:  41.0 q-value:  0\n",
      "loss: [7282.35009765625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 1308.0378\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [1308.037841796875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 2908.6318\n",
      "rewards:  47.0 q-value:  0\n",
      "loss: [2908.6318359375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 167us/step - loss: 4754.9941\n",
      "rewards:  27.0 q-value:  0\n",
      "loss: [4754.994140625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 15129.4395\n",
      "rewards:  17.0 q-value:  0\n",
      "loss: [15129.439453125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 11315.5488\n",
      "rewards:  49.0 q-value:  0\n",
      "loss: [11315.548828125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 7358.7661\n",
      "rewards:  55.0 q-value:  0\n",
      "loss: [7358.76611328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 10926.8721\n",
      "rewards:  58.0 q-value:  0\n",
      "loss: [10926.8720703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 982.0330\n",
      "rewards:  53.0 q-value:  0\n",
      "loss: [982.032958984375]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 15272.9277\n",
      "rewards:  43.0 q-value:  0\n",
      "loss: [15272.927734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 252us/step - loss: 6052.8403\n",
      "rewards:  38.0 q-value:  0\n",
      "loss: [6052.84033203125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 8205.2412\n",
      "rewards:  48.0 q-value:  [[0.         0.         0.         0.         0.         0.44756693\n",
      "  0.         0.         0.         0.         0.         0.01645088\n",
      "  0.10015336 0.         0.         0.         0.10797732 0.2469539\n",
      "  0.         0.         0.        ]]\n",
      "loss: [8205.2412109375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 3170.9741\n",
      "rewards:  42.0 q-value:  0\n",
      "loss: [3170.97412109375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 249us/step - loss: 6081.1172\n",
      "rewards:  35.0 q-value:  0\n",
      "loss: [6081.1171875]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 191us/step - loss: 3603.5828\n",
      "rewards:  33.0 q-value:  0\n",
      "loss: [3603.582763671875]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 18427.1387\n",
      "rewards:  26.0 q-value:  0\n",
      "loss: [18427.138671875]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 137us/step - loss: 7904.3970\n",
      "rewards:  -17.0 q-value:  0\n",
      "loss: [7904.39697265625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 8553.2695\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [8553.26953125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 3045.9243\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [3045.92431640625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 265us/step - loss: 2635.9341\n",
      "rewards:  11.0 q-value:  0\n",
      "loss: [2635.93408203125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 4266.5093\n",
      "rewards:  9.0 q-value:  0\n",
      "loss: [4266.50927734375]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 10003.0107\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [10003.0107421875]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 126us/step - loss: 11453.3672\n",
      "rewards:  7.0 q-value:  0\n",
      "loss: [11453.3671875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 4874.7607\n",
      "rewards:  6.0 q-value:  0\n",
      "loss: [4874.7607421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 3494.6992\n",
      "rewards:  4.0 q-value:  0\n",
      "loss: [3494.69921875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 7151.7798\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [7151.77978515625]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 116us/step - loss: 8469.0986\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [8469.0986328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 2494.7915\n",
      "rewards:  36.0 q-value:  0\n",
      "loss: [2494.79150390625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 5867.1650\n",
      "rewards:  76.0 q-value:  [[0.         0.         0.         0.         0.         0.32062542\n",
      "  0.         0.         0.         0.         0.         0.03442415\n",
      "  0.05699428 0.         0.         0.         0.04835392 0.1335339\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5867.1650390625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 114us/step - loss: 4337.4756\n",
      "rewards:  36.0 q-value:  0\n",
      "loss: [4337.4755859375]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 3758.3208\n",
      "rewards:  26.0 q-value:  0\n",
      "loss: [3758.32080078125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 11023.6133\n",
      "rewards:  13.0 q-value:  0\n",
      "loss: [11023.61328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 4790.5938\n",
      "rewards:  -13.0 q-value:  0\n",
      "loss: [4790.59375]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 10997.8252\n",
      "rewards:  -16.0 q-value:  0\n",
      "loss: [10997.8251953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 1833.4280\n",
      "rewards:  -21.0 q-value:  0\n",
      "loss: [1833.427978515625]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 6293.6011\n",
      "rewards:  -37.0 q-value:  0\n",
      "loss: [6293.60107421875]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 13509.2627\n",
      "rewards:  -42.0 q-value:  0\n",
      "loss: [13509.2626953125]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 1226.4236\n",
      "rewards:  -48.0 q-value:  [[0.         0.         0.         0.         0.         0.30603063\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0379081  0.         0.         0.         0.02417917 0.10998644\n",
      "  0.         0.046005   0.        ]]\n",
      "loss: [1226.423583984375]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 4903.2256\n",
      "rewards:  -66.0 q-value:  0\n",
      "loss: [4903.2255859375]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 5578.6621\n",
      "rewards:  -26.0 q-value:  [[0.         0.         0.         0.         0.         0.3361417\n",
      "  0.02266453 0.         0.         0.         0.         0.\n",
      "  0.04657511 0.         0.         0.         0.08886927 0.19203365\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5578.662109375]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 9569.3174\n",
      "rewards:  -34.0 q-value:  0\n",
      "loss: [9569.3173828125]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 2963.7610\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [2963.760986328125]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 9353.9336\n",
      "rewards:  -28.0 q-value:  0\n",
      "loss: [9353.93359375]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 9478.9258\n",
      "rewards:  -44.0 q-value:  0\n",
      "loss: [9478.92578125]\n",
      "Exploring\n",
      "Selected action  [0 2]\n",
      "[0 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 4909.1387\n",
      "rewards:  -29.0 q-value:  0\n",
      "loss: [4909.138671875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 7357.9395\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [7357.939453125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 132us/step - loss: 3100.7017\n",
      "rewards:  -6.0 q-value:  0\n",
      "loss: [3100.70166015625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 325us/step - loss: 1479.6506\n",
      "rewards:  18.0 q-value:  0\n",
      "loss: [1479.650634765625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 8093.7100\n",
      "rewards:  26.0 q-value:  [[0.         0.         0.         0.         0.         0.3646671\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06910945 0.         0.         0.         0.065116   0.16416334\n",
      "  0.         0.00672938 0.        ]]\n",
      "loss: [8093.7099609375]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 200us/step - loss: 9705.4219\n",
      "rewards:  19.0 q-value:  0\n",
      "loss: [9705.421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 147us/step - loss: 6968.6689\n",
      "rewards:  14.0 q-value:  0\n",
      "loss: [6968.6689453125]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 3564.9302\n",
      "rewards:  26.0 q-value:  0\n",
      "loss: [3564.93017578125]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 109us/step - loss: 7326.5474\n",
      "rewards:  58.0 q-value:  0\n",
      "loss: [7326.54736328125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 204us/step - loss: 7533.9229\n",
      "rewards:  53.0 q-value:  0\n",
      "loss: [7533.9228515625]\n",
      "Exploring\n",
      "Selected action  [3 1]\n",
      "[3 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 18381.9277\n",
      "rewards:  23.0 q-value:  0\n",
      "loss: [18381.927734375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 1242.1149\n",
      "rewards:  -3.0 q-value:  0\n",
      "loss: [1242.1148681640625]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 5712.5732\n",
      "rewards:  -9.0 q-value:  0\n",
      "loss: [5712.5732421875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 99us/step - loss: 3487.4136\n",
      "rewards:  -13.0 q-value:  0\n",
      "loss: [3487.41357421875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 4026.4307\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [4026.4306640625]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 7983.6104\n",
      "rewards:  8.0 q-value:  0\n",
      "loss: [7983.6103515625]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 8453.2178\n",
      "rewards:  36.0 q-value:  0\n",
      "loss: [8453.2177734375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 89us/step - loss: 9254.2197\n",
      "rewards:  55.0 q-value:  0\n",
      "loss: [9254.2197265625]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 9894.8389\n",
      "rewards:  39.0 q-value:  0\n",
      "loss: [9894.8388671875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 87us/step - loss: 1408.6470\n",
      "rewards:  50.0 q-value:  0\n",
      "loss: [1408.64697265625]\n",
      "Exploiting\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 13490.7959\n",
      "rewards:  50.0 q-value:  [[0.         0.         0.         0.         0.         0.25198\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02528083 0.         0.         0.01031376 0.         0.09032547\n",
      "  0.         0.01692985 0.        ]]\n",
      "loss: [13490.7958984375]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 2971.5391\n",
      "rewards:  39.0 q-value:  0\n",
      "loss: [2971.5390625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 196us/step - loss: 7831.0908\n",
      "rewards:  55.0 q-value:  0\n",
      "loss: [7831.0908203125]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 1258.3282\n",
      "rewards:  37.0 q-value:  0\n",
      "loss: [1258.3282470703125]\n",
      "Exploring\n",
      "Selected action  [3 4]\n",
      "[3 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 2210.2588\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [2210.2587890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 4871.1172\n",
      "rewards:  29.0 q-value:  0\n",
      "loss: [4871.1171875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 3791.1948\n",
      "rewards:  26.0 q-value:  0\n",
      "loss: [3791.19482421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 92us/step - loss: 2637.2842\n",
      "rewards:  24.0 q-value:  0\n",
      "loss: [2637.2841796875]\n",
      "Exploring\n",
      "Selected action  [0 4]\n",
      "[0 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 5195.1431\n",
      "rewards:  24.0 q-value:  0\n",
      "loss: [5195.14306640625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 2858.4766\n",
      "rewards:  32.0 q-value:  0\n",
      "loss: [2858.4765625]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 1191.7600\n",
      "rewards:  30.0 q-value:  0\n",
      "loss: [1191.760009765625]\n",
      "Exploring\n",
      "Selected action  [1 2]\n",
      "[1 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 185us/step - loss: 2018.3055\n",
      "rewards:  20.0 q-value:  0\n",
      "loss: [2018.3055419921875]\n",
      "Exploring\n",
      "Selected action  [1 4]\n",
      "[1 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 2492.6248\n",
      "rewards:  15.0 q-value:  0\n",
      "loss: [2492.624755859375]\n",
      "Exploring\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 1657.6533\n",
      "rewards:  31.0 q-value:  0\n",
      "loss: [1657.6533203125]\n",
      "Exploring\n",
      "Selected action  [4 2]\n",
      "[4 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 5020.0127\n",
      "rewards:  1.0 q-value:  0\n",
      "loss: [5020.0126953125]\n",
      "Number of actions available 3\n",
      "Episode : 15\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 249us/step - loss: 10488.1270\n",
      "rewards:  -5 q-value:  0\n",
      "loss: [10488.126953125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 246us/step - loss: 1843.5276\n",
      "rewards:  -10 q-value:  0\n",
      "loss: [1843.527587890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 8826.9990\n",
      "rewards:  -15 q-value:  0\n",
      "loss: [8826.9990234375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 16096.8428\n",
      "rewards:  -25.0 q-value:  0\n",
      "loss: [16096.8427734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 8469.8867\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [8469.88671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 184us/step - loss: 3374.7395\n",
      "rewards:  -35.0 q-value:  0\n",
      "loss: [3374.739501953125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 10840.9922\n",
      "rewards:  -38.0 q-value:  0\n",
      "loss: [10840.9921875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 263us/step - loss: 5430.3350\n",
      "rewards:  -40.0 q-value:  0\n",
      "loss: [5430.3349609375]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 5239.6426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -42.0 q-value:  [[0.         0.         0.         0.         0.         0.17813192\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0278997  0.         0.         0.01525487 0.         0.11890816\n",
      "  0.         0.02513181 0.        ]]\n",
      "loss: [5239.642578125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 1915.7162\n",
      "rewards:  -47.0 q-value:  0\n",
      "loss: [1915.7161865234375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 7636.0635\n",
      "rewards:  -52.0 q-value:  0\n",
      "loss: [7636.0634765625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 248us/step - loss: 1891.6599\n",
      "rewards:  -57.0 q-value:  [[0.         0.         0.         0.         0.         0.19964492\n",
      "  0.         0.         0.         0.02480842 0.         0.\n",
      "  0.00202443 0.         0.         0.04684129 0.         0.06943158\n",
      "  0.         0.0444722  0.        ]]\n",
      "loss: [1891.659912109375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 194us/step - loss: 5439.3984\n",
      "rewards:  -59.0 q-value:  0\n",
      "loss: [5439.3984375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 187us/step - loss: 15268.0928\n",
      "rewards:  -64.0 q-value:  0\n",
      "loss: [15268.0927734375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 142us/step - loss: 8417.2910\n",
      "rewards:  -69.0 q-value:  0\n",
      "loss: [8417.291015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 243us/step - loss: 6197.3892\n",
      "rewards:  -74.0 q-value:  0\n",
      "loss: [6197.38916015625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 2831.5642\n",
      "rewards:  -79.0 q-value:  0\n",
      "loss: [2831.564208984375]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 3963.9548\n",
      "rewards:  -84.0 q-value:  [[0.         0.         0.         0.         0.         0.40479267\n",
      "  0.         0.00198686 0.         0.         0.         0.\n",
      "  0.11488099 0.         0.         0.         0.10158676 0.19094022\n",
      "  0.         0.         0.        ]]\n",
      "loss: [3963.954833984375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 9491.9883\n",
      "rewards:  -98.0 q-value:  0\n",
      "loss: [9491.98828125]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 202us/step - loss: 23334.7480\n",
      "rewards:  -103.0 q-value:  [[0.         0.         0.         0.         0.         0.30688462\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05799669 0.         0.         0.006115   0.01696903 0.16303253\n",
      "  0.         0.         0.        ]]\n",
      "loss: [23334.748046875]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 3987.6990\n",
      "rewards:  -100.0 q-value:  [[0.         0.         0.         0.         0.         0.4738858\n",
      "  0.         0.         0.         0.         0.         0.01985888\n",
      "  0.00137258 0.         0.         0.00794163 0.01780019 0.16254835\n",
      "  0.         0.06050638 0.        ]]\n",
      "loss: [3987.698974609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 152us/step - loss: 1468.8301\n",
      "rewards:  -105.0 q-value:  0\n",
      "loss: [1468.830078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 6100.9272\n",
      "rewards:  -110.0 q-value:  0\n",
      "loss: [6100.92724609375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 234us/step - loss: 3938.4287\n",
      "rewards:  -111.0 q-value:  0\n",
      "loss: [3938.4287109375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 176us/step - loss: 3423.6406\n",
      "rewards:  -112.0 q-value:  0\n",
      "loss: [3423.640625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 16727.8223\n",
      "rewards:  -144.0 q-value:  0\n",
      "loss: [16727.822265625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 221us/step - loss: 7468.3984\n",
      "rewards:  -146.0 q-value:  0\n",
      "loss: [7468.3984375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 225us/step - loss: 4506.4702\n",
      "rewards:  -148.0 q-value:  0\n",
      "loss: [4506.47021484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 229us/step - loss: 1118.2358\n",
      "rewards:  -150.0 q-value:  0\n",
      "loss: [1118.23583984375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 100us/step - loss: 2394.8892\n",
      "rewards:  -152.0 q-value:  0\n",
      "loss: [2394.88916015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 201us/step - loss: 10108.9395\n",
      "rewards:  -154.0 q-value:  0\n",
      "loss: [10108.939453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 9417.1816\n",
      "rewards:  -155.0 q-value:  0\n",
      "loss: [9417.181640625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 140us/step - loss: 11969.6074\n",
      "rewards:  -157.0 q-value:  0\n",
      "loss: [11969.607421875]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 2581.6765\n",
      "rewards:  -159.0 q-value:  [[0.         0.         0.         0.         0.         0.35935634\n",
      "  0.00550783 0.         0.         0.         0.         0.\n",
      "  0.05003618 0.         0.         0.         0.06934999 0.1832262\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2581.676513671875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 233us/step - loss: 2003.6677\n",
      "rewards:  -161.0 q-value:  0\n",
      "loss: [2003.667724609375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 3999.0244\n",
      "rewards:  -166.0 q-value:  0\n",
      "loss: [3999.0244140625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 117us/step - loss: 1184.1624\n",
      "rewards:  -171.0 q-value:  0\n",
      "loss: [1184.162353515625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 10338.0195\n",
      "rewards:  -196.0 q-value:  0\n",
      "loss: [10338.01953125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 7624.9761\n",
      "rewards:  -176.0 q-value:  0\n",
      "loss: [7624.97607421875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 10341.0078\n",
      "rewards:  -181.0 q-value:  [[0.         0.         0.         0.         0.         0.36944294\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06903709 0.         0.         0.         0.07951924 0.13983117\n",
      "  0.         0.0320149  0.        ]]\n",
      "loss: [10341.0078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 6431.2476\n",
      "rewards:  -184.0 q-value:  0\n",
      "loss: [6431.24755859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 330us/step - loss: 3882.7896\n",
      "rewards:  -189.0 q-value:  0\n",
      "loss: [3882.78955078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 164us/step - loss: 3438.7766\n",
      "rewards:  -194.0 q-value:  0\n",
      "loss: [3438.776611328125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 171us/step - loss: 4986.5078\n",
      "rewards:  -199.0 q-value:  0\n",
      "loss: [4986.5078125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 7332.3511\n",
      "rewards:  -202.0 q-value:  0\n",
      "loss: [7332.35107421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 8093.7017\n",
      "rewards:  -207.0 q-value:  0\n",
      "loss: [8093.70166015625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 12611.2725\n",
      "rewards:  -210.0 q-value:  0\n",
      "loss: [12611.2724609375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 9326.6885\n",
      "rewards:  -214.0 q-value:  0\n",
      "loss: [9326.6884765625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 6908.3311\n",
      "rewards:  -234.0 q-value:  0\n",
      "loss: [6908.3310546875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 120us/step - loss: 1026.3635\n",
      "rewards:  -239.0 q-value:  0\n",
      "loss: [1026.363525390625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 130us/step - loss: 12550.1240\n",
      "rewards:  -259.0 q-value:  0\n",
      "loss: [12550.1240234375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 14257.0068\n",
      "rewards:  -259.0 q-value:  0\n",
      "loss: [14257.0068359375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 7130.0684\n",
      "rewards:  -259.0 q-value:  0\n",
      "loss: [7130.068359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 6583.1045\n",
      "rewards:  -264.0 q-value:  0\n",
      "loss: [6583.1044921875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 4431.1514\n",
      "rewards:  -268.0 q-value:  0\n",
      "loss: [4431.1513671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 160us/step - loss: 11035.7939\n",
      "rewards:  -273.0 q-value:  0\n",
      "loss: [11035.7939453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 246us/step - loss: 1838.3779\n",
      "rewards:  -280.0 q-value:  0\n",
      "loss: [1838.3779296875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 5806.6318\n",
      "rewards:  -287.0 q-value:  0\n",
      "loss: [5806.6318359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 4187.2866\n",
      "rewards:  -292.0 q-value:  0\n",
      "loss: [4187.28662109375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 5564.2588\n",
      "rewards:  -296.0 q-value:  0\n",
      "loss: [5564.2587890625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 5022.0557\n",
      "rewards:  -321.0 q-value:  0\n",
      "loss: [5022.0556640625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 14611.8916\n",
      "rewards:  -326.0 q-value:  0\n",
      "loss: [14611.8916015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 6426.7793\n",
      "rewards:  -351.0 q-value:  0\n",
      "loss: [6426.779296875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 169us/step - loss: 1747.7649\n",
      "rewards:  -356.0 q-value:  0\n",
      "loss: [1747.764892578125]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 7190.6636\n",
      "rewards:  -381.0 q-value:  [[0.         0.         0.         0.         0.         0.28310367\n",
      "  0.         0.02053253 0.         0.         0.         0.\n",
      "  0.07785187 0.         0.         0.         0.04476193 0.16371593\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7190.66357421875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 6760.9717\n",
      "rewards:  -381.0 q-value:  0\n",
      "loss: [6760.9716796875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 198us/step - loss: 12933.7285\n",
      "rewards:  -361.0 q-value:  0\n",
      "loss: [12933.728515625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 148us/step - loss: 14021.5312\n",
      "rewards:  -368.0 q-value:  0\n",
      "loss: [14021.53125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 103us/step - loss: 2828.7383\n",
      "rewards:  -382.0 q-value:  0\n",
      "loss: [2828.73828125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 161us/step - loss: 3334.6912\n",
      "rewards:  -370.0 q-value:  0\n",
      "loss: [3334.691162109375]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 157us/step - loss: 8815.0625\n",
      "rewards:  -373.0 q-value:  [[0.         0.         0.         0.         0.         0.44820577\n",
      "  0.         0.         0.         0.         0.         0.03876816\n",
      "  0.0638662  0.         0.05517409 0.         0.09746162 0.2304306\n",
      "  0.         0.06404777 0.        ]]\n",
      "loss: [8815.0625]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 94us/step - loss: 5575.3652\n",
      "rewards:  -378.0 q-value:  [[0.         0.         0.         0.         0.         0.30091867\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06273489 0.         0.         0.         0.04118199 0.12234174\n",
      "  0.         0.         0.        ]]\n",
      "loss: [5575.365234375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 4101.3145\n",
      "rewards:  -383.0 q-value:  [[0.         0.         0.         0.         0.         0.36149418\n",
      "  0.         0.         0.         0.         0.         0.03617566\n",
      "  0.06007251 0.         0.         0.         0.0877143  0.17855573\n",
      "  0.         0.01095189 0.        ]]\n",
      "loss: [4101.314453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 163us/step - loss: 1775.6465\n",
      "rewards:  -388.0 q-value:  0\n",
      "loss: [1775.646484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 14608.0605\n",
      "rewards:  -393.0 q-value:  0\n",
      "loss: [14608.060546875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 10537.0605\n",
      "rewards:  -399.0 q-value:  0\n",
      "loss: [10537.060546875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 7483.6304\n",
      "rewards:  -400.0 q-value:  0\n",
      "loss: [7483.63037109375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 14653.8789\n",
      "rewards:  -406.0 q-value:  0\n",
      "loss: [14653.87890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 7670.7119\n",
      "rewards:  -411.0 q-value:  0\n",
      "loss: [7670.7119140625]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 179us/step - loss: 3453.4028\n",
      "rewards:  -413.0 q-value:  [[0.         0.         0.         0.         0.         0.2925523\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05486131 0.         0.         0.         0.02614104 0.1449795\n",
      "  0.         0.00986231 0.        ]]\n",
      "loss: [3453.40283203125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 135us/step - loss: 9240.9365\n",
      "rewards:  -415.0 q-value:  0\n",
      "loss: [9240.9365234375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 153us/step - loss: 2727.8320\n",
      "rewards:  -417.0 q-value:  0\n",
      "loss: [2727.83203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 5177.7769\n",
      "rewards:  -422.0 q-value:  0\n",
      "loss: [5177.77685546875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 4516.4492\n",
      "rewards:  -452.0 q-value:  0\n",
      "loss: [4516.44921875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 7081.1396\n",
      "rewards:  -457.0 q-value:  0\n",
      "loss: [7081.1396484375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 156us/step - loss: 3071.1653\n",
      "rewards:  -487.0 q-value:  0\n",
      "loss: [3071.165283203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 3106.4785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -492.0 q-value:  0\n",
      "loss: [3106.478515625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 6620.6484\n",
      "rewards:  -498.0 q-value:  0\n",
      "loss: [6620.6484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 143us/step - loss: 3329.0947\n",
      "rewards:  -503.0 q-value:  0\n",
      "loss: [3329.0947265625]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 106us/step - loss: 4010.6382\n",
      "rewards:  -509.0 q-value:  [[0.         0.         0.         0.         0.         0.34437835\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03924166 0.         0.         0.         0.0516016  0.11003996\n",
      "  0.         0.03479169 0.        ]]\n",
      "loss: [4010.63818359375]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 175us/step - loss: 5476.3652\n",
      "rewards:  -515.0 q-value:  [[0.         0.         0.         0.         0.         0.39173502\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.07012677 0.         0.02291518 0.         0.09454925 0.14729641\n",
      "  0.         0.03072543 0.        ]]\n",
      "loss: [5476.365234375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 5552.9365\n",
      "rewards:  -518.0 q-value:  0\n",
      "loss: [5552.9365234375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 118us/step - loss: 7458.1387\n",
      "rewards:  -521.0 q-value:  0\n",
      "loss: [7458.138671875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 129us/step - loss: 3521.3301\n",
      "rewards:  -523.0 q-value:  0\n",
      "loss: [3521.330078125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 188us/step - loss: 5019.0713\n",
      "rewards:  -525.0 q-value:  0\n",
      "loss: [5019.0712890625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 233us/step - loss: 9722.0742\n",
      "rewards:  -527.0 q-value:  0\n",
      "loss: [9722.07421875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 11819.4590\n",
      "rewards:  -532.0 q-value:  0\n",
      "loss: [11819.458984375]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 213us/step - loss: 2628.6255\n",
      "rewards:  -553.0 q-value:  [[0.         0.         0.         0.         0.         0.3728064\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06556693 0.         0.         0.         0.04340621 0.18717667\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2628.62548828125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 2725.9973\n",
      "rewards:  -538.0 q-value:  0\n",
      "loss: [2725.997314453125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 6443.4609\n",
      "rewards:  -544.0 q-value:  0\n",
      "loss: [6443.4609375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 11778.5859\n",
      "rewards:  -541.0 q-value:  0\n",
      "loss: [11778.5859375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 223us/step - loss: 7854.5991\n",
      "rewards:  -546.0 q-value:  0\n",
      "loss: [7854.59912109375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 97us/step - loss: 7449.2061\n",
      "rewards:  -551.0 q-value:  0\n",
      "loss: [7449.2060546875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 231us/step - loss: 4554.4414\n",
      "rewards:  -548.0 q-value:  0\n",
      "loss: [4554.44140625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 193us/step - loss: 4587.8901\n",
      "rewards:  -553.0 q-value:  0\n",
      "loss: [4587.89013671875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 5883.5356\n",
      "rewards:  -550.0 q-value:  0\n",
      "loss: [5883.53564453125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 149us/step - loss: 3798.3381\n",
      "rewards:  -552.0 q-value:  0\n",
      "loss: [3798.338134765625]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 104us/step - loss: 2735.7170\n",
      "rewards:  -557.0 q-value:  [[0.         0.         0.         0.         0.         0.3004117\n",
      "  0.02731103 0.         0.         0.         0.         0.\n",
      "  0.01250334 0.         0.         0.         0.04300261 0.12440567\n",
      "  0.         0.         0.        ]]\n",
      "loss: [2735.717041015625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 122us/step - loss: 9257.7480\n",
      "rewards:  -565.0 q-value:  0\n",
      "loss: [9257.748046875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 115us/step - loss: 2994.0090\n",
      "rewards:  -572.0 q-value:  0\n",
      "loss: [2994.009033203125]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 101us/step - loss: 4641.3506\n",
      "rewards:  -577.0 q-value:  [[0.         0.         0.         0.         0.         0.3969646\n",
      "  0.         0.         0.         0.         0.         0.02634066\n",
      "  0.04089535 0.         0.         0.         0.06397285 0.14053455\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4641.3505859375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 17084.0703\n",
      "rewards:  -583.0 q-value:  0\n",
      "loss: [17084.0703125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 2522.4971\n",
      "rewards:  -585.0 q-value:  0\n",
      "loss: [2522.4970703125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 5680.7256\n",
      "rewards:  -590.0 q-value:  0\n",
      "loss: [5680.7255859375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 119us/step - loss: 7012.9023\n",
      "rewards:  -592.0 q-value:  0\n",
      "loss: [7012.90234375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 133us/step - loss: 3597.6138\n",
      "rewards:  -597.0 q-value:  0\n",
      "loss: [3597.61376953125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 3430.4031\n",
      "rewards:  -603.0 q-value:  0\n",
      "loss: [3430.403076171875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 181us/step - loss: 4852.1099\n",
      "rewards:  -600.0 q-value:  0\n",
      "loss: [4852.10986328125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 123us/step - loss: 3886.5081\n",
      "rewards:  -625.0 q-value:  0\n",
      "loss: [3886.508056640625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 155us/step - loss: 8506.4658\n",
      "rewards:  -625.0 q-value:  0\n",
      "loss: [8506.4658203125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 113us/step - loss: 2877.5527\n",
      "rewards:  -630.0 q-value:  0\n",
      "loss: [2877.552734375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 166us/step - loss: 5717.9561\n",
      "rewards:  -635.0 q-value:  0\n",
      "loss: [5717.9560546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 5053.0767\n",
      "rewards:  -640.0 q-value:  0\n",
      "loss: [5053.07666015625]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 9181.3896\n",
      "rewards:  -651.0 q-value:  [[0.         0.         0.         0.         0.         0.32045433\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05669462 0.         0.         0.02009091 0.00321265 0.10103031\n",
      "  0.         0.02576705 0.        ]]\n",
      "loss: [9181.3896484375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 108us/step - loss: 9289.6055\n",
      "rewards:  -656.0 q-value:  0\n",
      "loss: [9289.60546875]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 3184.4983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  -659.0 q-value:  0\n",
      "loss: [3184.498291015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 11315.5166\n",
      "rewards:  -670.0 q-value:  0\n",
      "loss: [11315.5166015625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 86us/step - loss: 2319.9194\n",
      "rewards:  -671.0 q-value:  0\n",
      "loss: [2319.91943359375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 182us/step - loss: 11135.9004\n",
      "rewards:  -679.0 q-value:  0\n",
      "loss: [11135.900390625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 150us/step - loss: 6374.4678\n",
      "rewards:  -684.0 q-value:  0\n",
      "loss: [6374.4677734375]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 111us/step - loss: 3132.1968\n",
      "rewards:  -691.0 q-value:  0\n",
      "loss: [3132.19677734375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 138us/step - loss: 3817.7043\n",
      "rewards:  -689.0 q-value:  0\n",
      "loss: [3817.704345703125]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 112us/step - loss: 6197.2051\n",
      "rewards:  -709.0 q-value:  0\n",
      "loss: [6197.205078125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 125us/step - loss: 6917.5996\n",
      "rewards:  -714.0 q-value:  0\n",
      "loss: [6917.599609375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 174us/step - loss: 5445.0088\n",
      "rewards:  -718.0 q-value:  0\n",
      "loss: [5445.0087890625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 170us/step - loss: 4953.1685\n",
      "rewards:  -723.0 q-value:  0\n",
      "loss: [4953.16845703125]\n",
      "Exploiting\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 131us/step - loss: 7359.5151\n",
      "rewards:  -727.0 q-value:  [[0.         0.         0.         0.         0.         0.3449599\n",
      "  0.         0.         0.         0.         0.         0.00898233\n",
      "  0.04846479 0.         0.         0.         0.0587536  0.15256861\n",
      "  0.         0.02490059 0.        ]]\n",
      "loss: [7359.51513671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 91us/step - loss: 8523.9297\n",
      "rewards:  -732.0 q-value:  0\n",
      "loss: [8523.9296875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 128us/step - loss: 8315.2031\n",
      "rewards:  -767.0 q-value:  0\n",
      "loss: [8315.203125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 4416.6313\n",
      "rewards:  -739.0 q-value:  0\n",
      "loss: [4416.63134765625]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 172us/step - loss: 10908.6934\n",
      "rewards:  -742.0 q-value:  0\n",
      "loss: [10908.693359375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 256us/step - loss: 1522.7283\n",
      "rewards:  -747.0 q-value:  [[0.         0.         0.         0.         0.         0.37820938\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.07781298 0.         0.         0.         0.01111797 0.17751846\n",
      "  0.         0.         0.        ]]\n",
      "loss: [1522.728271484375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 199us/step - loss: 8171.4512\n",
      "rewards:  -752.0 q-value:  0\n",
      "loss: [8171.451171875]\n",
      "Exploiting\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 144us/step - loss: 4107.2173\n",
      "rewards:  -777.0 q-value:  [[0.         0.         0.         0.         0.         0.34310234\n",
      "  0.         0.         0.         0.         0.         0.00319873\n",
      "  0.08362886 0.         0.         0.00246775 0.03871008 0.16722097\n",
      "  0.         0.         0.        ]]\n",
      "loss: [4107.21728515625]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 207us/step - loss: 9448.2207\n",
      "rewards:  -782.0 q-value:  0\n",
      "loss: [9448.220703125]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 154us/step - loss: 3786.9507\n",
      "rewards:  -787.0 q-value:  0\n",
      "loss: [3786.95068359375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 3145.0815\n",
      "rewards:  -794.0 q-value:  0\n",
      "loss: [3145.08154296875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 124us/step - loss: 10642.8184\n",
      "rewards:  -799.0 q-value:  [[0.         0.         0.         0.         0.         0.43465\n",
      "  0.         0.         0.         0.         0.         0.00759832\n",
      "  0.08693793 0.         0.         0.         0.10991791 0.14188528\n",
      "  0.         0.         0.        ]]\n",
      "loss: [10642.818359375]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 146us/step - loss: 8217.0488\n",
      "rewards:  -804.0 q-value:  0\n",
      "loss: [8217.048828125]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 177us/step - loss: 6763.4990\n",
      "rewards:  -809.0 q-value:  0\n",
      "loss: [6763.4990234375]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 159us/step - loss: 12226.5967\n",
      "rewards:  -814.0 q-value:  [[0.         0.         0.         0.         0.         0.43465\n",
      "  0.         0.         0.         0.         0.         0.00759832\n",
      "  0.08693793 0.         0.         0.         0.10991791 0.14188528\n",
      "  0.         0.         0.        ]]\n",
      "loss: [12226.5966796875]\n",
      "Exploring\n",
      "Selected action  [2 4]\n",
      "[2 4]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 141us/step - loss: 11141.2646\n",
      "rewards:  -828.0 q-value:  0\n",
      "loss: [11141.2646484375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 162us/step - loss: 4843.9600\n",
      "rewards:  -816.0 q-value:  0\n",
      "loss: [4843.9599609375]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 165us/step - loss: 2429.8066\n",
      "rewards:  -821.0 q-value:  0\n",
      "loss: [2429.806640625]\n",
      "Exploring\n",
      "Selected action  [2 0]\n",
      "[2 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 127us/step - loss: 4541.8667\n",
      "rewards:  -826.0 q-value:  0\n",
      "loss: [4541.86669921875]\n",
      "Number of actions available 13\n",
      "Episode : 16\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 203us/step - loss: 12216.0840\n",
      "rewards:  -5 q-value:  0\n",
      "loss: [12216.083984375]\n",
      "Exploiting\n",
      "Selected action  [3 2]\n",
      "[3 2]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 168us/step - loss: 7121.4526\n",
      "rewards:  0.0 q-value:  [[0.         0.         0.         0.         0.         0.3230577\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03866635 0.         0.         0.         0.06195512 0.15955721\n",
      "  0.         0.         0.        ]]\n",
      "loss: [7121.45263671875]\n",
      "Exploring\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 209us/step - loss: 4551.7646\n",
      "rewards:  -5.0 q-value:  0\n",
      "loss: [4551.7646484375]\n",
      "Exploring\n",
      "Selected action  [0 3]\n",
      "[0 3]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 95us/step - loss: 14638.9590\n",
      "rewards:  -1.0 q-value:  0\n",
      "loss: [14638.958984375]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 121us/step - loss: 4450.0420\n",
      "rewards:  -27.0 q-value:  0\n",
      "loss: [4450.0419921875]\n",
      "Exploring\n",
      "Selected action  [2 1]\n",
      "[2 1]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 87us/step - loss: 9966.0049\n",
      "rewards:  -30.0 q-value:  0\n",
      "loss: [9966.0048828125]\n",
      "Exploiting\n",
      "Selected action  [1 0]\n",
      "[1 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 136us/step - loss: 2706.7217\n",
      "rewards:  -18.0 q-value:  [[0.         0.         0.         0.         0.         0.25438878\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02354418 0.         0.         0.02552643 0.         0.06902142\n",
      "  0.         0.00485159 0.        ]]\n",
      "loss: [2706.7216796875]\n",
      "Exploiting\n",
      "Selected action  [0 0]\n",
      "[0 0]\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 98us/step - loss: 7506.4512\n",
      "rewards:  -23.0 q-value:  [[0.         0.         0.         0.         0.         0.36423838\n",
      "  0.00713171 0.         0.         0.         0.         0.\n",
      "  0.03763171 0.         0.03708072 0.         0.08234625 0.2242484\n",
      "  0.         0.00312193 0.        ]]\n",
      "loss: [7506.451171875]\n",
      "Exploring\n",
      "Selected action  [0 1]\n",
      "[0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 180us/step - loss: 3469.4907\n",
      "rewards:  -11.0 q-value:  0\n",
      "loss: [3469.49072265625]\n",
      "Exploring\n",
      "Selected action  [4 0]\n",
      "[4 0]\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    " # Call all the initialised variables of the environment\n",
    "env = CabDriver()\n",
    "#Call the DQN agent\n",
    "dqn = DQNAgent(env.state_size, env.action_size)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "   \n",
    "    _,_,curr_state = env.reset()\n",
    "    state_size = env.state_size\n",
    "    pos_act_ind, actions = env.requests(curr_state)\n",
    "    action = random.choice(actions)\n",
    "    #action_size = len(actions)\n",
    "    reward = 0\n",
    "    curr_time = 0\n",
    "    q_val_list = []\n",
    "    #print(curr_state)\n",
    "    \n",
    "    \n",
    "    terminal_state = False\n",
    "    print(\"Episode :\", episode)\n",
    "    \n",
    "    while not terminal_state:\n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        encoded_state = env.state_encod_arch1(curr_state)\n",
    "        #encoded_state = np.reshape(encoded_state, [1, env.state_size])\n",
    "        action, q_value = dqn.get_action(encoded_state, env.action_space, pos_act_ind)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = reward + env.reward_func(curr_state, action, Time_matrix)\n",
    "        next_state = env.next_state_func(curr_state,action,Time_matrix)\n",
    "        \n",
    "        q_val_list.append(q_value)\n",
    "        \n",
    "        p = action[0]\n",
    "        q = action[1]\n",
    "        i = curr_state[0]\n",
    "        time = curr_state[1]\n",
    "        day = curr_state[2]\n",
    "        print(action)\n",
    "        curr_time = curr_time + Time_matrix[i][p][time][day]\n",
    "        \n",
    "        \n",
    "        day = int((day+int(time/24)) % 7)\n",
    "        time = int(time % 24)\n",
    "        \n",
    "        curr_time = curr_time + Time_matrix[p][q][time][day]\n",
    "        day = int((day+int(time/24)) % 7)\n",
    "        time = int(time % 24)\n",
    "        # 3. Append the experience to the memory\n",
    "        dqn.append_sample(curr_state, action, reward, next_state, terminal_state)\n",
    "        curr_state = next_state\n",
    "        \n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        history = dqn.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        \n",
    "        if history:\n",
    "            print('rewards: ', reward, 'q-value: ', q_value)\n",
    "            print('loss:', history.history['loss'])\n",
    "        \n",
    "        if curr_time >= 24*30:\n",
    "            terminal_state = True\n",
    "    \n",
    "        # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(reward)\n",
    "    episodes.append(episode)\n",
    "        \n",
    "    if dqn.epsilon_max > dqn.epsilon_min:\n",
    "        dqn.epsilon_max *= dqn.epsilon_decay        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory\n",
    "if not os.path.exists(\"saved_pickle_files\"):\n",
    "    os.mkdir(\"saved_pickle_files\")\n",
    "\n",
    "# save rewards_per_episode\n",
    "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")\n",
    "\n",
    "\n",
    "# plot results\n",
    "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
    "    rewards_per_episode = pickle.load(f)\n",
    "\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"reward per episode\")\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('rewards.png')\n",
    "\n",
    "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
